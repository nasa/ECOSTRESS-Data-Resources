[
  {
    "objectID": "welcome.html",
    "href": "welcome.html",
    "title": "ECOSTRESS Data Resources",
    "section": "",
    "text": "Welcome! This repository provides guides, short how-tos, and tutorials to help users access and work with data from the Ecosystem Spaceborne Thermal Radiometer Experiment on Space Station (ECOSTRESS) mission distributed by the Land Processes Distributed Active Archive Center (LP DAAC). In the interest of open science this repository has been made public but is still under active development. All jupyter notebooks and scripts should be functional, however, changes or additions may be made. Contributions from all parties are welcome.",
    "crumbs": [
      "Overview",
      "Welcome"
    ]
  },
  {
    "objectID": "welcome.html#ecostress-background",
    "href": "welcome.html#ecostress-background",
    "title": "ECOSTRESS Data Resources",
    "section": "ECOSTRESS Background",
    "text": "ECOSTRESS Background\nThe ECOsystem Spaceborne Thermal Radiometer Experiment on Space Station (ECOSTRESS) is aboard the International Space Station (ISS) and measures the temperature of plants to better understand how much water plants need and how they respond to stress. ECOSTRESS was launched to the ISS on June 29, 2018. It has a viewing swath width of around 384 km and views the surface of the Earth from 53.6° N latitude to 53.6° S latitude with variable revisit times, dependent on the orbit of the ISS.\nECOSTRESS addresses three overarching science questions: How is the terrestrial biosphere responding to changes in water availability? How do changes in diurnal vegetation water stress impact the global carbon cycle? Can agricultural vulnerability be reduced through advanced monitoring of agricultural water consumptive use and improved drought estimation? ECOSTRESS uses a multispectral thermal infrared radiometer to measure the surface temperature. The radiometer obtains detailed images of the Earth’s surface at ~70 m spatial resolution that can provide information on the temperature of an individual farmer’s field. Learn more on the ECOSTRESS website.",
    "crumbs": [
      "Overview",
      "Welcome"
    ]
  },
  {
    "objectID": "welcome.html#contact-info",
    "href": "welcome.html#contact-info",
    "title": "ECOSTRESS Data Resources",
    "section": "Contact Info:",
    "text": "Contact Info:\nEmail: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)¹\nWebsite: https://www.earthdata.nasa.gov/centers/lp-daac",
    "crumbs": [
      "Overview",
      "Welcome"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "2025_satm.html",
    "href": "2025_satm.html",
    "title": "2025 ECOTRESS Science and Application Team Meeting",
    "section": "",
    "text": "Welcome to 2025 ECOSTRESS Science and Applications Team Meeting workshop inspired by the NASA Space Apps Challenge! In this hands-on workshop, you will learn different ways to find, access, and explore ECOSTRESS data. You will also have two hours to create the most compelling ECOSTRESS image or application. The winning submission will be featured on the ECOSTRESS website.\nThis session is hosted by NASA Land Processes Distributed Active Archive Center (LP DAAC) and NASA Jet Propulsion Laboratory (JPL) with support from the NASA Openscapes project.\nHands-on exercises will be executed from a Jupyter Hub on the Openscapes 2i2c cloud instance.",
    "crumbs": [
      "Workshops",
      "2025 ECOSTRESS Science and Application Team Meeting"
    ]
  },
  {
    "objectID": "2025_satm.html#learning-objectives",
    "href": "2025_satm.html#learning-objectives",
    "title": "2025 ECOTRESS Science and Application Team Meeting",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nLP DAAC ECOSTRESS data updates\nWays to find and access ECOSTRESS data\nHow to explore ECOSTRESS tiled data using Earthdata Search\nHow to explore ECOSTRESS tiled data programmatically\nHow to evaluate quality of ECOSTESS data",
    "crumbs": [
      "Workshops",
      "2025 ECOSTRESS Science and Application Team Meeting"
    ]
  },
  {
    "objectID": "2025_satm.html#slides",
    "href": "2025_satm.html#slides",
    "title": "2025 ECOTRESS Science and Application Team Meeting",
    "section": "Slides",
    "text": "Slides",
    "crumbs": [
      "Workshops",
      "2025 ECOSTRESS Science and Application Team Meeting"
    ]
  },
  {
    "objectID": "2025_satm.html#knowledge-career-level",
    "href": "2025_satm.html#knowledge-career-level",
    "title": "2025 ECOTRESS Science and Application Team Meeting",
    "section": "Knowledge & Career Level:",
    "text": "Knowledge & Career Level:\nBeginner, intermediate",
    "crumbs": [
      "Workshops",
      "2025 ECOSTRESS Science and Application Team Meeting"
    ]
  },
  {
    "objectID": "2025_satm.html#contact-info",
    "href": "2025_satm.html#contact-info",
    "title": "2025 ECOTRESS Science and Application Team Meeting",
    "section": "Contact Info:",
    "text": "Contact Info:\nEmail: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)¹\nWebsite: https://www.earthdata.nasa.gov/centers/lp-daac",
    "crumbs": [
      "Workshops",
      "2025 ECOSTRESS Science and Application Team Meeting"
    ]
  },
  {
    "objectID": "CONTRIBUTING.html",
    "href": "CONTRIBUTING.html",
    "title": "Contributing to Our GitHub",
    "section": "",
    "text": "This page is your one-stop shop for uncovering how to contribute to our Github!\n\n\nNo, really, we do! Please come and participate in our community and lets do science together! Depending on your level of interaction with the Land Processes Data Active Archive Center (LP DAAC) and the LP DAAC GitHub, visitors to the site can be described as: - A community member: anyone in the open science community who visits a LP DAAC site, utilizes LP DAAC online tools, or attends a LP DAAC event. - A participant: anyone who posts a comment or poses a question in the GitHub Discussion Space, reports a site bug or requests a new resource in GitHub Issues, or attends a LP DAAC event and utilizes any virtual chat features during that event. - A contributor: anyone who forks this GitHub repository and submits pull requests to make additions or changes to the posted content.\nEveryone reading this page is a community member, and we hope everyone will post comments and join discussions as a participant. Contributors are welcome, particularly to help find and point to other open science resources.\n\n\n\nThere are two main ways to contribute to the LP DAAC GitHub.\n- Suggest a change, addition, or deletion to what is already on the GitHub using Issues. Issues can be about any LP DAAC plans, timelines, and content. - Before submitting a new issue, check to make sure a similar issue isn’t already open. If one is, contribute to that issue thread with your feedback. - When submitting a bug report, please try to provide as much detail as possible, i.e. a screenshot or gist that demonstrates the problem, the technology you are using, and any relevant links. - Issues labeled :sparkles:help wanted:sparkles: make it easy for you to find ways you can contribute today. - Become a contributor! Fork the repository and make commits to add resources and additional materials. Here are some ways you can contribute: - by reporting bugs - by suggesting new features - by translating content to a new language - by writing or editing documentation - by writing specifications - by writing code and documentation (no pull request is too small: fix typos, add code comments, clean up inconsistent whitespace) - by closing issues\nIn the spirit of open source software, everyone is encouraged to help improve this project!\n\n\n\nYou can sign up for GitHub here! The NASA Transform to Open Science Team has made a short video demonstrating how to make an easy pull request here.\nFor a more in-depth start, we suggest Getting Started with Git and GitHub: The Complete Beginner’s Guide and The Beginners Guide to Git and GitHub. We’ve summarized some of the most important points below.\n\n\nThis section is attributed to NumFOCUS and Adrienne Friend.\nOnce you’ve identified something you’d like to help with you’re ready to make a change to the project repository!\n\nFirst, describe what you’re planning to do as a comment to the issue, (and this might mean making a new issue).\nThis blog is a nice explanation of why putting this work in up front is so useful to everyone involved.\nFork this repository to your profile.\nYou can now do whatever you want with this copy of the project. You won’t mess up anyone else’s work so you’re super safe.\nMake sure to keep your fork up to date with the master repository.\nMake the changes you’ve discussed.\nTry to keep the changes focused rather than changing lots of things at once. If you feel tempted to branch out then please literally branch out: create separate branches for different updates to make the next step much easier!\nSubmit a pull request.\nA member of the executive team will review your changes, have a bit of discussion and hopefully merge them in!\nN.B. you don’t have to be ready to merge to make a pull request! We encourage you to submit a pull request as early as you want to. They help us to keep track of progress and help you to get earlier feedback.\n\n\n\n\n\nFor accepting new contributions, TOPS uses the forking workflow. As the first step of your contribution, you’ll want to fork this repository, make a local clone of it, add your contribution, and then create a pull request back to the LP DAAC repository.\nAll documentation should be written using Markdown and Github Markdown-supported HTML.\n\n\n\nThese contributing guidelines are adapted from the NASA Transform to Open Science github, available at https://github.com/nasa/Transform-to-Open-Science/blob/main/CONTRIBUTING.md.",
    "crumbs": [
      "Contributing",
      "Contributing to Our GitHub"
    ]
  },
  {
    "objectID": "CONTRIBUTING.html#we-want-your-help",
    "href": "CONTRIBUTING.html#we-want-your-help",
    "title": "Contributing to Our GitHub",
    "section": "",
    "text": "No, really, we do! Please come and participate in our community and lets do science together! Depending on your level of interaction with the Land Processes Data Active Archive Center (LP DAAC) and the LP DAAC GitHub, visitors to the site can be described as: - A community member: anyone in the open science community who visits a LP DAAC site, utilizes LP DAAC online tools, or attends a LP DAAC event. - A participant: anyone who posts a comment or poses a question in the GitHub Discussion Space, reports a site bug or requests a new resource in GitHub Issues, or attends a LP DAAC event and utilizes any virtual chat features during that event. - A contributor: anyone who forks this GitHub repository and submits pull requests to make additions or changes to the posted content.\nEveryone reading this page is a community member, and we hope everyone will post comments and join discussions as a participant. Contributors are welcome, particularly to help find and point to other open science resources.",
    "crumbs": [
      "Contributing",
      "Contributing to Our GitHub"
    ]
  },
  {
    "objectID": "CONTRIBUTING.html#ways-to-contribute-to-the-github",
    "href": "CONTRIBUTING.html#ways-to-contribute-to-the-github",
    "title": "Contributing to Our GitHub",
    "section": "",
    "text": "There are two main ways to contribute to the LP DAAC GitHub.\n- Suggest a change, addition, or deletion to what is already on the GitHub using Issues. Issues can be about any LP DAAC plans, timelines, and content. - Before submitting a new issue, check to make sure a similar issue isn’t already open. If one is, contribute to that issue thread with your feedback. - When submitting a bug report, please try to provide as much detail as possible, i.e. a screenshot or gist that demonstrates the problem, the technology you are using, and any relevant links. - Issues labeled :sparkles:help wanted:sparkles: make it easy for you to find ways you can contribute today. - Become a contributor! Fork the repository and make commits to add resources and additional materials. Here are some ways you can contribute: - by reporting bugs - by suggesting new features - by translating content to a new language - by writing or editing documentation - by writing specifications - by writing code and documentation (no pull request is too small: fix typos, add code comments, clean up inconsistent whitespace) - by closing issues\nIn the spirit of open source software, everyone is encouraged to help improve this project!",
    "crumbs": [
      "Contributing",
      "Contributing to Our GitHub"
    ]
  },
  {
    "objectID": "CONTRIBUTING.html#new-to-github-start-here",
    "href": "CONTRIBUTING.html#new-to-github-start-here",
    "title": "Contributing to Our GitHub",
    "section": "",
    "text": "You can sign up for GitHub here! The NASA Transform to Open Science Team has made a short video demonstrating how to make an easy pull request here.\nFor a more in-depth start, we suggest Getting Started with Git and GitHub: The Complete Beginner’s Guide and The Beginners Guide to Git and GitHub. We’ve summarized some of the most important points below.\n\n\nThis section is attributed to NumFOCUS and Adrienne Friend.\nOnce you’ve identified something you’d like to help with you’re ready to make a change to the project repository!\n\nFirst, describe what you’re planning to do as a comment to the issue, (and this might mean making a new issue).\nThis blog is a nice explanation of why putting this work in up front is so useful to everyone involved.\nFork this repository to your profile.\nYou can now do whatever you want with this copy of the project. You won’t mess up anyone else’s work so you’re super safe.\nMake sure to keep your fork up to date with the master repository.\nMake the changes you’ve discussed.\nTry to keep the changes focused rather than changing lots of things at once. If you feel tempted to branch out then please literally branch out: create separate branches for different updates to make the next step much easier!\nSubmit a pull request.\nA member of the executive team will review your changes, have a bit of discussion and hopefully merge them in!\nN.B. you don’t have to be ready to merge to make a pull request! We encourage you to submit a pull request as early as you want to. They help us to keep track of progress and help you to get earlier feedback.",
    "crumbs": [
      "Contributing",
      "Contributing to Our GitHub"
    ]
  },
  {
    "objectID": "CONTRIBUTING.html#development-model",
    "href": "CONTRIBUTING.html#development-model",
    "title": "Contributing to Our GitHub",
    "section": "",
    "text": "For accepting new contributions, TOPS uses the forking workflow. As the first step of your contribution, you’ll want to fork this repository, make a local clone of it, add your contribution, and then create a pull request back to the LP DAAC repository.\nAll documentation should be written using Markdown and Github Markdown-supported HTML.",
    "crumbs": [
      "Contributing",
      "Contributing to Our GitHub"
    ]
  },
  {
    "objectID": "CONTRIBUTING.html#attribution",
    "href": "CONTRIBUTING.html#attribution",
    "title": "Contributing to Our GitHub",
    "section": "",
    "text": "These contributing guidelines are adapted from the NASA Transform to Open Science github, available at https://github.com/nasa/Transform-to-Open-Science/blob/main/CONTRIBUTING.md.",
    "crumbs": [
      "Contributing",
      "Contributing to Our GitHub"
    ]
  },
  {
    "objectID": "guides/earthdata_search_ecostress.html",
    "href": "guides/earthdata_search_ecostress.html",
    "title": "Earthdata Search",
    "section": "",
    "text": "Earthdata Search\nThis tutorial guides you through how to use Earthdata Search for NASA Earth observations search and discovery, and how to connect the search output (e.g. download or access links) to a programmatic workflow (locally or from within the cloud).\n\nStep 1. Go to Earthdata Search and Login\nGo to Earthdata Search https://search.earthdata.nasa.gov and use your Earthdata login credentials to log in. If you do not have an Earthdata account, please see the Workshop Prerequisites for guidance.\n\n\nStep 2. Search for dataset of interest\nUse the search box in the upper left to type key words. In this example we are interested in the ECOSTRESS LSTE which is managed by the LP DAAC and made available from the NASA Earthdata Cloud archive hosted in AWS cloud.\nType ECOSTRESS in the search bar, then click on the “Available from AWS Cloud” filter option on the left.\n\n\n\nFigure caption: Search for ECOSTRESS data available in AWS cloud in Earthdata Search portal\n\n\nLet’s refine our search further. Now, let’s search for ECOSTRESS ECO_L2T_LSTE in the search box. A single Earthdata Search Collection is returned.\nClick on the (i) icon for the dataset to read more details, including the dataset shortname (helpful for programmatic workflows) just below the dataset name; here ECO_L2T_LSTE.\n\n\n\nFigure caption: Refine search\n\n\n\n\nStep 3. Explore the dataset details, including Cloud Access information\nOnce we click the (i), scrolling down the info page for the dataset we will see Cloud Access information, such as:\n\nwhether the dataset is available in the cloud\n\nthe cloud Region (all NASA Earthdata Cloud data is/will be in us-west-2 region)\n\nthe S3 storage bucket and object prefix where this data is located\n\nlink that generates AWS S3 Credentials for in-cloud data access (we will cover this in the Direct Data Access Tutorials)\n\nlink to documentation describing the In-region Direct S3 Access to Buckets. Note: these will be unique depending on the DAAC where the data is archived. (We will show examples of direct in-region access in Tutorial 3.)\n\n\n\n\nFigure caption: Cloud access info in EDS\n\n\n\n\n\nFigure caption: Documentation describing the In-region Direct S3 Access to Buckets\n\n\nNote: Clicking on “For Developers” to expand will provide programmatic endpoints such as those for the CMR API, and more.\nFor now, let’s say we are interested in getting download link(s) or access link(s) for specific data files (granules) within this collection.\nAt the top of the dataset info section, click on Search Results, which will take us back to the list of datasets matching our search parameters. Clicking on the dataset (ECOSTRESS ECO_L2T_LSTE) displaying a list of files (granules) that are part of the dataset (collection).\n\n\nStep 4a. Download or data access for a single granule\nTo download files for a granule click the download arrow on the card (or list row)\n\n\n\nFigure caption: Download granules\n\n\nYou can also get the S3 information (e.g., AWS region, bucket, temperary credentials for S3 access, and file names) by selecting the AWS S3 Access tab.\n\n\n\nFigure caption: S3 access for granules\n\n\n\nStep 4b. Download or data access for multiple granule\nTo download multiple granules, click on the green + symbol to add files to our project. Click on the green button towards the bottom that says “Download”. This will take us to another page with options to customize our download or access link(s).\n\n\n\nFigure caption: Select granules and click download\n\n\nOn the next page click the Direct Download option and click the green Download Data on the bottom left side of the page.\n\n\n\nFigure caption: Direct download multiple granules\n\n\nWe’re now taken to the final page for instructions to download and links for data access in the cloud. You should see three tabs: Download Files, AWS S3 Access, Download Script:\n\n\n\nFigure caption: Download to local\n\n\n\n\n\nFigure caption: Direct S3 access\n\n\nThe Download Files tab provides the https:// links for downloading the files locally\nThe AWS S3 Access tab provides the S3:// links, which is what we would use to access the data directly in-region (us-west-2) within the AWS cloud.",
    "crumbs": [
      "Guides",
      "Search and Access ECOSTRESS Data using Earthdata Search"
    ]
  },
  {
    "objectID": "python/scripts/extract_geolocation_flag/geolocation.html",
    "href": "python/scripts/extract_geolocation_flag/geolocation.html",
    "title": "Extract ECOSTRESS Geolocation Accuracy QA Flag",
    "section": "",
    "text": "NASA’s Land Processes Distributed Active Archive Center (LP DAAC) is responsible for archiving and distributing ECOSTRESS data products through the LP DAAC Cumulus cloud archive. ECOSTRESS Version 2 data is available in swath, gridded, and tiled formats, provided in both Cloud Optimized GeoTIFF (COG) and HDF5 file formats. ECOSTRESS Swath Geolocation data (ECO_L1B_GEO v002) includes essential metadata fields, such as the Geolocation Accuracy QA flag, which are not carried over to higher-level data products. Geolocation Accuracy QA flag provides an indication of geolocation error. The gelocation accuracy for corrected data could be better than 50 meters but the data was processed without correcting the geolocation could have up to 7 km geolocation error. Users of ECOSTRESS data should evaluate the geolocation accuracy and whether larger errors can be tolerated for their applications. Otherwise, data flagged as “Suspect” or “No Match” may not be suitable for you. Additional details are provided in Section 4.2 of the User Guide.\n\nGeolocation Accuracy QA could have four different flags:\n\n\n\nBest: Image matching was performed for this scene. Good geolocation accuracy.\nGood: Image matching was performed in a nearby scene. Good geolocation accuracy.\nSuspect: Image matching occurred somewhere in the orbit. Expect increased error.\nNo Match: No matches in orbit. Expect large geolocation errors.\n\n\nThis Python-based command-line script enables users to retrieve the Geolocation Accuracy QA flags for the input data. Input data can be provided as:\n1. A list of download URLs or granule filenames stored in a text or CSV file. 2. A download URL or granule filename as a string for a single file.\nThe script automates the retrieval process, allowing users to efficiently determine the geolocation accuracy of their ECOSTRESS data products. Retrieved geolocation flags will be stored as a .csv file.\n\n\n\nThis script works with all the ECOSTRESS version 2 data products.\n\n\n\n\n\nIf you do not have an Environment Manager installed, we recommend mamba to manage Python packages. Details instruction on how to install an environment manager can be found here. Once you have it installed, type the following in your preferred command line interface (command prompt, terminal, cmder, etc.) to create a compatible environment.\nmamba create -n ecostress_geo python=3.12 earthaccess xarray h5netcdf \nThis script is tested using earthaccess version 0.13.\nIf you are using conda, replace the “mamba” with “conda” and be patient.\nInstruction for setting up a compatible environment for all LP DAAC Python resources is available at: https://github.com/nasa/LPDAAC-Data-Resources/blob/main/setup/setup_instructions_python.md\n\n\n\nYou will need a NASA Earthdata Login account to access LP DAAC data (and use this script). To create a NASA Earthdata Login account, go to the Earthdata Login website and click the “Register” button, which is next to the green “Log In” button under the Password entry box. Fill in the required boxes (indicated with a red asterisk), then click on the “Register for Earthdata Login” green button at the bottom of the page. An email including instructions for activating your profile will be sent to you. Activate your profile to complete the registration process.\n\n\n\nThe netrc file is needed to download NASA Earthdata science data from a scripting environment like Python. There are multiple methods to create a .netrc file for authntication. Here, the earthaccess package is used to automatically create a .netrc file using your Earthdata login credentials. If you do not have a netrc file configured in your home directory, the script will prompt you for input on your NASA Earthdata Login Username and Password. Enter your username and password and hit enter to continue downloading your data. Please note that your Earthdata Login info, your username, and password, will be stored in a .netrc file located in the Home directory on this system you are using. You will get the same message when you run the script as a reminder. If you do not trust the machine you are using, make sure to delete the created netrc file.  \n\n\n\n\n\n\n\n\n\n\n\nYou can download the raw file for the script from https://github.com/nasa/ECOSTRESS-Data-Resources/tree/main/python/scripts/extract_geolocation_flag/ECOSTRESS_geolocation.py     Additionally, you can download all contents of this repository as a zip file. You can also clone the repository by typing the following into command line.\ngit clone https://github.com/nasa/ECOSTRESS-Data-Resources.git\nAfterwards, navigate to python/scripts/extract_geolocation_flag/ECOSTRESS_geolocation.py.  \n\n\n\n\n\nActivate your MacOS/Windows environment, run the script with the following in your Command Prompt/terminal window:\npython ECOSTRESS_geolocation.py -dir &lt;insert local directory to save output file to&gt; -f &lt;insert a single granule URL, or the location of a csv or text file containing granule URLs&gt;\nExample 1, extracting the geolocation flag for a list of ECOSTESS files:\npython C:\\User\\Downloads\\ECOSTRESS_geolocation.py -dir C:\\User\\downloads -f C:\\User\\downloads\\ECOSTRESS-granule-list.txt\nExample 2, extracting the geolocation flags for a single ECOSTRESS file:\npython C:\\User\\Downloads\\DAACDataDownload.py  -dir C:\\User\\downloads -f https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L4T_ESI.002/ECOv002_L4T_ESI_36662_013_07LFK_20241223T220254_0713_01/ECOv002_L4T_ESI_36662_013_07LFK_20241223T220254_0713_01_water.tif\n\n\n\n\n\nContact: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)\nWebsite: https://www.earthdata.nasa.gov/centers/lp-daac\nDate last modified: 02-03-2025\n1LP DAAC Work performed under NASA contract NNG14HH33I.",
    "crumbs": [
      "Scripts",
      "Extract ECOSTRESS Geolocation Accuracy QA Flag"
    ]
  },
  {
    "objectID": "python/scripts/extract_geolocation_flag/geolocation.html#objective",
    "href": "python/scripts/extract_geolocation_flag/geolocation.html#objective",
    "title": "Extract ECOSTRESS Geolocation Accuracy QA Flag",
    "section": "",
    "text": "NASA’s Land Processes Distributed Active Archive Center (LP DAAC) is responsible for archiving and distributing ECOSTRESS data products through the LP DAAC Cumulus cloud archive. ECOSTRESS Version 2 data is available in swath, gridded, and tiled formats, provided in both Cloud Optimized GeoTIFF (COG) and HDF5 file formats. ECOSTRESS Swath Geolocation data (ECO_L1B_GEO v002) includes essential metadata fields, such as the Geolocation Accuracy QA flag, which are not carried over to higher-level data products. Geolocation Accuracy QA flag provides an indication of geolocation error. The gelocation accuracy for corrected data could be better than 50 meters but the data was processed without correcting the geolocation could have up to 7 km geolocation error. Users of ECOSTRESS data should evaluate the geolocation accuracy and whether larger errors can be tolerated for their applications. Otherwise, data flagged as “Suspect” or “No Match” may not be suitable for you. Additional details are provided in Section 4.2 of the User Guide.\n\nGeolocation Accuracy QA could have four different flags:\n\n\n\nBest: Image matching was performed for this scene. Good geolocation accuracy.\nGood: Image matching was performed in a nearby scene. Good geolocation accuracy.\nSuspect: Image matching occurred somewhere in the orbit. Expect increased error.\nNo Match: No matches in orbit. Expect large geolocation errors.\n\n\nThis Python-based command-line script enables users to retrieve the Geolocation Accuracy QA flags for the input data. Input data can be provided as:\n1. A list of download URLs or granule filenames stored in a text or CSV file. 2. A download URL or granule filename as a string for a single file.\nThe script automates the retrieval process, allowing users to efficiently determine the geolocation accuracy of their ECOSTRESS data products. Retrieved geolocation flags will be stored as a .csv file.",
    "crumbs": [
      "Scripts",
      "Extract ECOSTRESS Geolocation Accuracy QA Flag"
    ]
  },
  {
    "objectID": "python/scripts/extract_geolocation_flag/geolocation.html#products",
    "href": "python/scripts/extract_geolocation_flag/geolocation.html#products",
    "title": "Extract ECOSTRESS Geolocation Accuracy QA Flag",
    "section": "",
    "text": "This script works with all the ECOSTRESS version 2 data products.",
    "crumbs": [
      "Scripts",
      "Extract ECOSTRESS Geolocation Accuracy QA Flag"
    ]
  },
  {
    "objectID": "python/scripts/extract_geolocation_flag/geolocation.html#prerequisitessetup-instructions",
    "href": "python/scripts/extract_geolocation_flag/geolocation.html#prerequisitessetup-instructions",
    "title": "Extract ECOSTRESS Geolocation Accuracy QA Flag",
    "section": "",
    "text": "If you do not have an Environment Manager installed, we recommend mamba to manage Python packages. Details instruction on how to install an environment manager can be found here. Once you have it installed, type the following in your preferred command line interface (command prompt, terminal, cmder, etc.) to create a compatible environment.\nmamba create -n ecostress_geo python=3.12 earthaccess xarray h5netcdf \nThis script is tested using earthaccess version 0.13.\nIf you are using conda, replace the “mamba” with “conda” and be patient.\nInstruction for setting up a compatible environment for all LP DAAC Python resources is available at: https://github.com/nasa/LPDAAC-Data-Resources/blob/main/setup/setup_instructions_python.md\n\n\n\nYou will need a NASA Earthdata Login account to access LP DAAC data (and use this script). To create a NASA Earthdata Login account, go to the Earthdata Login website and click the “Register” button, which is next to the green “Log In” button under the Password entry box. Fill in the required boxes (indicated with a red asterisk), then click on the “Register for Earthdata Login” green button at the bottom of the page. An email including instructions for activating your profile will be sent to you. Activate your profile to complete the registration process.\n\n\n\nThe netrc file is needed to download NASA Earthdata science data from a scripting environment like Python. There are multiple methods to create a .netrc file for authntication. Here, the earthaccess package is used to automatically create a .netrc file using your Earthdata login credentials. If you do not have a netrc file configured in your home directory, the script will prompt you for input on your NASA Earthdata Login Username and Password. Enter your username and password and hit enter to continue downloading your data. Please note that your Earthdata Login info, your username, and password, will be stored in a .netrc file located in the Home directory on this system you are using. You will get the same message when you run the script as a reminder. If you do not trust the machine you are using, make sure to delete the created netrc file.",
    "crumbs": [
      "Scripts",
      "Extract ECOSTRESS Geolocation Accuracy QA Flag"
    ]
  },
  {
    "objectID": "python/scripts/extract_geolocation_flag/geolocation.html#procedures",
    "href": "python/scripts/extract_geolocation_flag/geolocation.html#procedures",
    "title": "Extract ECOSTRESS Geolocation Accuracy QA Flag",
    "section": "",
    "text": "You can download the raw file for the script from https://github.com/nasa/ECOSTRESS-Data-Resources/tree/main/python/scripts/extract_geolocation_flag/ECOSTRESS_geolocation.py     Additionally, you can download all contents of this repository as a zip file. You can also clone the repository by typing the following into command line.\ngit clone https://github.com/nasa/ECOSTRESS-Data-Resources.git\nAfterwards, navigate to python/scripts/extract_geolocation_flag/ECOSTRESS_geolocation.py.",
    "crumbs": [
      "Scripts",
      "Extract ECOSTRESS Geolocation Accuracy QA Flag"
    ]
  },
  {
    "objectID": "python/scripts/extract_geolocation_flag/geolocation.html#script-execution",
    "href": "python/scripts/extract_geolocation_flag/geolocation.html#script-execution",
    "title": "Extract ECOSTRESS Geolocation Accuracy QA Flag",
    "section": "",
    "text": "Activate your MacOS/Windows environment, run the script with the following in your Command Prompt/terminal window:\npython ECOSTRESS_geolocation.py -dir &lt;insert local directory to save output file to&gt; -f &lt;insert a single granule URL, or the location of a csv or text file containing granule URLs&gt;\nExample 1, extracting the geolocation flag for a list of ECOSTESS files:\npython C:\\User\\Downloads\\ECOSTRESS_geolocation.py -dir C:\\User\\downloads -f C:\\User\\downloads\\ECOSTRESS-granule-list.txt\nExample 2, extracting the geolocation flags for a single ECOSTRESS file:\npython C:\\User\\Downloads\\DAACDataDownload.py  -dir C:\\User\\downloads -f https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L4T_ESI.002/ECOv002_L4T_ESI_36662_013_07LFK_20241223T220254_0713_01/ECOv002_L4T_ESI_36662_013_07LFK_20241223T220254_0713_01_water.tif",
    "crumbs": [
      "Scripts",
      "Extract ECOSTRESS Geolocation Accuracy QA Flag"
    ]
  },
  {
    "objectID": "python/scripts/extract_geolocation_flag/geolocation.html#contact-information",
    "href": "python/scripts/extract_geolocation_flag/geolocation.html#contact-information",
    "title": "Extract ECOSTRESS Geolocation Accuracy QA Flag",
    "section": "",
    "text": "Contact: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)\nWebsite: https://www.earthdata.nasa.gov/centers/lp-daac\nDate last modified: 02-03-2025\n1LP DAAC Work performed under NASA contract NNG14HH33I.",
    "crumbs": [
      "Scripts",
      "Extract ECOSTRESS Geolocation Accuracy QA Flag"
    ]
  },
  {
    "objectID": "python/how-tos/how_to_direct_access_s3_ecostress_cog.html",
    "href": "python/how-tos/how_to_direct_access_s3_ecostress_cog.html",
    "title": "How to: Directly Access ECOSTRESS Data (S3)",
    "section": "",
    "text": "NASA Earthdata Cloud data is stored in an Amazon Web Services (AWS) Simple Storage Service (S3) bucket. Data located within these buckets can be directly accessed via temporary credentials; this access is limited to requests made within the US West (Oregon) (code: us-west-2) AWS region. In this notebook, we will access data for the ECOSTRESS Tiled Land Surface Temperature and Emissivity Instantaneous L2 Global 70 m V002 data product. These data are archived and distributed as Cloud Optimized GeoTIFF (COG) files, one file for each spectral band. We will access a single COG file, Land Surface Temperature (LST), directly loading it into memory from inside the AWS cloud (us-west-2 region, specifically). To accomplish this we will retrieve temporary AWS credentials and load the data directly to memory as an xarray dataarray. This approach leverages native protocols for efficient access to the data directly in its S3 bucket.\nBackground\nThe ECOSTRESS mission is answering these questions by accurately measuring the temperature of plants. Plants regulate their temperature by releasing water through tiny pores on their leaves called stomata. If they have sufficient water they can maintain their temperature, but if there is insufficient water, their temperatures rise and this temperature rise can be measured with ECOSTRESS. The images acquired by ECOSTRESS are the most detailed temperature images of the surface ever acquired from space and can be used to measure the temperature of an individual farmers field. These temperature images, along with auxiliary inputs, are used to produce one of the primary science outputs of ECOSTRESS: evapotranspiration, an indicator of plant health via the measure of evaporation and transpiration of water through a plant.\nLearning Objectives\nRequirements\nOutline 1. Setup 2. Load file directly to memory 3. Visualize the data",
    "crumbs": [
      "Python Notebooks",
      "How to Directly Access ECOSTRESS Data (S3)"
    ]
  },
  {
    "objectID": "python/how-tos/how_to_direct_access_s3_ecostress_cog.html#setup",
    "href": "python/how-tos/how_to_direct_access_s3_ecostress_cog.html#setup",
    "title": "How to: Directly Access ECOSTRESS Data (S3)",
    "section": "1. Setup",
    "text": "1. Setup\nImport the required libraries.\n\nimport os\nimport boto3\nimport rasterio as rio\nfrom rasterio.session import AWSSession\nimport rioxarray as rxr\nimport hvplot.xarray\nimport earthaccess\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\nAuthentication and Temporary AWS Credentials\nLog into Earthdata using the login functions from the earthaccess library. The persist=True argument will create a local .netrc file if it doesn’t exist, or add your login info to an existing .netrc file. If no Earthdata Login credentials are found in the .netrc you’ll be prompted for them. After signing into Earthdata, these credentials can be used to get temporary AWS credentials so we can interact with S3 objects from applicable Earthdata Cloud buckets. Each NASA data center has different AWS credentials endpoints. The earthaccess library can be used to retrieve credentials by just providing a data center name (e.g. ‘podaac’,‘gesdisc’,‘lpdaac’,‘ornldaac’,‘ghrcdaac’). In this case, ECOSTRESS data is archived by the LP DAAC (“lpdaac”), so we’ll want those temporary credentials.\n\n# Log into Earthdata\nauth = earthaccess.login(persist = True)\n\n\n# Retrieve Temporary Credentials for LP DAAC Data\ntemp_creds_req = earthaccess.get_s3_credentials(daac='lpdaac')\n\nAlternatively, we can manually define S3 credential endpoints and make a request using the requests python library to retrive them. This process will use the login info from the .netrc file. Uncomment and use below if you prefer.\n\n# import requests\n# s3_cred_endpoint = {\n#     'podaac':'https://archive.podaac.earthdata.nasa.gov/s3credentials',\n#     'gesdisc': 'https://data.gesdisc.earthdata.nasa.gov/s3credentials',\n#     'lpdaac':'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials',\n#     'ornldaac': 'https://data.ornldaac.earthdata.nasa.gov/s3credentials',\n#     'ghrcdaac': 'https://data.ghrc.earthdata.nasa.gov/s3credentials'\n# }\n# def get_temp_creds(provider):\n#     return requests.get(s3_cred_endpoint[provider]).json()\n# temp_creds_req = get_temp_creds('lpdaac')\n\nCreate a boto3 Session object using your temporary credentials. This Session is used to pass credentials and configuration to AWS so we can interact with S3 objects from applicable buckets.\n\nsession = boto3.Session(aws_access_key_id=temp_creds_req['accessKeyId'], \n                        aws_secret_access_key=temp_creds_req['secretAccessKey'],\n                        aws_session_token=temp_creds_req['sessionToken'],\n                        region_name='us-west-2')\n\n\n\nContext Manager\nFor this exercise, we are going to open up a context manager for the notebook using the rasterio.env module to store the required GDAL configurations we need to access the data from Earthdata Cloud. The context manager sends the authentication information when connecting to a file and can also customize how the file is handled locally. The GDAL environment variables must be configured to access COGs in Earthdata Cloud. Geospatial data access Python packages like rasterio and rioxarray depend on GDAL, leveraging GDAL’s “Virtual File Systems” to read remote files. GDAL has a lot of environment variables that control its behavior. Changing these settings can mean the difference between being able to access a file or not. They can also have an impact on the performance. Please see the GDAL config options documentation for more details and all available options.\nWhile the context manager is open (rio_env.__enter__()) we will be able to run commands that open or get data that would typically be executed within a “with” statement. Entering the context manager for the entirety of the notebook allows us to more freely interact with the data. We’ll close the context manager (rio_env.__exit__()) at the end of the notebook.\n\nrio_env = rio.Env(AWSSession(session),\n                  GDAL_DISABLE_READDIR_ON_OPEN='TRUE',\n                  GDAL_HTTP_COOKIEFILE=os.path.expanduser('~/cookies.txt'),\n                  GDAL_HTTP_COOKIEJAR=os.path.expanduser('~/cookies.txt'),\n                  GDAL_HTTP_MAX_RETRY=10,\n                  GDAL_HTTP_RETRY_DELAY=0.5)\nrio_env.__enter__()\n\n&lt;rasterio.env.Env at 0x7f9b258a9850&gt;\n\n\nAbove, GDAL_HTTP_COOKIEFILE and GDAL_HTTP_COOKIEJAR tell GDAL to use a cookie for authentication and where to find that cookie. GDAL_DISABLE_READDIR_ON_OPEN tells gdal not to look for any auxiliary or sidecar files in the directory, which can slow down access. GDAL_HTTP_MAX_RETRY and GDAL_HTTP_RETRY_DELAY tell GDAL to retry the connection a number of times and how long to wait before retrying. These are nice options to add in the case that a connection fails temporarily, and will allow the workflow to continue without re-running.",
    "crumbs": [
      "Python Notebooks",
      "How to Directly Access ECOSTRESS Data (S3)"
    ]
  },
  {
    "objectID": "python/how-tos/how_to_direct_access_s3_ecostress_cog.html#load-the-file-directly-into-memory-s3",
    "href": "python/how-tos/how_to_direct_access_s3_ecostress_cog.html#load-the-file-directly-into-memory-s3",
    "title": "How to: Directly Access ECOSTRESS Data (S3)",
    "section": "2. Load the File Directly into Memory (S3)",
    "text": "2. Load the File Directly into Memory (S3)\nIn this example we’re interested in the ECOSTRESS data collection from NASA’s LP DAAC in Earthdata Cloud. Below we specify the URL to the data asset in Earthdata Cloud. This URL can be found via Earthdata Search or programmatically through earthaccess, the CMR API or CMR-STAC API. There are programmatic examples in the Python tutorials for ECOSTRESS, and an earthdata search example available as well.\n\ns3_url_lst = 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_LST.tif'\n\nRead in the ECOSTRESS LSt URL into our workspace using rioxarray. This utilizes the context manager that we have entered. Optionally we can use the mask_and_scale argument to mask and apply the scale and offset values for the data.\n\nda = rxr.open_rasterio(s3_url_lst)\nda\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (band: 1, y: 1568, x: 1568)&gt; Size: 10MB\n[2458624 values with dtype=float32]\nCoordinates:\n  * band         (band) int64 8B 1\n  * x            (x) float64 13kB 2e+05 2.001e+05 ... 3.096e+05 3.097e+05\n  * y            (y) float64 13kB 3.9e+06 3.9e+06 3.9e+06 ... 3.79e+06 3.79e+06\n    spatial_ref  int64 8B 0\nAttributes:\n    AREA_OR_POINT:  Area\n    _FillValue:     nan\n    scale_factor:   1.0\n    add_offset:     0.0xarray.DataArrayband: 1y: 1568x: 1568...[2458624 values with dtype=float32]Coordinates: (4)band(band)int641array([1])x(x)float642e+05 2.001e+05 ... 3.097e+05array([200015., 200085., 200155., ..., 309565., 309635., 309705.])y(y)float643.9e+06 3.9e+06 ... 3.79e+06array([3899965., 3899895., 3899825., ..., 3790415., 3790345., 3790275.])spatial_ref()int640crs_wkt :PROJCS[\"WGS 84 / UTM zone 11N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984projected_crs_name :WGS 84 / UTM zone 11Ngrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-117.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"WGS 84 / UTM zone 11N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :199980.0 70.0 0.0 3900000.0 0.0 -70.0array(0)Indexes: (3)bandPandasIndexPandasIndex(Index([1], dtype='int64', name='band'))xPandasIndexPandasIndex(Index([200015.0, 200085.0, 200155.0, 200225.0, 200295.0, 200365.0, 200435.0,\n       200505.0, 200575.0, 200645.0,\n       ...\n       309075.0, 309145.0, 309215.0, 309285.0, 309355.0, 309425.0, 309495.0,\n       309565.0, 309635.0, 309705.0],\n      dtype='float64', name='x', length=1568))yPandasIndexPandasIndex(Index([3899965.0, 3899895.0, 3899825.0, 3899755.0, 3899685.0, 3899615.0,\n       3899545.0, 3899475.0, 3899405.0, 3899335.0,\n       ...\n       3790905.0, 3790835.0, 3790765.0, 3790695.0, 3790625.0, 3790555.0,\n       3790485.0, 3790415.0, 3790345.0, 3790275.0],\n      dtype='float64', name='y', length=1568))Attributes: (4)AREA_OR_POINT :Area_FillValue :nanscale_factor :1.0add_offset :0.0\n\n\nThe file is read into Python as an xarray dataarray with a band, x, and y dimension. In this example the band dimension is meaningless, so we’ll use the squeeze() function to remove band as a dimension.\n\nda_lst = da.squeeze('band', drop=True)\nda_lst\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (y: 1568, x: 1568)&gt; Size: 10MB\n[2458624 values with dtype=float32]\nCoordinates:\n  * x            (x) float64 13kB 2e+05 2.001e+05 ... 3.096e+05 3.097e+05\n  * y            (y) float64 13kB 3.9e+06 3.9e+06 3.9e+06 ... 3.79e+06 3.79e+06\n    spatial_ref  int64 8B 0\nAttributes:\n    AREA_OR_POINT:  Area\n    _FillValue:     nan\n    scale_factor:   1.0\n    add_offset:     0.0xarray.DataArrayy: 1568x: 1568...[2458624 values with dtype=float32]Coordinates: (3)x(x)float642e+05 2.001e+05 ... 3.097e+05array([200015., 200085., 200155., ..., 309565., 309635., 309705.])y(y)float643.9e+06 3.9e+06 ... 3.79e+06array([3899965., 3899895., 3899825., ..., 3790415., 3790345., 3790275.])spatial_ref()int640crs_wkt :PROJCS[\"WGS 84 / UTM zone 11N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984projected_crs_name :WGS 84 / UTM zone 11Ngrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-117.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"WGS 84 / UTM zone 11N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :199980.0 70.0 0.0 3900000.0 0.0 -70.0array(0)Indexes: (2)xPandasIndexPandasIndex(Index([200015.0, 200085.0, 200155.0, 200225.0, 200295.0, 200365.0, 200435.0,\n       200505.0, 200575.0, 200645.0,\n       ...\n       309075.0, 309145.0, 309215.0, 309285.0, 309355.0, 309425.0, 309495.0,\n       309565.0, 309635.0, 309705.0],\n      dtype='float64', name='x', length=1568))yPandasIndexPandasIndex(Index([3899965.0, 3899895.0, 3899825.0, 3899755.0, 3899685.0, 3899615.0,\n       3899545.0, 3899475.0, 3899405.0, 3899335.0,\n       ...\n       3790905.0, 3790835.0, 3790765.0, 3790695.0, 3790625.0, 3790555.0,\n       3790485.0, 3790415.0, 3790345.0, 3790275.0],\n      dtype='float64', name='y', length=1568))Attributes: (4)AREA_OR_POINT :Area_FillValue :nanscale_factor :1.0add_offset :0.0",
    "crumbs": [
      "Python Notebooks",
      "How to Directly Access ECOSTRESS Data (S3)"
    ]
  },
  {
    "objectID": "python/how-tos/how_to_direct_access_s3_ecostress_cog.html#visualize-the-data",
    "href": "python/how-tos/how_to_direct_access_s3_ecostress_cog.html#visualize-the-data",
    "title": "How to: Directly Access ECOSTRESS Data (S3)",
    "section": "3. Visualize the Data",
    "text": "3. Visualize the Data\nPlot the dataarray, representing the ECOSTRESS band, using hvplot. Since ECOSTRESS tiles are in UTM projections, to visualize this with a basemap tile, we’ll need to reproject to EPSG:4326 for the visual. This can be accomplished using the rio.reproject() function.\n\nda_lst_reproj = da_lst.rio.reproject(\"EPSG:4326\")\n\n\nda_lst_reproj.hvplot.image(x = 'x',\n                           y = 'y',\n                           crs = 'EPSG:4326',\n                           cmap='jet',\n                           tiles='EsriImagery',\n                           title = f'{s3_url_lst.split(\"/\")[-1]}',\n                           frame_width=500)\n\n\n\n\n\n  \n\n\n\n\nExit the context manager.\n\nrio_env.__exit__()",
    "crumbs": [
      "Python Notebooks",
      "How to Directly Access ECOSTRESS Data (S3)"
    ]
  },
  {
    "objectID": "python/how-tos/how_to_direct_access_s3_ecostress_cog.html#contact-info",
    "href": "python/how-tos/how_to_direct_access_s3_ecostress_cog.html#contact-info",
    "title": "How to: Directly Access ECOSTRESS Data (S3)",
    "section": "Contact Info:",
    "text": "Contact Info:\nEmail: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)¹\nWebsite: https://www.earthdata.nasa.gov/centers/lp-daac\n¹Work performed under USGS contract G15PD00467 for NASA contract NNG14HH33I.",
    "crumbs": [
      "Python Notebooks",
      "How to Directly Access ECOSTRESS Data (S3)"
    ]
  },
  {
    "objectID": "python/tutorials/Working_with_ECOSTRESS_Tiled_data.html",
    "href": "python/tutorials/Working_with_ECOSTRESS_Tiled_data.html",
    "title": "Exploring ECOSTRESS Tiled Data",
    "section": "",
    "text": "Summary\nThis notebook will show how to access ECOsystem Spaceborne Thermal Radiometer Experiment on Space Station (ECOSTRESS) data hosted in the Earthdata Cloud programmatically using the earthaccess python library. It shows how to authenticate, search, review browse images, download, and stream (directly access) data. It also shows ways to review the tiled data for quality.\nLearning Objectives\nRequirements\nTutorial Outline",
    "crumbs": [
      "Python Notebooks",
      "Explore the ECOSTRESS Tiled Data"
    ]
  },
  {
    "objectID": "python/tutorials/Working_with_ECOSTRESS_Tiled_data.html#setup",
    "href": "python/tutorials/Working_with_ECOSTRESS_Tiled_data.html#setup",
    "title": "Exploring ECOSTRESS Tiled Data",
    "section": "1. Setup",
    "text": "1. Setup\nImport the required libraries.\n\n# Import Packages\nimport warnings\n# Some cells may generate warnings that we can ignore. Comment below lines to see.\nwarnings.filterwarnings('ignore')\n\nimport os, sys, shutil\nimport earthaccess\nimport numpy as np\nimport geopandas as gp\nimport pandas as pd\n\nfrom osgeo import gdal\nimport rasterio as rio\nimport rioxarray as rxr\nimport xarray as xr\nimport hvplot.xarray\nimport hvplot.pandas\n\nfrom zipfile import ZipFile \n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAuthentication\nLog into Earthdata using the Auth and login functions from the earthaccess library. The persist=True argument will create a local .netrc file if it doesn’t exist, or add your login info to an existing .netrc file. If no Earthdata Login credentials are found in the .netrc you’ll be prompted for them.\n\nauth = earthaccess.login(persist = True)\n# are we authenticated?\nprint(auth.authenticated)\n\nTrue",
    "crumbs": [
      "Python Notebooks",
      "Explore the ECOSTRESS Tiled Data"
    ]
  },
  {
    "objectID": "python/tutorials/Working_with_ECOSTRESS_Tiled_data.html#searching-for-ecostress-tiled-data",
    "href": "python/tutorials/Working_with_ECOSTRESS_Tiled_data.html#searching-for-ecostress-tiled-data",
    "title": "Exploring ECOSTRESS Tiled Data",
    "section": "2. Searching for ECOSTRESS Tiled Data",
    "text": "2. Searching for ECOSTRESS Tiled Data\nIn this example, we will use cloud-hosted ECOSTRESS Tiled data, but the same search process can be applied to other NASA Earth data collections from different providers. The ECOSTRESS tiled data product uses a modified version of the Military Grid Reference System (MGRS) which divides Universal Transverse Mercator (UTM) zones into square tiles that are 109.8 km by 109.8 km with a 70 meter (m) spatial resolution. This allows the end user to assume that each 70 m ECOSTRESS pixel will remain in the same location at each timestep observed in analysis (tiled data User Guide.\nWe will use the earthaccess search_data function to search for ECOSTRESS granules (tiles). By providing query parameters such as collection short name, version, collection ID, acquisition time, and spatial region of interest (ROI), you can filter results to only those granules that meet your criteria. In this tutorial, we’ll go straight into searching the data. For more details on additional search methods, check the earthaccess introduction tutorial.\n\nRegion of Interest (ROI)\nIn this exercise, you can specify your Region of Interest (ROI) in one of two ways: - Draw a bounding box or polygon in geojson.io and save it as a GeoJSON or Shapefile. Store the file in /data folder. - The polygon argument accepts a list of vertices in counter-clockwise order. - Provide a tuple with coordinates in the following order: (lower_left_lon, lower_left_lat, upper_right_lon, upper_right_lat) - bbox = (-156.08325817024362, 18.884416413037513, -154.7956649879457, 20.289321000519536) #### Temporal Period Pass a tuple containing the start and end datetime, each in ISO-8601 format: - YYYY-MM-DDTHH:mm:ss - YYYY-MM-DDTHH:mm - YYYY-MM-DD\n\ngdf = gp.read_file('../../data/hawaii.geojson')\n\n# Get the first geometry\npoly = gdf.geometry.iloc[0]\n\n# Extract exterior coordinates\nexterior_coords = list(poly.exterior.coords)\nexterior_coords\n\n[(-156.08325817024362, 19.761648519502998),\n (-155.94536162626957, 19.055265378036196),\n (-155.68912658483336, 18.884416413037513),\n (-154.7956649879457, 19.49868320811794),\n (-155.15701319757088, 19.977862511765977),\n (-155.88944801467946, 20.289321000519536),\n (-156.08325817024362, 19.761648519502998)]\n\n\n\ngdf.hvplot(tiles='ESRI', color='#d95f02',alpha=0.6, crs='EPSG:4326', frame_height=405, frame_width=720, fontscale=2)\n\n\n\n\n\n  \n\n\n\n\n\nbbox = tuple(list(gdf.total_bounds))\n\nNow let’s proceed to search for the granules. We’ll use the collection short name (ECO_L2T_LSTE) and version (002) to query the ECOSTRESS Tiled LSTE product. You can find the collection information using Earthdata Search, the dataset landing page, or through a collection search of the Common Metadata Repository (CMR).\nYou can replace polygon=polygon_coords with bounding_box = bbox for bounding box OR with point = (-156.08325817024362, 18.884416413037513) for point query.\n\nresults = earthaccess.search_data(\n    short_name='ECO_L2T_LSTE',\n    version='002',\n    provider='LPCLOUD',\n    polygon=exterior_coords,            # Replace with `bounding_box = bbox` OR point = (-156.08325817024362, 18.884416413037513)\n    temporal=('2023-05-01','2023-05-10'),\n)\nprint(f'Granules found: {len(results)}')\n\nGranules found: 42\n\n\nWe will look at the results from the first granule returned.\n\nresults[0]\n\n\n    \n            \n            \n    \n      \n        \n          \n            Data: ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_water.tifECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_cloud.tifECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_view_zenith.tifECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_height.tifECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_QC.tifECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_LST.tifECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_LST_err.tifECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_EmisWB.tif\n            Size: 1.71 MB\n            Cloud Hosted: True\n          \n          \n            \n          \n        \n      \n    \n    \n\n\n\nresults[0]['umm']\n\n{'TemporalExtent': {'RangeDateTime': {'BeginningDateTime': '2023-05-03T19:05:17.740Z',\n   'EndingDateTime': '2023-05-03T19:06:09.710Z'}},\n 'OrbitCalculatedSpatialDomains': [{'BeginOrbitNumber': 27355,\n   'EndOrbitNumber': 27355}],\n 'GranuleUR': 'ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01',\n 'AdditionalAttributes': [{'Name': 'identifier_product_doi',\n   'Values': ['10.5067/ECOSTRESS/ECO_L2T_LSTE.002']},\n  {'Name': 'identifier_product_doi_authority', 'Values': ['http://doi.org']}],\n 'MeasuredParameters': [{'ParameterName': 'L2T_LSTE'}],\n 'SpatialExtent': {'HorizontalSpatialDomain': {'Geometry': {'BoundingRectangles': [{'WestBoundingCoordinate': -157.091125,\n      'EastBoundingCoordinate': -156.025345,\n      'NorthBoundingCoordinate': 20.789488,\n      'SouthBoundingCoordinate': 19.783957}]}}},\n 'ProviderDates': [{'Date': '2023-06-15T22:49:27.567Z', 'Type': 'Insert'},\n  {'Date': '2023-06-15T22:49:59.950Z', 'Type': 'Update'}],\n 'CollectionReference': {'ShortName': 'ECO_L2T_LSTE', 'Version': '002'},\n 'PGEVersionClass': {'PGEVersion': 'v1.6.2'},\n 'RelatedUrls': [{'URL': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_water.tif',\n   'Type': 'GET DATA',\n   'Description': 'Download ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_water.tif'},\n  {'URL': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_water.tif',\n   'Type': 'GET DATA VIA DIRECT ACCESS',\n   'Description': 'This link provides direct download access via S3 to the granule'},\n  {'URL': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_cloud.tif',\n   'Type': 'GET DATA',\n   'Description': 'Download ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_cloud.tif'},\n  {'URL': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_cloud.tif',\n   'Type': 'GET DATA VIA DIRECT ACCESS',\n   'Description': 'This link provides direct download access via S3 to the granule'},\n  {'URL': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_view_zenith.tif',\n   'Type': 'GET DATA',\n   'Description': 'Download ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_view_zenith.tif'},\n  {'URL': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_view_zenith.tif',\n   'Type': 'GET DATA VIA DIRECT ACCESS',\n   'Description': 'This link provides direct download access via S3 to the granule'},\n  {'URL': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_height.tif',\n   'Type': 'GET DATA',\n   'Description': 'Download ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_height.tif'},\n  {'URL': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_height.tif',\n   'Type': 'GET DATA VIA DIRECT ACCESS',\n   'Description': 'This link provides direct download access via S3 to the granule'},\n  {'URL': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_QC.tif',\n   'Type': 'GET DATA',\n   'Description': 'Download ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_QC.tif'},\n  {'URL': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_QC.tif',\n   'Type': 'GET DATA VIA DIRECT ACCESS',\n   'Description': 'This link provides direct download access via S3 to the granule'},\n  {'URL': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_LST.tif',\n   'Type': 'GET DATA',\n   'Description': 'Download ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_LST.tif'},\n  {'URL': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_LST.tif',\n   'Type': 'GET DATA VIA DIRECT ACCESS',\n   'Description': 'This link provides direct download access via S3 to the granule'},\n  {'URL': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_LST_err.tif',\n   'Type': 'GET DATA',\n   'Description': 'Download ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_LST_err.tif'},\n  {'URL': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_LST_err.tif',\n   'Type': 'GET DATA VIA DIRECT ACCESS',\n   'Description': 'This link provides direct download access via S3 to the granule'},\n  {'URL': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_EmisWB.tif',\n   'Type': 'GET DATA',\n   'Description': 'Download ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_EmisWB.tif'},\n  {'URL': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_EmisWB.tif',\n   'Type': 'GET DATA VIA DIRECT ACCESS',\n   'Description': 'This link provides direct download access via S3 to the granule'},\n  {'URL': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01.json',\n   'Type': 'VIEW RELATED INFORMATION',\n   'Description': 'Download ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01.json'},\n  {'URL': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01.json',\n   'Type': 'VIEW RELATED INFORMATION',\n   'Description': 'This link provides direct download access via S3 to the granule'},\n  {'URL': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01.cmr.xml',\n   'Type': 'VIEW RELATED INFORMATION',\n   'Description': 'Download ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01.cmr.xml'},\n  {'URL': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01.cmr.xml',\n   'Type': 'VIEW RELATED INFORMATION',\n   'Description': 'This link provides direct download access via S3 to the granule'},\n  {'URL': 'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials',\n   'Type': 'VIEW RELATED INFORMATION',\n   'Description': 'api endpoint to retrieve temporary credentials valid for same-region direct s3 access'},\n  {'URL': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-public/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01.png',\n   'Type': 'GET RELATED VISUALIZATION',\n   'Description': 'Download ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01.png'},\n  {'URL': 's3://lp-prod-public/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01.png',\n   'Type': 'GET RELATED VISUALIZATION',\n   'Description': 'This link provides direct download access via S3 to the granule'},\n  {'URL': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-public/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_water.jpeg',\n   'Type': 'GET RELATED VISUALIZATION',\n   'Description': 'Download ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_water.jpeg'},\n  {'URL': 's3://lp-prod-public/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_water.jpeg',\n   'Type': 'GET RELATED VISUALIZATION',\n   'Description': 'This link provides direct download access via S3 to the granule'},\n  {'URL': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-public/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_water.jpeg.aux.xml',\n   'Type': 'GET RELATED VISUALIZATION',\n   'Description': 'Download ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_water.jpeg.aux.xml'},\n  {'URL': 's3://lp-prod-public/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_water.jpeg.aux.xml',\n   'Type': 'GET RELATED VISUALIZATION',\n   'Description': 'This link provides direct download access via S3 to the granule'},\n  {'URL': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-public/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_cloud.jpeg',\n   'Type': 'GET RELATED VISUALIZATION',\n   'Description': 'Download ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_cloud.jpeg'},\n  {'URL': 's3://lp-prod-public/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_cloud.jpeg',\n   'Type': 'GET RELATED VISUALIZATION',\n   'Description': 'This link provides direct download access via S3 to the granule'},\n  {'URL': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-public/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_cloud.jpeg.aux.xml',\n   'Type': 'GET RELATED VISUALIZATION',\n   'Description': 'Download ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_cloud.jpeg.aux.xml'},\n  {'URL': 's3://lp-prod-public/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_cloud.jpeg.aux.xml',\n   'Type': 'GET RELATED VISUALIZATION',\n   'Description': 'This link provides direct download access via S3 to the granule'},\n  {'URL': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-public/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_view_zenith.jpeg',\n   'Type': 'GET RELATED VISUALIZATION',\n   'Description': 'Download ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_view_zenith.jpeg'},\n  {'URL': 's3://lp-prod-public/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_view_zenith.jpeg',\n   'Type': 'GET RELATED VISUALIZATION',\n   'Description': 'This link provides direct download access via S3 to the granule'},\n  {'URL': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-public/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_view_zenith.jpeg.aux.xml',\n   'Type': 'GET RELATED VISUALIZATION',\n   'Description': 'Download ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_view_zenith.jpeg.aux.xml'},\n  {'URL': 's3://lp-prod-public/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_view_zenith.jpeg.aux.xml',\n   'Type': 'GET RELATED VISUALIZATION',\n   'Description': 'This link provides direct download access via S3 to the granule'},\n  {'URL': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-public/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_height.jpeg',\n   'Type': 'GET RELATED VISUALIZATION',\n   'Description': 'Download ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_height.jpeg'},\n  {'URL': 's3://lp-prod-public/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_height.jpeg',\n   'Type': 'GET RELATED VISUALIZATION',\n   'Description': 'This link provides direct download access via S3 to the granule'},\n  {'URL': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-public/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_height.jpeg.aux.xml',\n   'Type': 'GET RELATED VISUALIZATION',\n   'Description': 'Download ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_height.jpeg.aux.xml'},\n  {'URL': 's3://lp-prod-public/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_height.jpeg.aux.xml',\n   'Type': 'GET RELATED VISUALIZATION',\n   'Description': 'This link provides direct download access via S3 to the granule'},\n  {'URL': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-public/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_QC.jpeg',\n   'Type': 'GET RELATED VISUALIZATION',\n   'Description': 'Download ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_QC.jpeg'},\n  {'URL': 's3://lp-prod-public/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_QC.jpeg',\n   'Type': 'GET RELATED VISUALIZATION',\n   'Description': 'This link provides direct download access via S3 to the granule'},\n  {'URL': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-public/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_QC.jpeg.aux.xml',\n   'Type': 'GET RELATED VISUALIZATION',\n   'Description': 'Download ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_QC.jpeg.aux.xml'},\n  {'URL': 's3://lp-prod-public/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_QC.jpeg.aux.xml',\n   'Type': 'GET RELATED VISUALIZATION',\n   'Description': 'This link provides direct download access via S3 to the granule'},\n  {'URL': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-public/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_LST.jpeg',\n   'Type': 'GET RELATED VISUALIZATION',\n   'Description': 'Download ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_LST.jpeg'},\n  {'URL': 's3://lp-prod-public/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_LST.jpeg',\n   'Type': 'GET RELATED VISUALIZATION',\n   'Description': 'This link provides direct download access via S3 to the granule'},\n  {'URL': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-public/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_LST.jpeg.aux.xml',\n   'Type': 'GET RELATED VISUALIZATION',\n   'Description': 'Download ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_LST.jpeg.aux.xml'},\n  {'URL': 's3://lp-prod-public/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_LST.jpeg.aux.xml',\n   'Type': 'GET RELATED VISUALIZATION',\n   'Description': 'This link provides direct download access via S3 to the granule'},\n  {'URL': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-public/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_LST_err.jpeg',\n   'Type': 'GET RELATED VISUALIZATION',\n   'Description': 'Download ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_LST_err.jpeg'},\n  {'URL': 's3://lp-prod-public/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_LST_err.jpeg',\n   'Type': 'GET RELATED VISUALIZATION',\n   'Description': 'This link provides direct download access via S3 to the granule'},\n  {'URL': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-public/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_LST_err.jpeg.aux.xml',\n   'Type': 'GET RELATED VISUALIZATION',\n   'Description': 'Download ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_LST_err.jpeg.aux.xml'},\n  {'URL': 's3://lp-prod-public/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_LST_err.jpeg.aux.xml',\n   'Type': 'GET RELATED VISUALIZATION',\n   'Description': 'This link provides direct download access via S3 to the granule'},\n  {'URL': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-public/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_EmisWB.jpeg',\n   'Type': 'GET RELATED VISUALIZATION',\n   'Description': 'Download ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_EmisWB.jpeg'},\n  {'URL': 's3://lp-prod-public/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_EmisWB.jpeg',\n   'Type': 'GET RELATED VISUALIZATION',\n   'Description': 'This link provides direct download access via S3 to the granule'},\n  {'URL': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-public/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_EmisWB.jpeg.aux.xml',\n   'Type': 'GET RELATED VISUALIZATION',\n   'Description': 'Download ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_EmisWB.jpeg.aux.xml'},\n  {'URL': 's3://lp-prod-public/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01/ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01_EmisWB.jpeg.aux.xml',\n   'Type': 'GET RELATED VISUALIZATION',\n   'Description': 'This link provides direct download access via S3 to the granule'}],\n 'DataGranule': {'DayNightFlag': 'Day',\n  'Identifiers': [{'Identifier': 'ECOv002_L2T_LSTE_27355_010_04QGH_20230503T190517_0710_01',\n    'IdentifierType': 'ProducerGranuleId'}],\n  'ProductionDateTime': '2023-05-04T14:18:45.217Z',\n  'ArchiveAndDistributionInformation': [{'Name': 'Not provided',\n    'Size': 1.71441,\n    'SizeUnit': 'MB'}]},\n 'Platforms': [{'ShortName': 'ISS',\n   'Instruments': [{'ShortName': 'ECOSTRESS',\n     'ComposedOf': [{'ShortName': 'PHyTIR'}]}]}],\n 'MetadataSpecification': {'URL': 'https://cdn.earthdata.nasa.gov/umm/granule/v1.6.6',\n  'Name': 'UMM-G',\n  'Version': '1.6.6'}}\n\n\n\n# pd.json_normalize(results).keys()\n\n\nresults_df = pd.json_normalize(results)[['meta.native-id', 'umm.TemporalExtent.RangeDateTime.BeginningDateTime','umm.SpatialExtent.HorizontalSpatialDomain.Geometry.BoundingRectangles','umm.RelatedUrls', 'umm.DataGranule.DayNightFlag']]\nresults_df = results_df.rename(columns={'meta.native-id':'Granule', 'umm.TemporalExtent.RangeDateTime.BeginningDateTime':'Date','umm.SpatialExtent.HorizontalSpatialDomain.Geometry.BoundingRectangles':'bbox', 'umm.RelatedUrls':'URL', 'umm.DataGranule.DayNightFlag':'DayNightFlag'})\n\n# Get browse image links and other data links\nresults_df['Browse'] = results_df['URL'].apply(\n    lambda urls: [l['URL'] for l in urls if l['URL'].endswith('.png') and l['URL'].startswith('https')][0]\n)\nresults_df['data_link'] = results_df['URL'].apply(\n    lambda urls: [l['URL'] for l in urls if l['URL'].endswith('.tif') and l['URL'].startswith('https')]\n)\n# remove the URL column\nresults_df = results_df.drop(columns=['URL'])\n\nresults_df.iloc[0]\n\nGranule         ECOv002_L2T_LSTE_27355_010_04QGH_20230503T1905...\nDate                                     2023-05-03T19:05:17.740Z\nbbox            [{'WestBoundingCoordinate': -157.091125, 'East...\nDayNightFlag                                                  Day\nBrowse          https://data.lpdaac.earthdatacloud.nasa.gov/lp...\ndata_link       [https://data.lpdaac.earthdatacloud.nasa.gov/l...\nName: 0, dtype: object",
    "crumbs": [
      "Python Notebooks",
      "Explore the ECOSTRESS Tiled Data"
    ]
  },
  {
    "objectID": "python/tutorials/Working_with_ECOSTRESS_Tiled_data.html#review-the-browse-images",
    "href": "python/tutorials/Working_with_ECOSTRESS_Tiled_data.html#review-the-browse-images",
    "title": "Exploring ECOSTRESS Tiled Data",
    "section": "3. Review the Browse Images",
    "text": "3. Review the Browse Images\nBefore accessing the data layers, we can review the browse images provided as .png files. These previews allow us to visually assess data quality. For example, we can quickly filter out granules that only partially intersect the ROI or overly cloudy granules can be easily identified and excluded. While water and cloud pixels are removed in higher-level ECOSTRESS products, reviewing the browse images is still helpful for identifying any potential quality issues.\n\nfrom ipyleaflet import Map, ImageOverlay, Rectangle, LayersControl\nfrom ipywidgets import SelectMultiple, VBox, Button, Output\nfrom IPython.display import display\n\n# Output widget for map\nout = Output()\n\n# Create selection widget\ngranule_options = [(row[\"Granule\"], idx) for idx, row in results_df.iterrows()]\ngranule_select = SelectMultiple(\n    options=granule_options,\n    value=[0],  # initially select first row\n    description='Granules',\n    rows=10,\n    layout={'width': '400px'}\n)\n\n# Variable to store the selected DataFrame\nselected_df = pd.DataFrame()\n\n# Function to create map based on selection\ndef create_map(selected_indices):\n    all_bounds = [results_df.iloc[idx][\"bbox\"][0] for idx in selected_indices]\n    lats = [(b[\"SouthBoundingCoordinate\"] + b[\"NorthBoundingCoordinate\"])/2 for b in all_bounds]\n    lons = [(b[\"WestBoundingCoordinate\"] + b[\"EastBoundingCoordinate\"])/2 for b in all_bounds]\n\n    m = Map(center=[sum(lats)/len(lats), sum(lons)/len(lons)], zoom=7.5)\n\n    for idx in selected_indices:\n        row = results_df.iloc[idx]\n        bbox = row[\"bbox\"][0]\n        west, east = bbox[\"WestBoundingCoordinate\"], bbox[\"EastBoundingCoordinate\"]\n        south, north = bbox[\"SouthBoundingCoordinate\"], bbox[\"NorthBoundingCoordinate\"]\n\n        overlay = ImageOverlay(\n            url=row[\"Browse\"],\n            bounds=((south, west), (north, east)),\n            opacity=0.7\n        )\n        m.add_layer(overlay)\n\n        rect = Rectangle(bounds=((south, west), (north, east)), color='red', fill=False)\n        m.add_layer(rect)\n\n    m.add_control(LayersControl())\n    return m\n\n# Button to generate new dataframe from selected granules\nbutton = Button(description=\"Create DataFrame from Selection\")\ndef on_button_click(b):\n    global selected_df\n    selected_indices = list(granule_select.value)\n    selected_df = results_df.iloc[selected_indices].copy()\n    with out:\n        out.clear_output(wait=True)\n        print(\"New DataFrame with selected granules:\")\n        display(selected_df)\n\nbutton.on_click(on_button_click)\n\n# Update map when selection changes\ndef update_map(change):\n    with out:\n        out.clear_output(wait=True)\n        display(create_map(list(granule_select.value)))\n        display(button)\n\ngranule_select.observe(update_map, names='value')\n\n# Display widgets\ndisplay(VBox([granule_select, out]))\nupdate_map({'new': [0]})\n\n\n\n\nIn this example, data from rows 6, 8, and 9 are selected and stored in a new DataFrame called selected_df. These indices can also be used to filter the original DataFrame as needed.\n\nselected_df = results_df.iloc[[6,8,9]]\nselected_df\n\n\n\n\n\n\n\n\nGranule\nDate\nbbox\nDayNightFlag\nBrowse\ndata_link\n\n\n\n\n6\nECOv002_L2T_LSTE_27355_011_05QKC_20230503T1906...\n2023-05-03T19:06:10.890Z\n[{'WestBoundingCoordinate': -155.881592, 'East...\nDay\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\n[https://data.lpdaac.earthdatacloud.nasa.gov/l...\n\n\n8\nECOv002_L2T_LSTE_27355_011_04QHG_20230503T1906...\n2023-05-03T19:06:10.890Z\n[{'WestBoundingCoordinate': -156.152756, 'East...\nDay\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\n[https://data.lpdaac.earthdatacloud.nasa.gov/l...\n\n\n9\nECOv002_L2T_LSTE_27355_011_05QKB_20230503T1906...\n2023-05-03T19:06:10.890Z\n[{'WestBoundingCoordinate': -155.864914, 'East...\nDay\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\n[https://data.lpdaac.earthdatacloud.nasa.gov/l...",
    "crumbs": [
      "Python Notebooks",
      "Explore the ECOSTRESS Tiled Data"
    ]
  },
  {
    "objectID": "python/tutorials/Working_with_ECOSTRESS_Tiled_data.html#select-bands",
    "href": "python/tutorials/Working_with_ECOSTRESS_Tiled_data.html#select-bands",
    "title": "Exploring ECOSTRESS Tiled Data",
    "section": "4. Select Bands",
    "text": "4. Select Bands\nFrom the filtered DataFrame, we will only select the LST, Quality, and Cloud bands for this example. Other bands can be selected in the same way if needed.\n\ni = 1\nselected_df.iloc[i]['data_link']\n\n['https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27355_011_04QHG_20230503T190610_0710_01/ECOv002_L2T_LSTE_27355_011_04QHG_20230503T190610_0710_01_water.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27355_011_04QHG_20230503T190610_0710_01/ECOv002_L2T_LSTE_27355_011_04QHG_20230503T190610_0710_01_cloud.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27355_011_04QHG_20230503T190610_0710_01/ECOv002_L2T_LSTE_27355_011_04QHG_20230503T190610_0710_01_view_zenith.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27355_011_04QHG_20230503T190610_0710_01/ECOv002_L2T_LSTE_27355_011_04QHG_20230503T190610_0710_01_height.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27355_011_04QHG_20230503T190610_0710_01/ECOv002_L2T_LSTE_27355_011_04QHG_20230503T190610_0710_01_QC.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27355_011_04QHG_20230503T190610_0710_01/ECOv002_L2T_LSTE_27355_011_04QHG_20230503T190610_0710_01_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27355_011_04QHG_20230503T190610_0710_01/ECOv002_L2T_LSTE_27355_011_04QHG_20230503T190610_0710_01_LST_err.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27355_011_04QHG_20230503T190610_0710_01/ECOv002_L2T_LSTE_27355_011_04QHG_20230503T190610_0710_01_EmisWB.tif']\n\n\n\nselected_df['lst_link'] = selected_df['data_link'].apply(\n    lambda links: [link for link in links if 'LST.tif' in link][0]\n)\nselected_df['quality_link'] = selected_df['data_link'].apply(\n    lambda links: [link for link in links if 'QC' in link][0]\n)\nselected_df['cloud_link'] = selected_df['data_link'].apply(\n    lambda links: [link for link in links if 'cloud' in link][0]\n)\n\n\nselected_df\n\n\n\n\n\n\n\n\nGranule\nDate\nbbox\nDayNightFlag\nBrowse\ndata_link\nlst_link\nquality_link\ncloud_link\n\n\n\n\n6\nECOv002_L2T_LSTE_27355_011_05QKC_20230503T1906...\n2023-05-03T19:06:10.890Z\n[{'WestBoundingCoordinate': -155.881592, 'East...\nDay\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\n[https://data.lpdaac.earthdatacloud.nasa.gov/l...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\n\n\n8\nECOv002_L2T_LSTE_27355_011_04QHG_20230503T1906...\n2023-05-03T19:06:10.890Z\n[{'WestBoundingCoordinate': -156.152756, 'East...\nDay\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\n[https://data.lpdaac.earthdatacloud.nasa.gov/l...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\n\n\n9\nECOv002_L2T_LSTE_27355_011_05QKB_20230503T1906...\n2023-05-03T19:06:10.890Z\n[{'WestBoundingCoordinate': -155.864914, 'East...\nDay\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\n[https://data.lpdaac.earthdatacloud.nasa.gov/l...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...",
    "crumbs": [
      "Python Notebooks",
      "Explore the ECOSTRESS Tiled Data"
    ]
  },
  {
    "objectID": "python/tutorials/Working_with_ECOSTRESS_Tiled_data.html#accessing-cog-bands",
    "href": "python/tutorials/Working_with_ECOSTRESS_Tiled_data.html#accessing-cog-bands",
    "title": "Exploring ECOSTRESS Tiled Data",
    "section": "5. Accessing COG Bands",
    "text": "5. Accessing COG Bands\nECOSTRESS data is stored in NASA’s Earthdata Cloud and can be accessed in different ways.\nDownloading – This has been a supported option since the inception of NASA’s DAACs. Users can use the data link(s) to download files to their local working environment. This method works in both cloud and non-cloud environments. earthaccess.download(list(selected_df['lst_link']), local_path='../../data')\nStreaming – Streaming enables on-the-fly reading of remote files (i.e., files not saved locally). However, the accessed data must fit into the workspace’s memory. Streaming works in both cloud and non-cloud environments. Streaming data stored in the cloud without downloading is called in-place access or direct S3 access, this is only available when working in a cloud environment deployed in AWS us-west-2.\nThe Python libraries used to access COG files in Earthdata Cloud leverage GDAL’s virtual file systems. Whether you are running this code in the Cloud or in a local workspace, GDAL configurations must be set in order to successfully access the ECOSTRESS COG files. Entering the context manager for multiple cells of the notebook allows us to more freely interact with the data. We’ll close the context manager (env.exit()) when we have all of the data loaded into memory.\n\nrio_env = rio.Env(GDAL_DISABLE_READDIR_ON_OPEN='EMPTY_DIR',\n                  GDAL_HTTP_COOKIEFILE=os.path.expanduser('~/cookies.txt'),\n                  GDAL_HTTP_COOKIEJAR=os.path.expanduser('~/cookies.txt'),\n                  GDAL_HTTP_MAX_RETRY=10,\n                  GDAL_HTTP_RETRY_DELAY=0.5)\nrio_env.__enter__()\n\n&lt;rasterio.env.Env at 0x7fb018479a10&gt;\n\n\nBelow, files are read into the workspace using open_rasterio from the rioxarray library. Since the files consists of only 1 layer, we can squeeze it, removing the band dimension.\n\ngranules = selected_df['Granule']\nlst = selected_df['lst_link']\nqa = selected_df['quality_link']\ncld = selected_df['cloud_link']\n\n\ngranule_name = granules.iloc[i]\neco_lst_ds = rxr.open_rasterio(lst.iloc[i]).squeeze('band', drop=True)\neco_qa_ds = rxr.open_rasterio(qa.iloc[i]).squeeze('band', drop=True)\neco_cld_ds = rxr.open_rasterio(cld.iloc[i]).squeeze('band', drop=True)\n\n\neco_lst_ds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (y: 1568, x: 1568)&gt; Size: 10MB\n[2458624 values with dtype=float32]\nCoordinates:\n  * x            (x) float64 13kB 8e+05 8.001e+05 ... 9.096e+05 9.097e+05\n  * y            (y) float64 13kB 2.2e+06 2.2e+06 2.2e+06 ... 2.09e+06 2.09e+06\n    spatial_ref  int64 8B 0\nAttributes:\n    AREA_OR_POINT:  Area\n    _FillValue:     nan\n    scale_factor:   1.0\n    add_offset:     0.0xarray.DataArrayy: 1568x: 1568...[2458624 values with dtype=float32]Coordinates: (3)x(x)float648e+05 8.001e+05 ... 9.097e+05array([800015., 800085., 800155., ..., 909565., 909635., 909705.],\n      shape=(1568,))y(y)float642.2e+06 2.2e+06 ... 2.09e+06array([2199985., 2199915., 2199845., ..., 2090435., 2090365., 2090295.],\n      shape=(1568,))spatial_ref()int640crs_wkt :PROJCS[\"WGS 84 / UTM zone 4N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-159],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984projected_crs_name :WGS 84 / UTM zone 4Ngrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-159.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"WGS 84 / UTM zone 4N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-159],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :799980.0 70.0 0.0 2200020.0 0.0 -70.0array(0)Indexes: (2)xPandasIndexPandasIndex(Index([800015.0, 800085.0, 800155.0, 800225.0, 800295.0, 800365.0, 800435.0,\n       800505.0, 800575.0, 800645.0,\n       ...\n       909075.0, 909145.0, 909215.0, 909285.0, 909355.0, 909425.0, 909495.0,\n       909565.0, 909635.0, 909705.0],\n      dtype='float64', name='x', length=1568))yPandasIndexPandasIndex(Index([2199985.0, 2199915.0, 2199845.0, 2199775.0, 2199705.0, 2199635.0,\n       2199565.0, 2199495.0, 2199425.0, 2199355.0,\n       ...\n       2090925.0, 2090855.0, 2090785.0, 2090715.0, 2090645.0, 2090575.0,\n       2090505.0, 2090435.0, 2090365.0, 2090295.0],\n      dtype='float64', name='y', length=1568))Attributes: (4)AREA_OR_POINT :Area_FillValue :nanscale_factor :1.0add_offset :0.0\n\n\nAs mentioned, the ECOSTRESS product we are using here is tiled and the CRS is dependent on UTM zone. For this tile, we can look at the spatial_ref variable through the interactive object above to see details such as the well-known-text (WKT) representation of the CRS and other attributes.\nNow let’s plot the data using hvplot. reproject function is applied only for the visualization. Make sure to specify the CRS argument within the hvplot.image function so the ESRI imagery RBG background tile aligns with our scene.\n\nsize_opts = dict(frame_height=405, frame_width=720, fontscale=2)\n\neco_lst_ds.rio.reproject('EPSG:4326').hvplot.image(x='x', y='y', **size_opts, \n                                                   cmap='inferno', tiles='ESRI', xlabel='Longitude', \n                                                   ylabel='Latitude', title='ECOSTRESS LST (K)', \n                                                   crs='EPSG:4326')*gdf.hvplot(color=None, line_color='Red',alpha=0.5, line_width=4, crs='EPSG:4326')",
    "crumbs": [
      "Python Notebooks",
      "Explore the ECOSTRESS Tiled Data"
    ]
  },
  {
    "objectID": "python/tutorials/Working_with_ECOSTRESS_Tiled_data.html#quality-filtering",
    "href": "python/tutorials/Working_with_ECOSTRESS_Tiled_data.html#quality-filtering",
    "title": "Exploring ECOSTRESS Tiled Data",
    "section": "6. Quality Filtering",
    "text": "6. Quality Filtering\nThe quality values are 16 digits bit values with bits 0 and 1 being the mandatory QA flag that will be interpreted as:\n    00 = Pixel produced, best quality\n    01 = Pixel produced, nominal quality. Either one or more of the following conditions are met:  \n\n            1. emissivity in both bands 4 and 5 &lt; 0.95, i.e. possible cloud contamination  \n            2. low transmissivity due to high water vapor loading (&lt;0.4), check PWV values and error estimates  \n            3. Pixel falls on missing scan line in bands 1&5, and filled using spatial neural net. Check error estimates  \n            Recommend more detailed analysis of other QC information  \n    10 = Pixel produced, but cloud detected  \n    11 = Pixel not produced due to missing/bad data, user should check Data quality flag bits  \nImportant:  There is a known data issue related to quality of ECOSTRESS Tile LSTE version 2 data:\nAll users of ECOSTRESS L2 v002 products (ECO_L2T_LSTE, ECO_L2_LSTE, ECO_L2G_LSTE) should be aware that the cloud mask information previously available in the Quality Control (QC) layer in v001, is not available in the v002 QC layer. Instead, users should be using the ‘cloud_mask’ layer in the L2 LSTE product, or the cloud information in the standard cloud mask products (ECO_L2_CLOUD, ECO_L2T_CLOUD, ECO_L2G_CLOUD) to assess if a pixel is clear or cloudy (see section 3 of the User Guide). \nThe detailed quality information is provided in Table 3-5 in ECOSTRESS Product Specification Document and product CMR page\nBelow, the unique quality values are extracted from the data, and only the values showing good quality are kept. To identify clear pixels, you must now apply an extra filtering step. In the cloud mask layer, value zero means absence of cloud, and one means presence of cloud. When processing, ensure you retain only pixels with a value of 0 in the cloud mask layer to exclude cloudy pixels.\n\nquality_vals = np.unique(eco_qa_ds.values).tolist()\ngood_q = [q for q in quality_vals if np.binary_repr(q, width=16)[-2:] == '00']\n\nThe .where method is used to filter the data by quality, retaining only the LST values flagged as best quality and cloud-free.\n\neco_lst_mask = eco_lst_ds.where(eco_qa_ds.isin(good_q))\neco_lst_mask = eco_lst_mask.where(eco_cld_ds.isin([0]))\n\n\neco_lst_mask.rio.reproject('EPSG:4326').hvplot.image(x='x', y='y', **size_opts, \n                                                   cmap='inferno', tiles='ESRI', xlabel='Longitude', \n                                                   ylabel='Latitude', title='ECOSTRESS LST (K)', \n                                                   crs='EPSG:4326')\n\n\n\n\n\n  \n\n\n\n\nOther quality considerations:\n\nSolar Array Obstruction: Some ECOSTRESS scenes may be affected by solar array obstructions from the International Space Station (ISS), potentially impacting data quality of obstructed pixels. The ‘FieldOfViewObstruction’ metadata field is included in all Version 2 products to indicate possible obstructions: Before October 24, 2024 (orbits prior to 35724): The field is present but was not populated and does not reliably identify affected scenes. On or after October 24, 2024 (starting with orbit 35724): The field is populated and generally accurate, except for late December 2024, when a temporary processing error may have caused false positives. A list of scenes confirmed to be affected by obstructions is available and is recommended for verifying historical data (before October 24, 2024) and scenes from late December 2024.\nGeolocation: The ISS native pointing information is coarse relative to ECOSTRESS pixels, so ECOSTRESS geolocation is improved through image matching with a basemap. GeolocationAccuracyQA metadata in the L1B_GEO file shows the success of this geolocation improvement, using categorizations “best”, “good”, “suspect”, and “poor”. We recommend that users use only “best” and “good” scenes for evaluations where geolocation is important (e.g., comparison to field sites). For some scenes, this metadata is not reflected in the higher-level products (e.g., land surface temperature, evapotranspiration, etc.). While this metadata is always available in the geolocation product, to save users additional download, we have produced a summary text file that includes the geolocation quality flags for all scenes from launch to present.\nNoisy Data: During the time period of May 15th 2025 through July 1st, 2025 ECOSTRESS data was noisier than expected. Cycling the payload resolved the issue, but researchers should use all levels of ECOSTRESS data acquired during this time period with caution.\n\nMore details available at https://cmr.earthdata.nasa.gov/search/concepts/C2076090826-LPCLOUD.html.",
    "crumbs": [
      "Python Notebooks",
      "Explore the ECOSTRESS Tiled Data"
    ]
  },
  {
    "objectID": "python/tutorials/Working_with_ECOSTRESS_Tiled_data.html#cropping-ecostress-data",
    "href": "python/tutorials/Working_with_ECOSTRESS_Tiled_data.html#cropping-ecostress-data",
    "title": "Exploring ECOSTRESS Tiled Data",
    "section": "7. Cropping ECOSTRESS Data",
    "text": "7. Cropping ECOSTRESS Data\nThe clip function from rasterio is used to mask out data outside of our ROI. Before clipping, the ROI must be reprojected to match the projection of the dataset.\n\npolygon_reproj = gdf.to_crs(eco_lst_mask.rio.crs)\neco_lst_mask = eco_lst_mask.rio.clip(polygon_reproj.geometry.values, polygon_reproj.crs, all_touched=True)\n\n\neco_lst_mask.hvplot.image(\n    geo=True,cmap='inferno',**size_opts, tiles='ESRI',alpha=0.8, \n    title='Cropped ECOSTRESS LST (K)', xlabel='Longitude',ylabel='Latitude', \n    rasterize=True)",
    "crumbs": [
      "Python Notebooks",
      "Explore the ECOSTRESS Tiled Data"
    ]
  },
  {
    "objectID": "python/tutorials/Working_with_ECOSTRESS_Tiled_data.html#writing-outputs",
    "href": "python/tutorials/Working_with_ECOSTRESS_Tiled_data.html#writing-outputs",
    "title": "Exploring ECOSTRESS Tiled Data",
    "section": "8. Writing Outputs",
    "text": "8. Writing Outputs\nWe now have a ECOSTRESS tile that is clipped to our ROI with only good quality values. Now, we can save this file locally. The filed with only NA values will not be exported.\n\ngranule_name\n\n'ECOv002_L2T_LSTE_27355_011_04QHG_20230503T190610_0710_01'\n\n\n\nif eco_lst_mask.notnull().any():\n    out_name = f\"../../data/{granule_name}_clipped.tif\"\n    eco_lst_mask.rio.to_raster(raster_path=out_name, driver='COG')",
    "crumbs": [
      "Python Notebooks",
      "Explore the ECOSTRESS Tiled Data"
    ]
  },
  {
    "objectID": "python/tutorials/Working_with_ECOSTRESS_Tiled_data.html#automate",
    "href": "python/tutorials/Working_with_ECOSTRESS_Tiled_data.html#automate",
    "title": "Exploring ECOSTRESS Tiled Data",
    "section": "9. Automate",
    "text": "9. Automate\n\nfor i in range(len(lst)):\n    granule_name = granules.iloc[i]\n    eco_lst_ds = rxr.open_rasterio(lst.iloc[i]).squeeze('band', drop=True)\n    eco_qa_ds = rxr.open_rasterio(qa.iloc[i]).squeeze('band', drop=True)\n    eco_cld_ds = rxr.open_rasterio(cld.iloc[i]).squeeze('band', drop=True)\n    \n    quality_vals = np.unique(eco_qa_ds.values).tolist()\n    good_q = [q for q in quality_vals if np.binary_repr(q, width=16)[-2:] == '00']\n    \n    eco_lst_mask = eco_lst_ds.where(eco_qa_ds.isin(good_q))\n    eco_lst_mask = eco_lst_mask.where(eco_cld_ds.isin([0]))\n    eco_lst_mask = eco_lst_mask.rio.clip(polygon_reproj.geometry.values, polygon_reproj.crs, all_touched=True)\n    if eco_lst_mask.notnull().any():\n        out_name = f\"../../data/{granule_name}_clipped.tif\"\n        eco_lst_mask.rio.to_raster(raster_path=out_name, driver='COG')\n\n\nrio_env.__exit__()",
    "crumbs": [
      "Python Notebooks",
      "Explore the ECOSTRESS Tiled Data"
    ]
  },
  {
    "objectID": "python/tutorials/Working_with_ECOSTRESS_Tiled_data.html#contact-info",
    "href": "python/tutorials/Working_with_ECOSTRESS_Tiled_data.html#contact-info",
    "title": "Exploring ECOSTRESS Tiled Data",
    "section": "Contact Info:",
    "text": "Contact Info:\nEmail: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)¹\nWebsite: https://www.earthdata.nasa.gov/centers/lp-daac\n¹Work performed under USGS contract G15PD00467 for NASA contract NNG14HH33I.",
    "crumbs": [
      "Python Notebooks",
      "Explore the ECOSTRESS Tiled Data"
    ]
  },
  {
    "objectID": "python/tutorials/ECOSTRESS_Tutorial.html",
    "href": "python/tutorials/ECOSTRESS_Tutorial.html",
    "title": "Working with ECOSTRESS Evapotranspiration Data",
    "section": "",
    "text": "The Land Processes Distributed Active Archive Center (LP DAAC) distributes the Ecosystem Spaceborne Thermal Radiometer Experiment on Space Station (ECOSTRESS) data products. The ECOSTRESS mission is tasked with measuring the temperature of plants to better understand how much water plants need and how they respond to stress. ECOSTRESS products are archived and distributed in the HDF5 file format as swath-based products.\nIn this tutorial, you will use Python to perform a swath to grid conversion to project the swath data on to a grid with a defined coordinate reference system (CRS), compare ECOSTRESS data with ground-based AmeriFlux flux tower observations, and export science dataset (SDS) layers as GeoTIFF files that can be loaded into a GIS and/or Remote Sensing software program.  ### Example: Converting a swath ECO3ETPTJPL.001 HDF5 file into a GeoTIFF with a defined CRS and comparing ECOSTRESS Evapotranspiration (ET) with ground-based ET observations from an AmeriFlux flux tower location in California.\n#### Data Used in the Example:\n- Data Product: ECOSTRESS Evapotranspiration PT-JPL Daily L3 Global 70m Version 1 (ECO3ETPTJPL.001)\n- Science Dataset (SDS) layers:\n- ETinst\n- ETinstUncertainty\n- Data Product: ECOSTRESS Geolocation Daily L1B Global 70m Version 1 (ECO1BGEO.001)\n- Science Dataset (SDS) layers:\n- Latitude\n- Longitude\n- Data Product: AmeriFlux Ground Observations for Flux Tower US-CZ3: Sierra Critical Zone, Sierra Transect, Sierran Mixed Conifer, P301 - Variables:\n- Latent Heat (W/m\\(^{2}\\))\n\n# Topics Covered: 1. Getting Started\n1a. Import Packages\n1b. Set Up the Working Environment\n1c. Retrieve Files\n2. Importing and Interpreting Data\n2a. Open an ECOSTRESS HDF5 File and Read File Metadata\n2b. Subset SDS Layers\n3. Performing Swath2grid Conversion\n3a. Import Geolocation File\n3b. Define Projection and Output Grid\n3c. Read SDS Metadata\n3d. Perform K-D Tree Resampling\n3e. Basic Image Processing\n4. Exporting Results\n4a. Set Up a Dictionary\n4b. Define CRS and Export as GeoTIFFs\n5. Combining ECOSTRESS and AmeriFlux Tower Data\n5a. Loading Tables with Pandas\n5b. Locate ECOSTRESS Pixel from Lat/Lon Coordinates\n6. Visualizing Data\n6a. Create Colormap\n6b. Plot ET Data\n6c. Exporting an Image\n7. Comparing Observations\n7a. Calculate Distribution of ECOSTRESS Data\n7b. Visualize Ground Observations\n7c. Combine ECOSTRESS and Ground Observations"
  },
  {
    "objectID": "python/tutorials/ECOSTRESS_Tutorial.html#getting-started",
    "href": "python/tutorials/ECOSTRESS_Tutorial.html#getting-started",
    "title": "Working with ECOSTRESS Evapotranspiration Data",
    "section": "Getting Started:",
    "text": "Getting Started:\n\n1. This tutorial uses data from ECOSTRESS Version 1, including an ECO3ETPTJPL.001 (and accompanying ECO1BGEO.001) observation from August 05, 2018. You can download the files directly from the LP DAAC Data Pool at:\n\nECOSTRESS_L1B_GEO_00468_007_20180805T220314_0601_03.h5\n\nECOSTRESS_L3_ET_PT-JPL_00468_007_20180805T220314_0601_04.h5\nA NASA Earthdata Login account is required to download the data used in this tutorial. You can create an account at the link provided.\nAncillary Files Needed:\n\nThe AmeriFlux Latent Heat data used in Section 4 can be downloaded via a csv file.\n\n\nThe tower_data.csv file will need to be downloaded into the same directory as the tutorial in order to execute the tutorial.\n#### 2. Copy/clone/download the ECOSTRESS Tutorial repo, or the desired tutorial from the LP DAAC Data User Resources Repository:\n- Working with ECOSTRESS Evapotranspiration Data in Python Jupyter Notebook\n\n\nNOTE: This tutorial was developed specifically for the ECOSTRESS Evapotranspiration PT-JPL Level 3, Version 1 HDF5 files and will need to be adapted to work with other ECOSTRESS products.\n\n\n\n\n\nGetting Started"
  },
  {
    "objectID": "python/tutorials/ECOSTRESS_Tutorial.html#a.-import-packages",
    "href": "python/tutorials/ECOSTRESS_Tutorial.html#a.-import-packages",
    "title": "Working with ECOSTRESS Evapotranspiration Data",
    "section": "1a. Import Packages",
    "text": "1a. Import Packages\n\nImport the python packages required to complete this tutorial.\n\n# Import packages\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport h5py\nimport os\nfrom os.path import join\nimport pyproj\nimport numpy as np\nimport pandas as pd\nimport datetime\nfrom dateutil import parser\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import LinearSegmentedColormap\nfrom pyresample import geometry as geom\nfrom pyresample import kd_tree as kdt\nfrom osgeo import gdal, gdal_array, gdalconst, osr\n\n# Set plots inside of the Jupyter Notebook\n%matplotlib inline"
  },
  {
    "objectID": "python/tutorials/ECOSTRESS_Tutorial.html#b.-set-up-the-working-environment",
    "href": "python/tutorials/ECOSTRESS_Tutorial.html#b.-set-up-the-working-environment",
    "title": "Working with ECOSTRESS Evapotranspiration Data",
    "section": "1b. Set Up the Working Environment",
    "text": "1b. Set Up the Working Environment\n\nThe input directory is defined as the current working directory. Note that you will need to have the jupyter notebook and example data (.h5 and .csv) stored in this directory in order to execute the tutorial successfully.\n\n# Current working directory will be set as the input directory\ninDir =  \"../data\" #os.getcwd() + os.sep                                                     \nprint(\"input directory:\\n{}\".format(inDir))\n\n# Set output directory\noutDir = os.path.normpath(os.path.split(inDir)[0] + os.sep + 'output') + os.sep  \nprint(\"output directory:\\n{}\".format(outDir))\n\n# Create output directory\nif not os.path.exists(outDir): \n    os.makedirs(outDir)\n\ninput directory:\n../data\noutput directory:\n..\\output\\\n\n\n\n\nMake sure that the ECOSTRESS .h5 data files, and Ameriflux ET data file (.csv) are located in the input directory printed above."
  },
  {
    "objectID": "python/tutorials/ECOSTRESS_Tutorial.html#c.-retrieve-files",
    "href": "python/tutorials/ECOSTRESS_Tutorial.html#c.-retrieve-files",
    "title": "Working with ECOSTRESS Evapotranspiration Data",
    "section": "1c. Retrieve Files",
    "text": "1c. Retrieve Files\n\nMake sure that the ECO1BGEO and ECO3ETPTJPL .h5 files listed in the directions have been downloaded to the inDir defined above to follow along in the tutorial.\n\nos.chdir(inDir)\n\n\n# List directory contents and create lists of ECOSTRESS HDF5 files (GEO, ET)\ngeoList = [file for file in os.listdir() if file.endswith('.h5') and 'GEO' in file]\nprint(\"geolocation:\\n{}\".format(\"\\n\".join(geoList)))\necoList = [file for file in os.listdir() if file.endswith('.h5') and 'GEO' not in file]\nprint(\"products:\\n{}\".format(\"\\n\".join(ecoList)))\n\ngeolocation:\nECOSTRESS_L1B_GEO_00468_007_20180805T220314_0601_03.h5\nproducts:\nECOSTRESS_L3_ET_PT-JPL_00468_007_20180805T220314_0601_04.h5\n\n\n\n\nThe standard format for ECOSTRESS filenames is as follows:\n\nECOSTRESS_L3_ET_PT-JPL: Product Type\n00468: Orbit number; starting at start of mission, ascending equatorial crossing\n007: Scene ID; starting at first scene of first orbit\n20180805T220314: Date and time of data start: YYYYMMDDThhmmss\n0601: Build ID of software that generated product, Major+Minor (2+2 digits)\n04: Product version number (2 digits)\n\n\n\n\n\nImporting and Interpreting Data"
  },
  {
    "objectID": "python/tutorials/ECOSTRESS_Tutorial.html#a.-open-an-ecostress-hdf5-file",
    "href": "python/tutorials/ECOSTRESS_Tutorial.html#a.-open-an-ecostress-hdf5-file",
    "title": "Working with ECOSTRESS Evapotranspiration Data",
    "section": "2a. Open an ECOSTRESS HDF5 File",
    "text": "2a. Open an ECOSTRESS HDF5 File\n\nRead in an ECOSTRESS HDF5 file using the h5py package.\n\nf = h5py.File(ecoList[0])             # Read in ECOSTRESS HDF5 file\necoName = ecoList[0].split('.h5')[0]  # Keep original filename\nprint(ecoName)\n\nECOSTRESS_L3_ET_PT-JPL_00468_007_20180805T220314_0601_04"
  },
  {
    "objectID": "python/tutorials/ECOSTRESS_Tutorial.html#b.-subset-sds-layers-and-read-sds-metadata",
    "href": "python/tutorials/ECOSTRESS_Tutorial.html#b.-subset-sds-layers-and-read-sds-metadata",
    "title": "Working with ECOSTRESS Evapotranspiration Data",
    "section": "2b. Subset SDS Layers and read SDS Metadata",
    "text": "2b. Subset SDS Layers and read SDS Metadata\n\nIdentify and generate a list of all the SDS layers in the HDF5 file.\n\n# Create a list of all SDS inside of the .h5 file\neco_objs = []\nf.visit(eco_objs.append)\necoSDS = [str(obj) for obj in eco_objs if isinstance(f[obj], h5py.Dataset)] \nfor dataset in ecoSDS[0:10]: \n    print(dataset)\n\nEVAPOTRANSPIRATION PT-JPL/ETcanopy\nEVAPOTRANSPIRATION PT-JPL/ETdaily\nEVAPOTRANSPIRATION PT-JPL/ETinst\nEVAPOTRANSPIRATION PT-JPL/ETinstUncertainty\nEVAPOTRANSPIRATION PT-JPL/ETinterception\nEVAPOTRANSPIRATION PT-JPL/ETsoil\nL3_ET_PT-JPL Metadata/AncillaryFileAerosolOpticalDepth\nL3_ET_PT-JPL Metadata/AncillaryFileAirTemperatureNWP\nL3_ET_PT-JPL Metadata/AncillaryFileAirTemperatureRS\nL3_ET_PT-JPL Metadata/AncillaryFileAlbedo\n\n\n\n\nBelow, subset the SDS list to the two layers needed for comparison with the ground-based AmeriFlux data, ETinst and ETinstUncertainty.\n\n# Subset list to ETinst and ETinstUncertainty\nsds = ['ETinst', 'ETinstUncertainty']\necoSDS = [dataset for dataset in ecoSDS if dataset.endswith(tuple(sds))]\nfor dataset in ecoSDS:\n    print(dataset.split('/')[-1])\n\nETinst\nETinstUncertainty\n\n\n\n\n\n\nPerforming Swath2grid Conversion\n\n\n\n\n\nResample the native ECOSTRESS swath data to a grid with defined coordinate reference system (CRS)."
  },
  {
    "objectID": "python/tutorials/ECOSTRESS_Tutorial.html#a.-import-geolocation-file",
    "href": "python/tutorials/ECOSTRESS_Tutorial.html#a.-import-geolocation-file",
    "title": "Working with ECOSTRESS Evapotranspiration Data",
    "section": "3a. Import Geolocation File",
    "text": "3a. Import Geolocation File\n\nThe latitude and longitude arrays from the ECO1BGEO product for the same ECOSTRESS orbit/scene ID are needed to perform the swath2grid conversion on the ECO3ETPT-JPL file.\n\n# Find the matching ECO1BGEO file from the file list\ngeo = [geoFile for geoFile in geoList if ecoList[0].split('ECOSTRESS_L3_ET_PT-JPL_')[-1].split('T')[0] in geoFile]\nprint(geo[0])\nprint(ecoList[0])\n\nECOSTRESS_L1B_GEO_00468_007_20180805T220314_0601_03.h5\nECOSTRESS_L3_ET_PT-JPL_00468_007_20180805T220314_0601_04.h5\n\n\n\n\nRead in the ECO1BGEO file, search for the latitude and longitude SDS, and import into Python as arrays.\n\n# Open Geo File\ng = h5py.File(geo[0])\ngeo_objs = []\ng.visit(geo_objs.append)\n\n# Search for lat/lon SDS inside data file\nlatSD = [str(obj) for obj in geo_objs if isinstance(g[obj], h5py.Dataset) and '/latitude' in obj]\nlonSD = [str(obj) for obj in geo_objs if isinstance(g[obj], h5py.Dataset) and '/longitude' in obj]\n\n# Open SDS as arrays\nlat = g[latSD[0]][()].astype(float)\nlon = g[lonSD[0]][()].astype(float)\n\n# Read the array dimensions\ndims = lat.shape\nprint(dims)\n\n(5632, 5400)"
  },
  {
    "objectID": "python/tutorials/ECOSTRESS_Tutorial.html#b.-define-projection-and-output-grid",
    "href": "python/tutorials/ECOSTRESS_Tutorial.html#b.-define-projection-and-output-grid",
    "title": "Working with ECOSTRESS Evapotranspiration Data",
    "section": "3b. Define Projection and Output Grid",
    "text": "3b. Define Projection and Output Grid\n\nThe latitude and longitude arrays from the ECO1BGEO product for the same ECOSTRESS orbit/scene ID are needed to perform the swath2grid conversion on the ECO3ETPT-JPL file.\n\n\nThe following sections use the pyresample package to resample the ECOSTRESS swath dataset to a grid using nearest neighbor method. This process begins by defining the swath dimensions using the lat/lon arrays below.\n\n# Set swath definition from lat/lon arrays\nswathDef = geom.SwathDefinition(lons=lon, lats=lat)\nswathDef.corners\n\n[(-118.7239829569036, 34.221086430090274),\n (-121.86703190925778, 36.84456587689783),\n (-118.81973784761588, 39.05327678830593),\n (-115.71697211227038, 36.35603480980171)]\n\n\n\n\nDefine the coordinates in the middle of the swath, which are used to calculate an estimate of the output rows/columns for the gridded output.\n\n# Define the lat/lon for the middle of the swath\nmid = [int(lat.shape[1] / 2) - 1, int(lat.shape[0] / 2) - 1]\nmidLat, midLon = lat[mid[0]][mid[1]], lon[mid[0]][mid[1]]\nmidLat, midLon\n\n(np.float64(36.63127602811011), np.float64(-118.88736212064671))\n\n\n\n\nBelow, pyproj.Proj is used to perform a cartographic transformation by defining an Azimuthal Equidistant projection centered on the midpoint of the swath. Once the projection is defined, convert the lower left and upper right corners of the lat/lon arrays to a location (in meters) in the new projection. Lastly, measure the distance between the corners and divide by 70 (meters), the nominal pixel size that we are aiming for. Azimuthal Equidistant projection was chosen here based on the following characteristics of this projection:\n\nUnits in meters (necessary for defining 70 m pixels)\n\nDistances between all points are proportionally correct from center point\nAzimuth (direction) are correct from the center point\n\n\n# Define AEQD projection centered at swath center\nepsgConvert = pyproj.Proj(\"+proj=aeqd +lat_0={} +lon_0={}\".format(midLat, midLon))\n\n# Use info from AEQD projection bbox to calculate output cols/rows/pixel size\nllLon, llLat = epsgConvert(np.min(lon), np.min(lat), inverse=False)\nurLon, urLat = epsgConvert(np.max(lon), np.max(lat), inverse=False)\nareaExtent = (llLon, llLat, urLon, urLat)\ncols = int(round((areaExtent[2] - areaExtent[0]) / 70))  # 70 m pixel size\nrows = int(round((areaExtent[3] - areaExtent[1]) / 70))\n\n\n\nUse number of rows and columns generated above from the AEQD projection to set a representative number of rows and columns in the Geographic area definition, which will then be translated to degrees below, then take the smaller of the two pixel dims to determine output size and ensure square pixels.\n\n# Define Geographic projection\nepsg, proj, pName = '4326', 'longlat', 'Geographic'\n\n# Define bounding box of swath\nllLon, llLat, urLon, urLat = np.min(lon), np.min(lat), np.max(lon), np.max(lat)\nareaExtent = (llLon, llLat, urLon, urLat)\n\n# Create area definition with estimated number of columns and rows\nprojDict = pyproj.CRS(\"epsg:4326\")\nareaDef = geom.AreaDefinition(epsg, pName, proj, projDict, cols, rows, areaExtent)\n\n\n\nBelow, square the pixels by setting the pixel size to the smaller of the x any y values output by the AreaDefinition, then use the pixel size to recalculate the number of output cols/rows.\n\n# Square pixels and calculate output cols/rows\nps = np.min([areaDef.pixel_size_x, areaDef.pixel_size_y])\ncols = int(round((areaExtent[2] - areaExtent[0]) / ps))\nrows = int(round((areaExtent[3] - areaExtent[1]) / ps))\n\n# Set up a new Geographic area definition with the refined cols/rows\nareaDef = geom.AreaDefinition(epsg, pName, proj, projDict, cols, rows, areaExtent)\n\n\n\nBelow, use pyresample kd_tree’s get_neighbour_info to create arrays with information on the nearest neighbor to each grid point.\n\n\nThis is the most computationally heavy task in the swath2grid conversion and using get_neighbour_info speeds up the process if you plan to resample multiple SDS within an ECOSTRESS product (compute once instead of for every SDS).\n\n# Get arrays with information about the nearest neighbor to each grid point \nindex, outdex, indexArr, distArr = kdt.get_neighbour_info(swathDef, areaDef, 210, neighbours=1)\n\n\n\nAbove, the function is comparing the swath and area definitions to locate the nearest neighbor (neighbours=1). 210 is the radius_of_influence, or the radius used to search for the nearest neighboring pixel in the swath (in meters)."
  },
  {
    "objectID": "python/tutorials/ECOSTRESS_Tutorial.html#c.-read-sds-metadata",
    "href": "python/tutorials/ECOSTRESS_Tutorial.html#c.-read-sds-metadata",
    "title": "Working with ECOSTRESS Evapotranspiration Data",
    "section": "3c. Read SDS Metadata",
    "text": "3c. Read SDS Metadata\n\nList the attributes for the ETinst layer, which can then be used to define the fill value and scale factor.\n\n# Read in ETinst and print out SDS attributes\ns = ecoSDS[0]\necoSD = f[s][()]\nfor attr in f[s].attrs:\n    if type(f[s].attrs[attr]) == np.ndarray:\n        print(f'{attr} = {f[s].attrs[attr][0]}')\n    else:\n        print(f'{attr} = {f[s].attrs[attr].decode(\"utf-8\")}')\n\n_FillValue = nan\nadd_offset = 0.0\nlong_name = Instantaneous Latent Heat Flux\nscale_factor = 1.0\nunits = W/m^2\nvalid_max = 2000.0\nvalid_min = 0.0\n\n\n\n\nExtract the scale factor, add offset and fill value from the SDS metadata.\n\nf[s].attrs['_FillValue'][0]\n\nnp.float32(nan)\n\n\n\n# Read SDS attributes and define fill value, add offset, and scale factor if available\ntry:\n    fv = int(f[s].attrs['_FillValue'])\nexcept KeyError:\n    fv = None\nexcept ValueError:\n    fv = f[s].attrs['_FillValue'][0]\ntry:\n    sf = f[s].attrs['_Scale'][0]\nexcept:\n    sf = 1\ntry:\n    add_off = f[s].attrs['_Offset'][0]\nexcept:\n    add_off = 0\ntry:\n    units = f[s].attrs['units'].decode(\"utf-8\")\nexcept:\n    units = 'none'"
  },
  {
    "objectID": "python/tutorials/ECOSTRESS_Tutorial.html#d.-perform-k-d-tree-resampling",
    "href": "python/tutorials/ECOSTRESS_Tutorial.html#d.-perform-k-d-tree-resampling",
    "title": "Working with ECOSTRESS Evapotranspiration Data",
    "section": "3d. Perform K-D Tree Resampling",
    "text": "3d. Perform K-D Tree Resampling\n\nRemember that the resampling has been split into two steps. In section 3b. arrays containing the nearest neighbor to each grid point were created. The second step is to use those arrays to retrieve a resampled result.\n\n# Perform K-D Tree nearest neighbor resampling (swath 2 grid conversion)\nETgeo = kdt.get_sample_from_neighbour_info('nn', areaDef.shape, ecoSD, index, outdex, indexArr, fill_value=None)\n\n\n\nAbove, resample the swath ecoSD array using nearest neighbor (already calculated in section 3b. and defined above as the index, outdex, and indexArr), and also set the fill value that was defined in section 3c.\n\n\nBelow, define the geotransform for the output (upper left x, horizontal pixel size, rotation, upper left y, rotation, vertical pixel size).\n\n# Define the geotransform \ngt = [areaDef.area_extent[0], ps, 0, areaDef.area_extent[3], 0, -ps]\ngt\n\n[np.float64(-121.86703190925778),\n np.float64(0.0006302582963630697),\n 0,\n np.float64(39.05327678830593),\n 0,\n np.float64(-0.0006302582963630697)]"
  },
  {
    "objectID": "python/tutorials/ECOSTRESS_Tutorial.html#e.-basic-image-processing",
    "href": "python/tutorials/ECOSTRESS_Tutorial.html#e.-basic-image-processing",
    "title": "Working with ECOSTRESS Evapotranspiration Data",
    "section": "3e. Basic Image Processing",
    "text": "3e. Basic Image Processing\n\nApply the scale factor and add offset and set the fill value defined in the previous section on the resampled data.\n\nETgeo = ETgeo * sf + add_off            # Apply Scale Factor and Add Offset\nETgeo[ETgeo == fv * sf + add_off] = fv  # Set Fill Value\n\n\n\nRerun steps 3c - 3e for ETinstUncertainty.\n\ns = ecoSDS[1]\necoSD = f[s][()]\ntry:\n    fv = int(f[s].attrs['_FillValue'])\nexcept KeyError:\n    fv = None\nexcept ValueError:\n    fv = f[s].attrs['_FillValue'][0]\ntry:\n    sf = f[s].attrs['_Scale'][0]\nexcept:\n    sf = 1\ntry:\n    add_off = f[s].attrs['_Offset'][0]\nexcept:\n    add_off = 0\nUNgeo = kdt.get_sample_from_neighbour_info('nn', areaDef.shape, ecoSD, index, outdex, indexArr, fill_value=None)\nUNgeo = UNgeo * sf + add_off\nUNgeo[UNgeo == fv * sf + add_off] = fv\n\n\n\n\n\nExporting Results"
  },
  {
    "objectID": "python/tutorials/ECOSTRESS_Tutorial.html#a.-set-up-a-dictionary",
    "href": "python/tutorials/ECOSTRESS_Tutorial.html#a.-set-up-a-dictionary",
    "title": "Working with ECOSTRESS Evapotranspiration Data",
    "section": "4a. Set Up a Dictionary",
    "text": "4a. Set Up a Dictionary\n\nIn this section, create a dictionary containing each of the arrays that will be exported as GeoTIFFs.\n\n# Set up dictionary of arrays to export\noutFiles = {'ETinst': ETgeo, 'ETinstUncertainty': UNgeo}"
  },
  {
    "objectID": "python/tutorials/ECOSTRESS_Tutorial.html#b.-define-crs-and-export-as-geotiffs",
    "href": "python/tutorials/ECOSTRESS_Tutorial.html#b.-define-crs-and-export-as-geotiffs",
    "title": "Working with ECOSTRESS Evapotranspiration Data",
    "section": "4b. Define CRS and Export as GeoTIFFs",
    "text": "4b. Define CRS and Export as GeoTIFFs\n\nNow that the data have been imported and resampled into a gridded raster array, export the results as GeoTIFFs using a for loop in this section.\n\nfv = np.nan\n\n\n# Loop through each item in dictionary created above\nfor file in outFiles:\n    \n    # Set up output name\n    outName = join(outDir, '{}_{}.tif'.format(ecoName, file))\n    print(\"output file:\\n{}\\n\".format(outName))\n    \n    # Get driver, specify dimensions, define and set output geotransform\n    height, width = outFiles[file].shape\n    driv = gdal.GetDriverByName('GTiff')\n    dataType = gdal_array.NumericTypeCodeToGDALTypeCode(outFiles[file].dtype)\n    d = driv.Create(outName, width, height, 1, dataType)\n    d.SetGeoTransform(gt)\n        \n    # Create and set output projection, write output array data\n    # Define target SRS\n    srs = osr.SpatialReference()\n    srs.ImportFromEPSG(int(epsg))\n    d.SetProjection(srs.ExportToWkt())\n    srs.ExportToWkt()\n    \n    # Write array to band\n    band = d.GetRasterBand(1)\n    band.WriteArray(outFiles[file])\n    \n    # Define fill value if it exists, if not, set to mask fill value\n    if fv is not None and fv != 'NaN':\n        band.SetNoDataValue(fv)\n    else:\n        try:\n            band.SetNoDataValue(outFiles[file].fill_value)\n        except:\n            pass\n    band.FlushCache()\n    d, band = None, None    \n\noutput file:\n..\\output\\ECOSTRESS_L3_ET_PT-JPL_00468_007_20180805T220314_0601_04_ETinst.tif\n\noutput file:\n..\\output\\ECOSTRESS_L3_ET_PT-JPL_00468_007_20180805T220314_0601_04_ETinstUncertainty.tif\n\n\n\n\n\n\n\nCombining ECOSTRESS and AmeriFlux Tower Data"
  },
  {
    "objectID": "python/tutorials/ECOSTRESS_Tutorial.html#a.-loading-tables-with-pandas",
    "href": "python/tutorials/ECOSTRESS_Tutorial.html#a.-loading-tables-with-pandas",
    "title": "Working with ECOSTRESS Evapotranspiration Data",
    "section": "5a. Loading Tables with Pandas",
    "text": "5a. Loading Tables with Pandas\n\nIn this section, begin by highlighting how to open a csv file using the pandas package.\n\nThe AmeriFlux tower data was provided by Mike Goulden for the AmeriFlux US-CZ3 tower. The csv includes half-hourly observations of Latent Heat (W/m\\(^{2}\\)) for the same day as the ECOSTRESS observation.\n\n\n# Import csv file with AmeriFlux data and drop NaNs\ntowerData = pd.read_csv('tower_data.csv')\ntowerData = towerData.dropna()\n\n\n\nNext, use the parser package and a lambda function to go through each time stamp and reformat to date and time objects.\n\n# Define a lambda function to use the parser packgage to convert each time stamp to a datetime object\ntowerData[\"Date/Time\"] = towerData[\"Time\"].apply(lambda x: parser.parse(x))\ntowerData[\"Time\"] = towerData[\"Date/Time\"].apply(lambda x: datetime.time(x.hour, x.minute))\ntowerData = towerData[[\"Date/Time\", \"Time\", \"LE\"]]\ntowerData\n\n\n\n\n\n\n\n\nDate/Time\nTime\nLE\n\n\n\n\n1\n2018-08-05 09:30:00\n09:30:00\n18.027752\n\n\n3\n2018-08-05 10:00:00\n10:00:00\n68.495970\n\n\n5\n2018-08-05 10:30:00\n10:30:00\n89.050031\n\n\n7\n2018-08-05 11:00:00\n11:00:00\n154.564199\n\n\n9\n2018-08-05 11:30:00\n11:30:00\n118.685090\n\n\n11\n2018-08-05 12:00:00\n12:00:00\n107.071870\n\n\n13\n2018-08-05 12:30:00\n12:30:00\n189.551249\n\n\n15\n2018-08-05 13:00:00\n13:00:00\n129.985112\n\n\n19\n2018-08-05 14:00:00\n14:00:00\n139.651745\n\n\n21\n2018-08-05 14:30:00\n14:30:00\n155.416112\n\n\n23\n2018-08-05 15:00:00\n15:00:00\n124.511936\n\n\n25\n2018-08-05 15:30:00\n15:30:00\n106.666146\n\n\n27\n2018-08-05 16:00:00\n16:00:00\n176.321112\n\n\n29\n2018-08-05 16:30:00\n16:30:00\n203.363058\n\n\n31\n2018-08-05 17:00:00\n17:00:00\n166.714904\n\n\n33\n2018-08-05 17:30:00\n17:30:00\n124.236028\n\n\n35\n2018-08-05 18:00:00\n18:00:00\n96.232450\n\n\n37\n2018-08-05 18:30:00\n18:30:00\n133.121365\n\n\n39\n2018-08-05 19:00:00\n19:00:00\n44.660266\n\n\n41\n2018-08-05 19:30:00\n19:30:00\n11.618052\n\n\n43\n2018-08-05 20:00:00\n20:00:00\n44.201492"
  },
  {
    "objectID": "python/tutorials/ECOSTRESS_Tutorial.html#b.-locate-ecostress-pixel-from-latlon-coordinates",
    "href": "python/tutorials/ECOSTRESS_Tutorial.html#b.-locate-ecostress-pixel-from-latlon-coordinates",
    "title": "Working with ECOSTRESS Evapotranspiration Data",
    "section": "5b. Locate ECOSTRESS Pixel from Lat/Lon Coordinates",
    "text": "5b. Locate ECOSTRESS Pixel from Lat/Lon Coordinates\n\nCalculate the gridded pixel nearest to the tower location.\n\ntowerLat, towerLon = 37.0674, -119.1951  # AmeriFlux US-CZ3 tower location\n\n# Calculate tower lat/lon distance from upper left corner, then divide by pixel size to find x,y pixel location\nTcol = int(round((towerLon - gt[0]) / gt[1]))\nTrow = int(round((towerLat - gt[3]) / gt[5]))\n\n# Print ET at the tower location\nETgeo[Trow, Tcol]                               \n\nnp.float64(172.5819549560547)\n\n\n\n\n\n\nVisualizing Data"
  },
  {
    "objectID": "python/tutorials/ECOSTRESS_Tutorial.html#a.-create-a-colormap",
    "href": "python/tutorials/ECOSTRESS_Tutorial.html#a.-create-a-colormap",
    "title": "Working with ECOSTRESS Evapotranspiration Data",
    "section": "6a. Create a Colormap",
    "text": "6a. Create a Colormap\n\nBefore plotting the ET data, set up an Evapotranspiration color map using LinearSegmentedColormap from the matplotlib package.\n\n# Create a colormap for the ET data\nETcolors = [\"#f6e8c3\", \"#d8b365\", \"#99974a\", \"#53792d\", \"#6bdfd2\", \"#1839c5\"]\nETcmap = LinearSegmentedColormap.from_list(\"ET\", ETcolors)"
  },
  {
    "objectID": "python/tutorials/ECOSTRESS_Tutorial.html#b.-calculate-local-overpass-time",
    "href": "python/tutorials/ECOSTRESS_Tutorial.html#b.-calculate-local-overpass-time",
    "title": "Working with ECOSTRESS Evapotranspiration Data",
    "section": "6b. Calculate Local Overpass Time",
    "text": "6b. Calculate Local Overpass Time\n\nECOSTRESS observation times are reported in Universal Time Coordinated (UTC). Below, grab the observation time from the filename and convert to local time using the longitude location of the tower.\n\n# Grab UTC time of observation from file name\necoTime = ecoName.split('_')[-3]\necoTime\n\n'20180805T220314'\n\n\n\n\nNext, convert UTC observation time to local overpass time.\n\nobservationTime = parser.parse(ecoTime)\nsolarOverpass = observationTime + datetime.timedelta(hours=(np.radians(towerLon) / np.pi * 12))\noverpass = datetime.time(solarOverpass.hour, solarOverpass.minute)\ndate = observationTime.strftime('%Y-%m-%d')"
  },
  {
    "objectID": "python/tutorials/ECOSTRESS_Tutorial.html#c.-plot-et-data",
    "href": "python/tutorials/ECOSTRESS_Tutorial.html#c.-plot-et-data",
    "title": "Working with ECOSTRESS Evapotranspiration Data",
    "section": "6c. Plot ET Data",
    "text": "6c. Plot ET Data\n\nIn this section, begin by highlighting the functionality of the matplotlib plotting package. First, make a plot of the entire gridded ET output. Next, zoom in on the tower location and add some additional parameters to the plot. Finally, export the completed plot to an image file.\n\ntitle = 'ECO3ETPTJPL Evapotranspiration'\nSDSname = ecoSDS[0].split(\"/\")[-1]\nfig = plt.figure(figsize=(9.7,7.6))                                                       # Set the figure size (x,y)\nfig.suptitle(f'{title} ({ecoSDS[0].split(\"/\")[-1]})\\n{date} at {overpass}', fontsize=22)  # Add title for the plots\nplt.axis('off')                                                                           # Remove axes from plot\nim = plt.imshow(ETgeo, cmap=ETcmap);                                                        # Plot array using colormap\nplt.scatter(Tcol, Trow, color=\"black\", marker='x')                                        # Plot tower location\n\n# Add a colormap legend\nplt.colorbar(im, orientation='horizontal', fraction=0.05, pad=0.004, label=f'ET ({units})', shrink=0.6).outline.set_visible(True)"
  },
  {
    "objectID": "python/tutorials/ECOSTRESS_Tutorial.html#d.-exporting-an-image",
    "href": "python/tutorials/ECOSTRESS_Tutorial.html#d.-exporting-an-image",
    "title": "Working with ECOSTRESS Evapotranspiration Data",
    "section": "6d. Exporting an Image",
    "text": "6d. Exporting an Image\n\nZoom in to get a closer look at the region surrounding the AmeriFlux tower by creating a subset.\n\n# Set a Radius and calculate subset region from flux tower location (row, col)\nradius = 1700\nETsubset = ETgeo[(Trow - radius):(Trow + radius + 1), (Tcol - radius):(Tcol + radius + 1)]\n\n\n\nMake another plot, this time zoomed in to the tower location. Export the plot as a .png file.\n\nfig = plt.figure(figsize=(14,12))                                                 # Set the figure size (x,y)\nfig.suptitle(f'{title} ({SDSname})\\n{date} at {overpass}', fontsize=26)           # Add title for the plots\nplt.axis('off')                                                                   # Remove axes from plot\nim = plt.imshow(ETsubset, cmap=ETcmap);                                             # Plot array using colormap\nplt.scatter(ETsubset.shape[0]/2, ETsubset.shape[1]/2, color=\"black\", marker='x')  # Tower is in middle of subset\n\n# Add a colormap legend\nplt.colorbar(im, orientation='horizontal', fraction=0.05, pad=0.004, label=f'ET ({units})', shrink=0.6).outline.set_visible(True)\n\n# Set up file name and export to png file\nfigure_filename = join(outDir, \"{}_{}.png\".format(ecoName, SDSname))\nprint(\"figure filename: {}\".format(figure_filename))\nfig.savefig(figure_filename, dpi=300)\nplt.show()\n\nfigure filename: ..\\output\\ECOSTRESS_L3_ET_PT-JPL_00468_007_20180805T220314_0601_04_ETinst.png\n\n\n\n\n\n\n\n\n\n\n\n\n\nComparing Observations"
  },
  {
    "objectID": "python/tutorials/ECOSTRESS_Tutorial.html#a.-calculate-distribution-of-ecostress-data",
    "href": "python/tutorials/ECOSTRESS_Tutorial.html#a.-calculate-distribution-of-ecostress-data",
    "title": "Working with ECOSTRESS Evapotranspiration Data",
    "section": "7a. Calculate Distribution of ECOSTRESS Data",
    "text": "7a. Calculate Distribution of ECOSTRESS Data\n\nFirst, collect a 3x3 grid centered on the flux tower pixel as a subset to calculate statistics on.\n\n# Subset data to 3x3 grid surrounding flux tower for both layers\nETfootprint = ETgeo[(Trow - 1):(Trow + 2), (Tcol - 1):(Tcol + 2)] \nUNfootprint = UNgeo[(Trow - 1):(Trow + 2), (Tcol - 1):(Tcol + 2)] \nprint(ETfootprint)\n\n[[161.3531036376953 161.25172424316406 145.38726806640625]\n [156.7587432861328 172.5819549560547 172.5819549560547]\n [140.9962158203125 140.9962158203125 185.904052734375]]\n\n\n\n\nIn case the 3x3 grid contains missing values, use np.nanmedian to ignoring missing values and calculate the measure of central tendency.\n\nETmedian = np.nanmedian(ETfootprint)\nUNmedian = np.nanmedian(UNfootprint)\nprint(f\"Median ET: {ETmedian:0.3f} \\nUncertainty: {UNmedian:0.3f}\")\n\nMedian ET: 161.252 \nUncertainty: 95.294\n\n\n\n\nNext, generate a probability density function for the 3x3 grid of ET values.\n\npd.DataFrame({title: ETfootprint.flatten()}).plot.kde()                                   # Pandas Kernel Density Estimate\nplt.title(f'Probability Density of {SDSname} Surrounding US-CZ3\\n{date} at {overpass}');  # Title\nplt.xlabel(f'{SDSname} ({units})');                                                       # X-axis label"
  },
  {
    "objectID": "python/tutorials/ECOSTRESS_Tutorial.html#b.-visualize-ground-observations",
    "href": "python/tutorials/ECOSTRESS_Tutorial.html#b.-visualize-ground-observations",
    "title": "Working with ECOSTRESS Evapotranspiration Data",
    "section": "7b. Visualize Ground Observations",
    "text": "7b. Visualize Ground Observations\n\nNext, examine the series of eddy covariance observations from the AmeriFlux US-CZ3 dataset.\n\nfig = plt.figure(figsize=(12,7))                                                                      # Set fig size (x,y)\nax = fig.add_subplot(111)                                                                             # Create a subplot\nax.plot(towerData['Date/Time'], towerData.LE, 'k', lw=2.5, color='black')                                     # Plot as a black line\nax.set_title(f'Time Series of Eddy Covariance Data at Site US-CZ3', fontsize=20, fontweight='bold');  # Set Title\nax.set_ylabel('Latent Heat (W/m^2)', fontsize=18);                                                    # Y-axis label\nax.set_xlabel(f'Time of Day ({date})', fontsize=18);                                                  # X-axis label\nax.set_xticks(towerData['Date/Time']);                                                              # Set the x ticks\nax.set_xticklabels(towerData['Time'], rotation=45,fontsize=12);                                          # Set x tick labels\n\n\n\n\n\n\n\n\n\n\nAbove, we can see the daily range in Latent Heat as captured by the eddy covariance observations on the flux tower."
  },
  {
    "objectID": "python/tutorials/ECOSTRESS_Tutorial.html#c.-combine-ecostress-and-ground-observations",
    "href": "python/tutorials/ECOSTRESS_Tutorial.html#c.-combine-ecostress-and-ground-observations",
    "title": "Working with ECOSTRESS Evapotranspiration Data",
    "section": "7c. Combine ECOSTRESS and Ground Observations",
    "text": "7c. Combine ECOSTRESS and Ground Observations\n\nFinally, compare the ECOSTRESS Evapotranspiration and uncertainty with the time series of observations from the flux tower.\n\n# Set the figure size, create a subplot\nfig = plt.figure(1, figsize=(12, 7))\nax = fig.add_subplot(111)\n\n# Plot the flux tower observatisons followed by the ecostress median ET and median uncertainty\nax.plot(towerData['Date/Time'], towerData.LE, 'k', lw=2.5, color='black')\nax.plot(solarOverpass, ETmedian, 'bo', ms=10, color='darkgreen')\nax.errorbar(solarOverpass, ETmedian, yerr=UNmedian, lw=2.0, c='lightgreen', fmt='o', capsize=3, capthick=2)\n\n# Set x/y axes and labels\nax.set_xlabel(f'Time of Day ({date})', fontsize=18);\nax.set_xticks(towerData['Date/Time']);\nax.set_xticklabels(towerData.Time, rotation=45,fontsize=12);\nax.set_ylabel(\"Latent Heat Flux (Wm2)\", fontsize=16, fontweight='bold')\n\n# Add a title and export figure as png file\nax.set_title(f\"Time Series of Eddy Covariance Data at Site US-CZ3\\n vs. {title} ({SDSname})\", fontsize=22)\nfigure_filename = join(outDir, \"{}_{}_vs_fluxtower.png\".format(ecoName, SDSname))\nprint(\"figure filename: {}\".format(figure_filename))\nfig.savefig(figure_filename, bbox_inches='tight')\n\nfigure filename: ..\\output\\ECOSTRESS_L3_ET_PT-JPL_00468_007_20180805T220314_0601_04_ETinst_vs_fluxtower.png"
  },
  {
    "objectID": "python/tutorials/ECOSTRESS_Tutorial.html#citations",
    "href": "python/tutorials/ECOSTRESS_Tutorial.html#citations",
    "title": "Working with ECOSTRESS Evapotranspiration Data",
    "section": "Citations",
    "text": "Citations\n\nHook, S., Fisher, J. (2019). ECOSTRESS Evapotranspiration PT-JPL Daily L3 Global 70 m V001 [Data set]. NASA EOSDIS Land Processes DAAC. Accessed 2021-03-11 from https://doi.org/10.5067/ECOSTRESS/ECO3ETPTJPL.001.\nGoulden, M., (2018). AmeriFlux US-CZ3 Sierra Critical Zone, Sierra Transect, Sierran Mixed Conifer, P301, doi:10.17190/AMF/1419512.\nKrehbiel, C., and Halverson, G.H., (2019). Working with ECOSTRESS Evapotranspiration Data [Jupyter Notebook]. Retrieved from https://git.earthdata.nasa.gov/projects/LPDUR/repos/tutorial-ecostress/browse"
  },
  {
    "objectID": "python/tutorials/ECOSTRESS_Tutorial.html#contact-info",
    "href": "python/tutorials/ECOSTRESS_Tutorial.html#contact-info",
    "title": "Working with ECOSTRESS Evapotranspiration Data",
    "section": "Contact Info:",
    "text": "Contact Info:\nMaterial written by Cole Krehbiel1 and Gregory Halverson2\nEmail: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)&lt; Website: https://www.earthdata.nasa.gov/centers/lp-daac\n1Innovate! Inc., contractor to the U.S. Geological Survey, Earth Resources Observation and Science (EROS) Center, Sioux Falls, South Dakota, 57198-001, USA. Work performed under USGS contract G15PD00467 for LP DAAC3.\n2Jet Propulsion Laboratory, California Institute of Technology, Pasadena, CA, USA.\n3LP DAAC Work performed under NASA contract NNG14HH33I."
  },
  {
    "objectID": "python/tutorials/Work_with_ECOSTRESS_L2T_LSTE_data.html",
    "href": "python/tutorials/Work_with_ECOSTRESS_L2T_LSTE_data.html",
    "title": "Working with ECOSTRESS L2T LSTE Data",
    "section": "",
    "text": "Summary\nThis notebook will show how to access ECOsystem Spaceborne Thermal Radiometer Experiment on Space Station (ECOSTRESS) data programmatically using the earthaccess python library to authenticate, search, download, and stream (directly access) data. It shows how to work with ECOSTRESS Tiled Land Surface Temperature and Emissivity (ECOSTRESS_L2T_LSTE) product hosted in the Earthdata Cloud and managed by the Land Processes Distributed Active Archive Center (LP DAAC).\nBackground\nThe ECOSTRESS mission is answering these questions by accurately measuring the temperature of plants. Plants regulate their temperature by releasing water through tiny pores on their leaves called stomata. If they have sufficient water they can maintain their temperature, but if there is insufficient water, their temperatures rise and this temperature rise can be measured with ECOSTRESS. The images acquired by ECOSTRESS are the most detailed temperature images of the surface ever acquired from space and can be used to measure the temperature of an individual farm field. These temperature images, along with auxiliary inputs, are used to produce one of the primary science outputs of ECOSTRESS: evapotranspiration, an indicator of plant health via the measure of evaporation and transpiration of water through a plant.\nMore details about ECOSTRESS and its associated products can be found on the ECOSTRESS website and ECOSTRESS product pages hosted by the Land Processes Distributed Active Archive Center (LP DAAC).\nLearning Objectives\nRequirements\nTutorial Outline"
  },
  {
    "objectID": "python/tutorials/Work_with_ECOSTRESS_L2T_LSTE_data.html#setup",
    "href": "python/tutorials/Work_with_ECOSTRESS_L2T_LSTE_data.html#setup",
    "title": "Working with ECOSTRESS L2T LSTE Data",
    "section": "1. Setup",
    "text": "1. Setup\nImport the required libraries.\n\n# Import Packages\nimport warnings\n# Some cells may generate warnings that we can ignore. Comment below lines to see.\nwarnings.filterwarnings('ignore')\n\nimport os, sys, shutil\nimport earthaccess\nimport numpy as np\nfrom osgeo import gdal\nimport rasterio as rio\nimport rioxarray as rxr\nimport xarray as xr\nimport hvplot.xarray\nimport hvplot.pandas\nimport geopandas as gp\nfrom zipfile import ZipFile \n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAuthentication\nLog into Earthdata using the Auth and login functions from the earthaccess library. The persist=True argument will create a local .netrc file if it doesn’t exist, or add your login info to an existing .netrc file. If no Earthdata Login credentials are found in the .netrc you’ll be prompted for them.\n\nauth = earthaccess.login(persist = True)\n# are we authenticated?\nprint(auth.authenticated)\n\nTrue"
  },
  {
    "objectID": "python/tutorials/Work_with_ECOSTRESS_L2T_LSTE_data.html#searching-for-ecostress-l2t-lste-data",
    "href": "python/tutorials/Work_with_ECOSTRESS_L2T_LSTE_data.html#searching-for-ecostress-l2t-lste-data",
    "title": "Working with ECOSTRESS L2T LSTE Data",
    "section": "2. Searching for ECOSTRESS L2T LSTE Data",
    "text": "2. Searching for ECOSTRESS L2T LSTE Data\nIn this example, we will use the cloud-hosted ECOSTRESS_L2T_LSTE product but the searching process can be used with other EMIT or ECOSTRESS products, other collections, or different data providers, as well as across multiple catalogs with some modification. The Land Surface Temperature and Emissivity values from ECOSTRESS Level 2 Tiled Land Surface Temperature (ECO_L2T_LSTE) are derived from five thermal infrared (TIR) bands using a physics-based Temperature and Emissivity Separation (TES) algorithm. This tiled data product uses a modified version of the Military Grid Reference System (MGRS) which divides Universal Transverse Mercator (UTM) zones into square tiles that are 109.8 km by 109.8 km with a 70 meter (m) spatial resolution.\n\nDefine Your Query Parameters\nWe can search for granules using attributes such as collection short name, collection ID, acquisition time, and spatial footprint.\n\nSpatial region of interest (ROI)\nFor this example, our spatial region of interest (ROI) will be Boulder city boundary. when we are working with multi-feature ROI, we can use a bounding box with larger spatial extent including all the features. To do this, we will first open a geojson file containing our region of interest (ROI) then simplify it to a bounding box by getting the bounds and putting them into a tuple. We will use the total_bounds property to get the bounding box of our ROI, and add that to a python tuple, which is the expected data type for the bounding_box parameter earthaccess search_data.\nNote: If our features are spread out spatially in the ROI, we can search for data available for each feature separately to avoid accessing a large volume of data we do not need.\nOur ROI is stored as a .zip file. First we need to extract all the members of the zip into a specific location.\n\nwith ZipFile('../../data/City_of_Boulder_City_Limits.zip', 'r') as zObject: \n    zObject.extractall( path=\"../../data\")\n\nBelow, the polygon is opened using geopandas library and Coordinate Reference System (crs) is printed. Our polygon is in geographic coordinate reference system (EPSG:3857).\n\npolygon = gp.read_file('../../data/City_of_Boulder_City_Limits.shp')\npolygon.crs\n\n&lt;Projected CRS: EPSG:3857&gt;\nName: WGS 84 / Pseudo-Mercator\nAxis Info [cartesian]:\n- X[east]: Easting (metre)\n- Y[north]: Northing (metre)\nArea of Use:\n- name: World between 85.06°S and 85.06°N.\n- bounds: (-180.0, -85.06, 180.0, 85.06)\nCoordinate Operation:\n- name: Popular Visualisation Pseudo-Mercator\n- method: Popular Visualisation Pseudo Mercator\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\nSince this data is in EPSG:3857, we need to reproject to EPSG:4326 to get latitude and longitude values for our search. We can use the to_crs method from geopandas to reproject our polygon.\n\npolygon_4326 = polygon.to_crs(\"EPSG:4326\")\n\n\npolygon_4326.hvplot(tiles='ESRI', color='#d95f02',alpha=0.6, crs='EPSG:4326', frame_height=405, frame_width=720, fontscale=2) \n\n\n\n\n\n  \n\n\n\n\n\nbbox = tuple(list(polygon_4326.total_bounds))\nbbox\n\n(np.float64(-105.3014509070552),\n np.float64(39.956916683424616),\n np.float64(-105.1780988154023),\n np.float64(40.094484708971066))\n\n\nBelow, the parameters including provider, short_name, version, bounding_box, temporal, and count are used for our query.\n\nresults = earthaccess.search_data(\n    provider='LPCLOUD',\n    short_name='ECO_L2T_LSTE',\n    version='002',\n    bounding_box=bbox,\n    temporal=('2023-07-01','2023-08-01'),\n    count=100\n)\n\nNext, get the downloadable links for LSTE and quality layers using data_links() method from earthaccess.\n\nlst_links = [l for dl in results for l in dl.data_links() if 'LST.tif' in l]\nlst_links\n\n['https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28279_007_13TDE_20230702T080920_0710_01/ECOv002_L2T_LSTE_28279_007_13TDE_20230702T080920_0710_01_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28279_007_13TDE_20230702T080920_0711_02/ECOv002_L2T_LSTE_28279_007_13TDE_20230702T080920_0711_02_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28279_008_13TDE_20230702T081012_0710_01/ECOv002_L2T_LSTE_28279_008_13TDE_20230702T081012_0710_01_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28279_008_13TDE_20230702T081012_0711_02/ECOv002_L2T_LSTE_28279_008_13TDE_20230702T081012_0711_02_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28283_009_13TDE_20230702T143900_0710_01/ECOv002_L2T_LSTE_28283_009_13TDE_20230702T143900_0710_01_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28283_009_13TDE_20230702T143900_0711_02/ECOv002_L2T_LSTE_28283_009_13TDE_20230702T143900_0711_02_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28283_010_13TDE_20230702T143952_0710_01/ECOv002_L2T_LSTE_28283_010_13TDE_20230702T143952_0710_01_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28283_010_13TDE_20230702T143952_0711_02/ECOv002_L2T_LSTE_28283_010_13TDE_20230702T143952_0711_02_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28298_010_13TDE_20230703T134950_0710_01/ECOv002_L2T_LSTE_28298_010_13TDE_20230703T134950_0710_01_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28298_010_13TDE_20230703T134950_0711_02/ECOv002_L2T_LSTE_28298_010_13TDE_20230703T134950_0711_02_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28298_011_13TDE_20230703T135042_0710_01/ECOv002_L2T_LSTE_28298_011_13TDE_20230703T135042_0710_01_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28298_011_13TDE_20230703T135042_0711_02/ECOv002_L2T_LSTE_28298_011_13TDE_20230703T135042_0711_02_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28340_007_13TDE_20230706T063131_0712_01/ECOv002_L2T_LSTE_28340_007_13TDE_20230706T063131_0712_01_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28340_007_13TDE_20230706T063131_0712_02/ECOv002_L2T_LSTE_28340_007_13TDE_20230706T063131_0712_02_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28340_008_13TDE_20230706T063223_0712_01/ECOv002_L2T_LSTE_28340_008_13TDE_20230706T063223_0712_01_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28340_008_13TDE_20230706T063223_0712_02/ECOv002_L2T_LSTE_28340_008_13TDE_20230706T063223_0712_02_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28344_003_13TDE_20230706T130157_0710_01/ECOv002_L2T_LSTE_28344_003_13TDE_20230706T130157_0710_01_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28344_003_13TDE_20230706T130157_0712_02/ECOv002_L2T_LSTE_28344_003_13TDE_20230706T130157_0712_02_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28359_009_13TDE_20230707T121349_0710_01/ECOv002_L2T_LSTE_28359_009_13TDE_20230707T121349_0710_01_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28359_009_13TDE_20230707T121349_0712_02/ECOv002_L2T_LSTE_28359_009_13TDE_20230707T121349_0712_02_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28405_006_13TDE_20230710T112640_0710_01/ECOv002_L2T_LSTE_28405_006_13TDE_20230710T112640_0710_01_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28405_006_13TDE_20230710T112640_0712_02/ECOv002_L2T_LSTE_28405_006_13TDE_20230710T112640_0712_02_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28420_007_13TDE_20230711T103839_0710_01/ECOv002_L2T_LSTE_28420_007_13TDE_20230711T103839_0710_01_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28420_007_13TDE_20230711T103839_0712_02/ECOv002_L2T_LSTE_28420_007_13TDE_20230711T103839_0712_02_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28462_008_13TDE_20230714T032038_0710_01/ECOv002_L2T_LSTE_28462_008_13TDE_20230714T032038_0710_01_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28462_008_13TDE_20230714T032038_0712_02/ECOv002_L2T_LSTE_28462_008_13TDE_20230714T032038_0712_02_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28462_009_13TDE_20230714T032130_0710_01/ECOv002_L2T_LSTE_28462_009_13TDE_20230714T032130_0710_01_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28462_009_13TDE_20230714T032130_0712_02/ECOv002_L2T_LSTE_28462_009_13TDE_20230714T032130_0712_02_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28466_008_13TDE_20230714T095136_0710_01/ECOv002_L2T_LSTE_28466_008_13TDE_20230714T095136_0710_01_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28466_008_13TDE_20230714T095136_0712_02/ECOv002_L2T_LSTE_28466_008_13TDE_20230714T095136_0712_02_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28523_006_13TDE_20230718T014508_0710_01/ECOv002_L2T_LSTE_28523_006_13TDE_20230718T014508_0710_01_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28523_006_13TDE_20230718T014508_0712_02/ECOv002_L2T_LSTE_28523_006_13TDE_20230718T014508_0712_02_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28527_009_13TDE_20230718T081442_0710_01/ECOv002_L2T_LSTE_28527_009_13TDE_20230718T081442_0710_01_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28527_009_13TDE_20230718T081442_0712_02/ECOv002_L2T_LSTE_28527_009_13TDE_20230718T081442_0712_02_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28569_006_13TDE_20230721T005647_0710_01/ECOv002_L2T_LSTE_28569_006_13TDE_20230721T005647_0710_01_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28569_006_13TDE_20230721T005647_0712_02/ECOv002_L2T_LSTE_28569_006_13TDE_20230721T005647_0712_02_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28569_007_13TDE_20230721T005739_0710_01/ECOv002_L2T_LSTE_28569_007_13TDE_20230721T005739_0710_01_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28569_007_13TDE_20230721T005739_0712_02/ECOv002_L2T_LSTE_28569_007_13TDE_20230721T005739_0712_02_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28691_007_13TDE_20230728T214334_0710_01/ECOv002_L2T_LSTE_28691_007_13TDE_20230728T214334_0710_01_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28691_007_13TDE_20230728T214334_0712_02/ECOv002_L2T_LSTE_28691_007_13TDE_20230728T214334_0712_02_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28691_008_13TDE_20230728T214425_0710_01/ECOv002_L2T_LSTE_28691_008_13TDE_20230728T214425_0710_01_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28691_008_13TDE_20230728T214425_0712_02/ECOv002_L2T_LSTE_28691_008_13TDE_20230728T214425_0712_02_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28706_007_13TDE_20230729T205539_0710_01/ECOv002_L2T_LSTE_28706_007_13TDE_20230729T205539_0710_01_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28706_007_13TDE_20230729T205539_0712_02/ECOv002_L2T_LSTE_28706_007_13TDE_20230729T205539_0712_02_LST.tif']\n\n\n\nqc_links = [l for dl in results for l in dl.data_links() if 'QC.tif' in l]\nqc_links\n\n['https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28279_007_13TDE_20230702T080920_0710_01/ECOv002_L2T_LSTE_28279_007_13TDE_20230702T080920_0710_01_QC.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28279_007_13TDE_20230702T080920_0711_02/ECOv002_L2T_LSTE_28279_007_13TDE_20230702T080920_0711_02_QC.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28279_008_13TDE_20230702T081012_0710_01/ECOv002_L2T_LSTE_28279_008_13TDE_20230702T081012_0710_01_QC.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28279_008_13TDE_20230702T081012_0711_02/ECOv002_L2T_LSTE_28279_008_13TDE_20230702T081012_0711_02_QC.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28283_009_13TDE_20230702T143900_0710_01/ECOv002_L2T_LSTE_28283_009_13TDE_20230702T143900_0710_01_QC.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28283_009_13TDE_20230702T143900_0711_02/ECOv002_L2T_LSTE_28283_009_13TDE_20230702T143900_0711_02_QC.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28283_010_13TDE_20230702T143952_0710_01/ECOv002_L2T_LSTE_28283_010_13TDE_20230702T143952_0710_01_QC.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28283_010_13TDE_20230702T143952_0711_02/ECOv002_L2T_LSTE_28283_010_13TDE_20230702T143952_0711_02_QC.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28298_010_13TDE_20230703T134950_0710_01/ECOv002_L2T_LSTE_28298_010_13TDE_20230703T134950_0710_01_QC.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28298_010_13TDE_20230703T134950_0711_02/ECOv002_L2T_LSTE_28298_010_13TDE_20230703T134950_0711_02_QC.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28298_011_13TDE_20230703T135042_0710_01/ECOv002_L2T_LSTE_28298_011_13TDE_20230703T135042_0710_01_QC.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28298_011_13TDE_20230703T135042_0711_02/ECOv002_L2T_LSTE_28298_011_13TDE_20230703T135042_0711_02_QC.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28340_007_13TDE_20230706T063131_0712_01/ECOv002_L2T_LSTE_28340_007_13TDE_20230706T063131_0712_01_QC.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28340_007_13TDE_20230706T063131_0712_02/ECOv002_L2T_LSTE_28340_007_13TDE_20230706T063131_0712_02_QC.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28340_008_13TDE_20230706T063223_0712_01/ECOv002_L2T_LSTE_28340_008_13TDE_20230706T063223_0712_01_QC.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28340_008_13TDE_20230706T063223_0712_02/ECOv002_L2T_LSTE_28340_008_13TDE_20230706T063223_0712_02_QC.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28344_003_13TDE_20230706T130157_0710_01/ECOv002_L2T_LSTE_28344_003_13TDE_20230706T130157_0710_01_QC.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28344_003_13TDE_20230706T130157_0712_02/ECOv002_L2T_LSTE_28344_003_13TDE_20230706T130157_0712_02_QC.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28359_009_13TDE_20230707T121349_0710_01/ECOv002_L2T_LSTE_28359_009_13TDE_20230707T121349_0710_01_QC.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28359_009_13TDE_20230707T121349_0712_02/ECOv002_L2T_LSTE_28359_009_13TDE_20230707T121349_0712_02_QC.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28405_006_13TDE_20230710T112640_0710_01/ECOv002_L2T_LSTE_28405_006_13TDE_20230710T112640_0710_01_QC.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28405_006_13TDE_20230710T112640_0712_02/ECOv002_L2T_LSTE_28405_006_13TDE_20230710T112640_0712_02_QC.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28420_007_13TDE_20230711T103839_0710_01/ECOv002_L2T_LSTE_28420_007_13TDE_20230711T103839_0710_01_QC.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28420_007_13TDE_20230711T103839_0712_02/ECOv002_L2T_LSTE_28420_007_13TDE_20230711T103839_0712_02_QC.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28462_008_13TDE_20230714T032038_0710_01/ECOv002_L2T_LSTE_28462_008_13TDE_20230714T032038_0710_01_QC.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28462_008_13TDE_20230714T032038_0712_02/ECOv002_L2T_LSTE_28462_008_13TDE_20230714T032038_0712_02_QC.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28462_009_13TDE_20230714T032130_0710_01/ECOv002_L2T_LSTE_28462_009_13TDE_20230714T032130_0710_01_QC.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28462_009_13TDE_20230714T032130_0712_02/ECOv002_L2T_LSTE_28462_009_13TDE_20230714T032130_0712_02_QC.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28466_008_13TDE_20230714T095136_0710_01/ECOv002_L2T_LSTE_28466_008_13TDE_20230714T095136_0710_01_QC.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28466_008_13TDE_20230714T095136_0712_02/ECOv002_L2T_LSTE_28466_008_13TDE_20230714T095136_0712_02_QC.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28523_006_13TDE_20230718T014508_0710_01/ECOv002_L2T_LSTE_28523_006_13TDE_20230718T014508_0710_01_QC.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28523_006_13TDE_20230718T014508_0712_02/ECOv002_L2T_LSTE_28523_006_13TDE_20230718T014508_0712_02_QC.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28527_009_13TDE_20230718T081442_0710_01/ECOv002_L2T_LSTE_28527_009_13TDE_20230718T081442_0710_01_QC.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28527_009_13TDE_20230718T081442_0712_02/ECOv002_L2T_LSTE_28527_009_13TDE_20230718T081442_0712_02_QC.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28569_006_13TDE_20230721T005647_0710_01/ECOv002_L2T_LSTE_28569_006_13TDE_20230721T005647_0710_01_QC.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28569_006_13TDE_20230721T005647_0712_02/ECOv002_L2T_LSTE_28569_006_13TDE_20230721T005647_0712_02_QC.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28569_007_13TDE_20230721T005739_0710_01/ECOv002_L2T_LSTE_28569_007_13TDE_20230721T005739_0710_01_QC.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28569_007_13TDE_20230721T005739_0712_02/ECOv002_L2T_LSTE_28569_007_13TDE_20230721T005739_0712_02_QC.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28691_007_13TDE_20230728T214334_0710_01/ECOv002_L2T_LSTE_28691_007_13TDE_20230728T214334_0710_01_QC.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28691_007_13TDE_20230728T214334_0712_02/ECOv002_L2T_LSTE_28691_007_13TDE_20230728T214334_0712_02_QC.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28691_008_13TDE_20230728T214425_0710_01/ECOv002_L2T_LSTE_28691_008_13TDE_20230728T214425_0710_01_QC.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28691_008_13TDE_20230728T214425_0712_02/ECOv002_L2T_LSTE_28691_008_13TDE_20230728T214425_0712_02_QC.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28706_007_13TDE_20230729T205539_0710_01/ECOv002_L2T_LSTE_28706_007_13TDE_20230729T205539_0710_01_QC.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28706_007_13TDE_20230729T205539_0712_02/ECOv002_L2T_LSTE_28706_007_13TDE_20230729T205539_0712_02_QC.tif']\n\n\nLet’s take a look at the ECOSTRESS tiled data file name:\n    Filename: **ECOv002_L2T_LSTE_28527_009_13TDE_20230718T081442_0710_01_LST.tif**   \n\n    ECO : Sensor  \n    v002 : Product Version  \n    L2T : Processing Level and Type (T = Tile)  \n    LSTE : Geophysical Parameter  \n    28527 : Orbit Number  \n    009 : Scene ID  \n    13TDE : Military Grid Reference System (MGRS) Tile ID  \n    20230718 : Date of Acquisition (YYYYMMDD)  \n    T081442 : Time of Acquisition (HHMMSS) (in UTC)  \n    0710 : Build ID of software that generated product, Major+Minor (2+2 digits)  \n    01 : Product Iteration Number  \n    LST : Layer/band Name (each layer is a separate file)  \n    .tif : Data Format for Tile  \nLooking at Military Grid Reference System (MGRS) Tile ID of the outputs, they all are all in UTM Zone 13."
  },
  {
    "objectID": "python/tutorials/Work_with_ECOSTRESS_L2T_LSTE_data.html#accessing-ecostress-l2t-land-surface-temperature-and-emissivity-data",
    "href": "python/tutorials/Work_with_ECOSTRESS_L2T_LSTE_data.html#accessing-ecostress-l2t-land-surface-temperature-and-emissivity-data",
    "title": "Working with ECOSTRESS L2T LSTE Data",
    "section": "3. Accessing ECOSTRESS L2T Land Surface Temperature and Emissivity Data",
    "text": "3. Accessing ECOSTRESS L2T Land Surface Temperature and Emissivity Data\nECOSTRESS data is stored in NASA’s Earthdata Cloud and can be accessed in different ways.\n\nDownloaded - This has been available since the existance of the NASA DAAC. Users can use the data link(s) to download the data files to their local working environment. This can be done whether the user is working from a non-cloud or cloud environment.\nStreamed - Streaming is on-the-fly random reading of remote files, i.e. files not saved locally. The data accessed, however, must be able to be held in the workspaces’ memory. This can be done whether the user is working from a non-cloud or cloud environment.\nAccessed in-place (i.e., direct s3 access) - This is only available for working environment deployed in AWS us-west-2.\n\nIn this example, we will show how to stream the data. For that, the gdal configuration is set and the one of our LSTE files is read into the workspace using open_rasterio from the rioxarray library. Since the file consists of only 1 layer, we can squeeze it, removing the band dimension.\n\nrio_env = rio.Env(GDAL_DISABLE_READDIR_ON_OPEN='EMPTY_DIR',\n                  GDAL_HTTP_COOKIEFILE=os.path.expanduser('~/cookies.txt'),\n                  GDAL_HTTP_COOKIEJAR=os.path.expanduser('~/cookies.txt'),\n                  GDAL_HTTP_MAX_RETRY=10,\n                  GDAL_HTTP_RETRY_DELAY=0.5)\nrio_env.__enter__()\n\n&lt;rasterio.env.Env at 0x1c53b81b770&gt;\n\n\n\neco_lst_ds = rxr.open_rasterio(lst_links[14]).squeeze('band', drop=True)\neco_lst_ds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (y: 1568, x: 1568)&gt; Size: 10MB\n[2458624 values with dtype=float32]\nCoordinates:\n  * x            (x) float64 13kB 4e+05 4.001e+05 ... 5.096e+05 5.097e+05\n  * y            (y) float64 13kB 4.5e+06 4.5e+06 4.5e+06 ... 4.39e+06 4.39e+06\n    spatial_ref  int64 8B 0\nAttributes:\n    AREA_OR_POINT:  Area\n    _FillValue:     nan\n    scale_factor:   1.0\n    add_offset:     0.0xarray.DataArrayy: 1568x: 1568...[2458624 values with dtype=float32]Coordinates: (3)x(x)float644e+05 4.001e+05 ... 5.097e+05array([399995., 400065., 400135., ..., 509545., 509615., 509685.],\n      shape=(1568,))y(y)float644.5e+06 4.5e+06 ... 4.39e+06array([4499965., 4499895., 4499825., ..., 4390415., 4390345., 4390275.],\n      shape=(1568,))spatial_ref()int640crs_wkt :PROJCS[\"WGS 84 / UTM zone 13N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984projected_crs_name :WGS 84 / UTM zone 13Ngrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-105.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"WGS 84 / UTM zone 13N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :399960.0 70.0 0.0 4500000.0 0.0 -70.0array(0)Indexes: (2)xPandasIndexPandasIndex(Index([399995.0, 400065.0, 400135.0, 400205.0, 400275.0, 400345.0, 400415.0,\n       400485.0, 400555.0, 400625.0,\n       ...\n       509055.0, 509125.0, 509195.0, 509265.0, 509335.0, 509405.0, 509475.0,\n       509545.0, 509615.0, 509685.0],\n      dtype='float64', name='x', length=1568))yPandasIndexPandasIndex(Index([4499965.0, 4499895.0, 4499825.0, 4499755.0, 4499685.0, 4499615.0,\n       4499545.0, 4499475.0, 4499405.0, 4499335.0,\n       ...\n       4390905.0, 4390835.0, 4390765.0, 4390695.0, 4390625.0, 4390555.0,\n       4390485.0, 4390415.0, 4390345.0, 4390275.0],\n      dtype='float64', name='y', length=1568))Attributes: (4)AREA_OR_POINT :Area_FillValue :nanscale_factor :1.0add_offset :0.0\n\n\nAs mentioned, the ECOSTRESS product we are using here is tiled and the CRS is dependent on UTM zone. For this tile, we can look at the spatial_ref variable through the interactive object above to see details such as the well-known-text (WKT) representation of the CRS and other attributes. We are using hvplot for visualization here. For detailed information on available open-source Python tools and libraries for data visualization see https://pyviz.org/.\nNow let’s plot the data using hvplot. reproject function is applied only for the visualization. Make sure to specify the CRS argument within the hvplot.image function so the ESRI imagery RBG background tile aligns with our scene.\n\nsize_opts = dict(frame_height=405, frame_width=720, fontscale=2)\n\neco_lst_ds.rio.reproject('EPSG:4326').hvplot.image(x='x', y='y', **size_opts, \n                                                   cmap='inferno', tiles='ESRI', xlabel='Longitude', \n                                                   ylabel='Latitude', title='ECOSTRESS LST (K)', \n                                                   crs='EPSG:4326')*polygon_4326.hvplot(color='Green',alpha=0.5, \n                                                                                   crs='EPSG:4326', rasterize=True)"
  },
  {
    "objectID": "python/tutorials/Work_with_ECOSTRESS_L2T_LSTE_data.html#cropping-ecostress-data",
    "href": "python/tutorials/Work_with_ECOSTRESS_L2T_LSTE_data.html#cropping-ecostress-data",
    "title": "Working with ECOSTRESS L2T LSTE Data",
    "section": "4. Cropping ECOSTRESS Data",
    "text": "4. Cropping ECOSTRESS Data\nclip function from rasterio is used to mask data outside of our region of interest. Before clipping, we need to reproject our ROI to the projection of our dataset which is UTM zone 13N.\n\npolygon\n\n\n\n\n\n\n\n\nOBJECTID\nTYPE\nShapeSTAre\nShapeSTLen\ngeometry\n\n\n\n\n0\n38\nCity\n7.137130e+07\n45959.337125\nPOLYGON Z ((-11711572.939 4876932.599 0, -1171...\n\n\n1\n39\nCity\n4.082301e+07\n64848.593491\nPOLYGON Z ((-11709576.389 4876986.224 0, -1170...\n\n\n2\n40\nCity\n8.103168e+04\n1661.732555\nPOLYGON Z ((-11712065.95 4868450.24 0, -117120...\n\n\n3\n41\nCity\n2.092505e+07\n34856.338250\nPOLYGON Z ((-11717878.355 4877603.689 0, -1171...\n\n\n4\n56\nCity\n6.462819e+08\n309288.958272\nPOLYGON Z ((-11717379.163 4873347.362 0, -1171...\n\n\n\n\n\n\n\n\neco_lst_ds.rio.crs\n\nCRS.from_wkt('PROJCS[\"WGS 84 / UTM zone 13N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]')\n\n\n\npolygon_reproj = polygon.to_crs(eco_lst_ds.rio.crs)\n\n\npolygon_reproj.crs\n\n&lt;Projected CRS: EPSG:32613&gt;\nName: WGS 84 / UTM zone 13N\nAxis Info [cartesian]:\n- [east]: Easting (metre)\n- [north]: Northing (metre)\nArea of Use:\n- undefined\nCoordinate Operation:\n- name: UTM zone 13N\n- method: Transverse Mercator\nDatum: World Geodetic System 1984\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\n\neco_lst_roi = eco_lst_ds.rio.clip(polygon_reproj.geometry.values, polygon_reproj.crs, all_touched=True)\n\n\neco_lst_roi.hvplot.image(\n    geo=True,cmap='inferno',**size_opts, tiles='ESRI',alpha=0.8, \n    title='Cropped ECOSTRESS LST (K)', xlabel='Longitude',ylabel='Latitude', \n    crs='EPSG:32613', rasterize=True)\n\n\n\n\nNext, we will repeate the same process for the associated quality layer.\n\neco_qc_ds = rxr.open_rasterio(qc_links[14]).squeeze('band', drop=True)\n\neco_qc_roi = eco_qc_ds.rio.clip(polygon_reproj.geometry.values, polygon_reproj.crs, all_touched=True) # assign a different value to fill value \n\neco_qc_roi.hvplot.image(\n    geo=True, cmap='inferno',**size_opts, tiles='ESRI',alpha=0.8, \n    title='Cropped ECOSTRESS LST (K)', xlabel='Longitude',ylabel='Latitude', \n    crs='EPSG:32613', rasterize=True)"
  },
  {
    "objectID": "python/tutorials/Work_with_ECOSTRESS_L2T_LSTE_data.html#quality-filtering",
    "href": "python/tutorials/Work_with_ECOSTRESS_L2T_LSTE_data.html#quality-filtering",
    "title": "Working with ECOSTRESS L2T LSTE Data",
    "section": "5. Quality Filtering",
    "text": "5. Quality Filtering\nThe quality values are 16 digits bit values with bits 0 and 1 being the mandatory QA flag that will be interpreted as:\n    00 = Pixel produced, best quality\n    01 = Pixel produced, nominal quality. Either one or more of the following conditions are met:  \n\n            1. emissivity in both bands 4 and 5 &lt; 0.95, i.e. possible cloud contamination  \n            2. low transmissivity due to high water vapor loading (&lt;0.4), check PWV values and error estimates  \n            3. Pixel falls on missing scan line in bands 1&5, and filled using spatial neural net. Check error estimates  \n            Recommend more detailed analysis of other QC information  \n    10 = Pixel produced, but cloud detected  \n    11 = Pixel not produced due to missing/bad data, user should check Data quality flag bits  \nThe detailed quality information is provided in Table 3-5 in ECOSTRESS Product Specification Document.\nBelow, the unique quality values are extracted from the clipped data, and only the values showing good quality are kept.\n\nquality_vals = np.unique(eco_qc_roi.values).tolist()\ngood_q = [q for q in quality_vals if np.binary_repr(q, width=16)[-2:] == '00']\ngood_q\n\n[0,\n 16832,\n 17088,\n 17856,\n 18112,\n 18880,\n 19136,\n 20160,\n 33216,\n 33472,\n 34240,\n 34496,\n 35264,\n 35520,\n 36544]\n\n\n.where method is used to filter the quality and keep only the LSTE values generated with the best quality.\n\neco_lst_roi_mask = eco_lst_roi.where(eco_qc_roi.isin(good_q))\neco_lst_roi_mask\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (y: 219, x: 152)&gt; Size: 133kB\narray([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       ...,\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]],\n      shape=(219, 152), dtype=float32)\nCoordinates:\n  * x            (x) float64 1kB 4.743e+05 4.743e+05 ... 4.848e+05 4.848e+05\n  * y            (y) float64 2kB 4.438e+06 4.438e+06 ... 4.423e+06 4.423e+06\n    spatial_ref  int64 8B 0\nAttributes:\n    AREA_OR_POINT:  Area\n    scale_factor:   1.0\n    add_offset:     0.0\n    _FillValue:     nanxarray.DataArrayy: 219x: 152nan nan nan nan nan nan nan nan ... nan nan nan nan nan nan nan nanarray([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       ...,\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]],\n      shape=(219, 152), dtype=float32)Coordinates: (3)x(x)float644.743e+05 4.743e+05 ... 4.848e+05axis :Xlong_name :x coordinate of projectionstandard_name :projection_x_coordinateunits :metrearray([474265., 474335., 474405., 474475., 474545., 474615., 474685., 474755.,\n       474825., 474895., 474965., 475035., 475105., 475175., 475245., 475315.,\n       475385., 475455., 475525., 475595., 475665., 475735., 475805., 475875.,\n       475945., 476015., 476085., 476155., 476225., 476295., 476365., 476435.,\n       476505., 476575., 476645., 476715., 476785., 476855., 476925., 476995.,\n       477065., 477135., 477205., 477275., 477345., 477415., 477485., 477555.,\n       477625., 477695., 477765., 477835., 477905., 477975., 478045., 478115.,\n       478185., 478255., 478325., 478395., 478465., 478535., 478605., 478675.,\n       478745., 478815., 478885., 478955., 479025., 479095., 479165., 479235.,\n       479305., 479375., 479445., 479515., 479585., 479655., 479725., 479795.,\n       479865., 479935., 480005., 480075., 480145., 480215., 480285., 480355.,\n       480425., 480495., 480565., 480635., 480705., 480775., 480845., 480915.,\n       480985., 481055., 481125., 481195., 481265., 481335., 481405., 481475.,\n       481545., 481615., 481685., 481755., 481825., 481895., 481965., 482035.,\n       482105., 482175., 482245., 482315., 482385., 482455., 482525., 482595.,\n       482665., 482735., 482805., 482875., 482945., 483015., 483085., 483155.,\n       483225., 483295., 483365., 483435., 483505., 483575., 483645., 483715.,\n       483785., 483855., 483925., 483995., 484065., 484135., 484205., 484275.,\n       484345., 484415., 484485., 484555., 484625., 484695., 484765., 484835.])y(y)float644.438e+06 4.438e+06 ... 4.423e+06axis :Ylong_name :y coordinate of projectionstandard_name :projection_y_coordinateunits :metrearray([4438295., 4438225., 4438155., ..., 4423175., 4423105., 4423035.],\n      shape=(219,))spatial_ref()int640crs_wkt :PROJCS[\"WGS 84 / UTM zone 13N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984projected_crs_name :WGS 84 / UTM zone 13Ngrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-105.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"WGS 84 / UTM zone 13N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :474230.0 70.0 0.0 4438330.0 0.0 -70.0array(0)Indexes: (2)xPandasIndexPandasIndex(Index([474265.0, 474335.0, 474405.0, 474475.0, 474545.0, 474615.0, 474685.0,\n       474755.0, 474825.0, 474895.0,\n       ...\n       484205.0, 484275.0, 484345.0, 484415.0, 484485.0, 484555.0, 484625.0,\n       484695.0, 484765.0, 484835.0],\n      dtype='float64', name='x', length=152))yPandasIndexPandasIndex(Index([4438295.0, 4438225.0, 4438155.0, 4438085.0, 4438015.0, 4437945.0,\n       4437875.0, 4437805.0, 4437735.0, 4437665.0,\n       ...\n       4423665.0, 4423595.0, 4423525.0, 4423455.0, 4423385.0, 4423315.0,\n       4423245.0, 4423175.0, 4423105.0, 4423035.0],\n      dtype='float64', name='y', length=219))Attributes: (4)AREA_OR_POINT :Areascale_factor :1.0add_offset :0.0_FillValue :nan\n\n\n\neco_lst_roi_mask.hvplot.image(\n    geo=True,cmap='inferno',**size_opts, tiles='ESRI',alpha=0.9, \n    title='Quality Masked ECOSTRESS LST (K)', xlabel='Longitude',ylabel='Latitude', \n    crs='EPSG:32613', rasterize=True)"
  },
  {
    "objectID": "python/tutorials/Work_with_ECOSTRESS_L2T_LSTE_data.html#writing-outputs",
    "href": "python/tutorials/Work_with_ECOSTRESS_L2T_LSTE_data.html#writing-outputs",
    "title": "Working with ECOSTRESS L2T LSTE Data",
    "section": "6. Writing Outputs",
    "text": "6. Writing Outputs\nWe now have a ECOSTRESS scene that is clipped to our ROI with only good quality values. Finally, we can save this file locally.\n\nout_name = f\"../../data/{lst_links[14].split('/')[-1].split('.tif')[0]}_clipped.tif\"\n\neco_lst_roi_mask.rio.to_raster(raster_path=out_name, driver='COG')"
  },
  {
    "objectID": "python/tutorials/Work_with_ECOSTRESS_L2T_LSTE_data.html#contact-info",
    "href": "python/tutorials/Work_with_ECOSTRESS_L2T_LSTE_data.html#contact-info",
    "title": "Working with ECOSTRESS L2T LSTE Data",
    "section": "Contact Info:",
    "text": "Contact Info:\nEmail: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)¹\nWebsite: https://www.earthdata.nasa.gov/centers/lp-daac\n¹Work performed under USGS contract G15PD00467 for NASA contract NNG14HH33I."
  },
  {
    "objectID": "python/how-tos/how_to_direct_access_http_ecostress_cog.html",
    "href": "python/how-tos/how_to_direct_access_http_ecostress_cog.html",
    "title": "How to: Directly Access ECOSTRESS Data (HTTP)",
    "section": "",
    "text": "Summary\nIn this notebook, we will access data for the ECOSTRESS Tiled Land Surface Temperature and Emissivity Instantaneous L2 Global 70 m V002 data product. These data are archived and distributed as Cloud Optimized GeoTIFF (COG) files, one file for each spectral band. We will access a single COG file, Land Surface Temperature (LST), directly loading it into memory, leveraging the cloud-optimized format, rather than downloading the file. To accomplish this we will create a requests_https_session using the earthaccess Python library, which will handle passing our Earthdata Login credentials to the NASA Earthdata system, and the rasterio and rioxarray Python libraries to load the data into memory so we can easily work with it.\nBackground\nThe ECOSTRESS mission is answering these questions by accurately measuring the temperature of plants. Plants regulate their temperature by releasing water through tiny pores on their leaves called stomata. If they have sufficient water they can maintain their temperature, but if there is insufficient water, their temperatures rise and this temperature rise can be measured with ECOSTRESS. The images acquired by ECOSTRESS are the most detailed temperature images of the surface ever acquired from space and can be used to measure the temperature of an individual farm field. These temperature images, along with auxiliary inputs, are used to produce one of the primary science outputs of ECOSTRESS: evapotranspiration, an indicator of plant health vby measuring the evaporation and transpiration of water through a plant.\nLearning Objectives\nRequirements\nOutline 1. Setup 2. Load file directly to memory 3. Visualize the data",
    "crumbs": [
      "Python Notebooks",
      "How to Directly Access ECOSTRESS Data (HTTP)"
    ]
  },
  {
    "objectID": "python/how-tos/how_to_direct_access_http_ecostress_cog.html#setup",
    "href": "python/how-tos/how_to_direct_access_http_ecostress_cog.html#setup",
    "title": "How to: Directly Access ECOSTRESS Data (HTTP)",
    "section": "1. Setup",
    "text": "1. Setup\nImport the required libraries.\n\nimport os\nimport rasterio as rio\nimport rioxarray as rxr\nimport hvplot.xarray\nimport earthaccess\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\nAuthentication\nLog into Earthdata using the Auth and login functions from the earthaccess library. The persist=True argument will create a local .netrc file if it doesn’t exist, or add your login info to an existing .netrc file. If no Earthdata Login credentials are found in the .netrc you’ll be prompted for them.\n\nauth = earthaccess.login(persist = True)\n\n\n\nContext Manager\nFor this exercise, we are going to open up a context manager for the notebook using the rasterio.env module to store the required GDAL configurations we need to access the data from Earthdata Cloud. The context manager sends the authentication information when connecting to a file and can also customize how the file is handled locally. Geospatial data access Python packages like rasterio and rioxarray depend on GDAL, leveraging GDAL’s “Virtual File Systems” to read remote files. GDAL has a lot of environment variables that control its behavior. Changing these settings can mean the difference between being able to access a file or not. They can also have an impact on the performance. Please see the GDAL config options documentation for more details and all available options.\nWhile the context manager is open (rio_env.__enter__()) we will be able to run the open or get data commands that would typically be executed within a “with” statement. Entering the context manager for the entirety of the notebook allows us to more freely interact with the data. We’ll close the context manager (rio_env.__exit__()) at the end of the notebook.\n\n# Set up and enter context manager\nrio_env = rio.Env(GDAL_DISABLE_READDIR_ON_OPEN='TRUE',\n                  GDAL_HTTP_COOKIEFILE=os.path.expanduser('~/cookies.txt'),\n                  GDAL_HTTP_COOKIEJAR=os.path.expanduser('~/cookies.txt'),\n                  GDAL_HTTP_MAX_RETRY=10,\n                  GDAL_HTTP_RETRY_DELAY=0.5)\nrio_env.__enter__()\n\n&lt;rasterio.env.Env at 0x29b47308590&gt;\n\n\nAbove, GDAL_HTTP_COOKIEFILE and GDAL_HTTP_COOKIEJAR tell GDAL to use a cookie for authentication and where to find that cookie. GDAL_DISABLE_READDIR_ON_OPEN tells gdal not to look for any auxiliary or sidecar files in the directory, which can slow down access. GDAL_HTTP_MAX_RETRY and GDAL_HTTP_RETRY_DELAY tell GDAL to retry the connection a number of times and how long to wait before retrying. These are nice options to add in the case that a connection fails temporarily, and will allow the workflow to continue without re-running.\nIn this example, we use cookies to pass authentication information via the context manager; however, this can also be accomplished by sending an Earthdata Login token, when working with versions of gdal &gt; 3.7.0. See below for a commented out example.\n\n# rio_env = rio.Env(GDAL_DISABLE_READDIR_ON_OPEN='TRUE',\n#                   GDAL_HTTP_AUTH='BEARER',\n#                   GDAL_HTTP_BEARER=auth.token['access_token'],\n#                   GDAL_HTTP_MAX_RETRY=10,\n#                   GDAL_HTTP_RETRY_DELAY=0.5)\n# rio_env.__enter__()",
    "crumbs": [
      "Python Notebooks",
      "How to Directly Access ECOSTRESS Data (HTTP)"
    ]
  },
  {
    "objectID": "python/how-tos/how_to_direct_access_http_ecostress_cog.html#load-the-file-directly-into-memory",
    "href": "python/how-tos/how_to_direct_access_http_ecostress_cog.html#load-the-file-directly-into-memory",
    "title": "How to: Directly Access ECOSTRESS Data (HTTP)",
    "section": "2. Load the File Directly into Memory",
    "text": "2. Load the File Directly into Memory\nIn this example we’re interested in the ECOSTRESS data collection from NASA’s LP DAAC in Earthdata Cloud. Below we specify the URL to the data asset in Earthdata Cloud. This URL can be found via Earthdata Search or programmatically through earthaccess, the CMR API or CMR-STAC API. There are programmatic examples in the Python tutorials for ECOSTRESS, and an earthdata search example available as well.\n\nhttps_url = 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_LST.tif'\n\nRead in the ECOSTRESS LST URL into our workspace using rioxarray. This utilizes the context manager that we have entered. Optionally we can use the mask_and_scale argument to mask and apply the scale and offset values for the data.\n\n# Open data with rioxarray\nda = rxr.open_rasterio(https_url, mask_and_scale=True)\nda\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (band: 1, y: 1568, x: 1568)&gt; Size: 10MB\n[2458624 values with dtype=float32]\nCoordinates:\n  * band         (band) int64 8B 1\n  * x            (x) float64 13kB 2e+05 2.001e+05 ... 3.096e+05 3.097e+05\n  * y            (y) float64 13kB 3.9e+06 3.9e+06 3.9e+06 ... 3.79e+06 3.79e+06\n    spatial_ref  int64 8B 0\nAttributes:\n    AREA_OR_POINT:  Areaxarray.DataArrayband: 1y: 1568x: 1568...[2458624 values with dtype=float32]Coordinates: (4)band(band)int641array([1])x(x)float642e+05 2.001e+05 ... 3.097e+05array([200015., 200085., 200155., ..., 309565., 309635., 309705.],\n      shape=(1568,))y(y)float643.9e+06 3.9e+06 ... 3.79e+06array([3899965., 3899895., 3899825., ..., 3790415., 3790345., 3790275.],\n      shape=(1568,))spatial_ref()int640crs_wkt :PROJCS[\"WGS 84 / UTM zone 11N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984projected_crs_name :WGS 84 / UTM zone 11Ngrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-117.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"WGS 84 / UTM zone 11N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :199980.0 70.0 0.0 3900000.0 0.0 -70.0array(0)Indexes: (3)bandPandasIndexPandasIndex(Index([1], dtype='int64', name='band'))xPandasIndexPandasIndex(Index([200015.0, 200085.0, 200155.0, 200225.0, 200295.0, 200365.0, 200435.0,\n       200505.0, 200575.0, 200645.0,\n       ...\n       309075.0, 309145.0, 309215.0, 309285.0, 309355.0, 309425.0, 309495.0,\n       309565.0, 309635.0, 309705.0],\n      dtype='float64', name='x', length=1568))yPandasIndexPandasIndex(Index([3899965.0, 3899895.0, 3899825.0, 3899755.0, 3899685.0, 3899615.0,\n       3899545.0, 3899475.0, 3899405.0, 3899335.0,\n       ...\n       3790905.0, 3790835.0, 3790765.0, 3790695.0, 3790625.0, 3790555.0,\n       3790485.0, 3790415.0, 3790345.0, 3790275.0],\n      dtype='float64', name='y', length=1568))Attributes: (1)AREA_OR_POINT :Area\n\n\nThe file is read into Python as an xarray dataarray with a band, x, and y dimension. In this example the band dimension is meaningless, so we’ll use the squeeze() function to remove band as a dimension.\n\nda_lst = da.squeeze('band', drop=True)\nda_lst\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (y: 1568, x: 1568)&gt; Size: 10MB\n[2458624 values with dtype=float32]\nCoordinates:\n  * x            (x) float64 13kB 2e+05 2.001e+05 ... 3.096e+05 3.097e+05\n  * y            (y) float64 13kB 3.9e+06 3.9e+06 3.9e+06 ... 3.79e+06 3.79e+06\n    spatial_ref  int64 8B 0\nAttributes:\n    AREA_OR_POINT:  Areaxarray.DataArrayy: 1568x: 1568...[2458624 values with dtype=float32]Coordinates: (3)x(x)float642e+05 2.001e+05 ... 3.097e+05array([200015., 200085., 200155., ..., 309565., 309635., 309705.],\n      shape=(1568,))y(y)float643.9e+06 3.9e+06 ... 3.79e+06array([3899965., 3899895., 3899825., ..., 3790415., 3790345., 3790275.],\n      shape=(1568,))spatial_ref()int640crs_wkt :PROJCS[\"WGS 84 / UTM zone 11N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984projected_crs_name :WGS 84 / UTM zone 11Ngrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-117.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"WGS 84 / UTM zone 11N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :199980.0 70.0 0.0 3900000.0 0.0 -70.0array(0)Indexes: (2)xPandasIndexPandasIndex(Index([200015.0, 200085.0, 200155.0, 200225.0, 200295.0, 200365.0, 200435.0,\n       200505.0, 200575.0, 200645.0,\n       ...\n       309075.0, 309145.0, 309215.0, 309285.0, 309355.0, 309425.0, 309495.0,\n       309565.0, 309635.0, 309705.0],\n      dtype='float64', name='x', length=1568))yPandasIndexPandasIndex(Index([3899965.0, 3899895.0, 3899825.0, 3899755.0, 3899685.0, 3899615.0,\n       3899545.0, 3899475.0, 3899405.0, 3899335.0,\n       ...\n       3790905.0, 3790835.0, 3790765.0, 3790695.0, 3790625.0, 3790555.0,\n       3790485.0, 3790415.0, 3790345.0, 3790275.0],\n      dtype='float64', name='y', length=1568))Attributes: (1)AREA_OR_POINT :Area",
    "crumbs": [
      "Python Notebooks",
      "How to Directly Access ECOSTRESS Data (HTTP)"
    ]
  },
  {
    "objectID": "python/how-tos/how_to_direct_access_http_ecostress_cog.html#visualize-the-data",
    "href": "python/how-tos/how_to_direct_access_http_ecostress_cog.html#visualize-the-data",
    "title": "How to: Directly Access ECOSTRESS Data (HTTP)",
    "section": "3. Visualize the Data",
    "text": "3. Visualize the Data\nPlot the dataarray, representing the ECOSTRESS band, using hvplot. Since ECOSTRESS tiles are in UTM projections, to visualize this with a basemap tile, we’ll need to reproject to EPSG:4326 for the visual. This can be accomplished using the rio.reproject() function.\n\nda_lst_reproj = da_lst.rio.reproject(\"EPSG:4326\")\n\n\nda_lst_reproj.hvplot.image(x = 'x',\n                           y = 'y',\n                           crs = 'EPSG:4326',\n                           cmap='jet',\n                           tiles='EsriImagery',\n                           title = f'{https_url.split(\"/\")[-1]}',\n                           frame_width=500)\n\n\n\n\n\n  \n\n\n\n\nExit the context manager.\n\nrio_env.__exit__()",
    "crumbs": [
      "Python Notebooks",
      "How to Directly Access ECOSTRESS Data (HTTP)"
    ]
  },
  {
    "objectID": "python/how-tos/how_to_direct_access_http_ecostress_cog.html#contact-info",
    "href": "python/how-tos/how_to_direct_access_http_ecostress_cog.html#contact-info",
    "title": "How to: Directly Access ECOSTRESS Data (HTTP)",
    "section": "Contact Info:",
    "text": "Contact Info:\nEmail: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)¹\nWebsite: https://www.earthdata.nasa.gov/centers/lp-daac\n¹Work performed under USGS contract G15PD00467 for NASA contract NNG14HH33I.",
    "crumbs": [
      "Python Notebooks",
      "How to Directly Access ECOSTRESS Data (HTTP)"
    ]
  },
  {
    "objectID": "python/scripts/ecostress_swath2grid/swath2grid.html",
    "href": "python/scripts/ecostress_swath2grid/swath2grid.html",
    "title": "ECOSTRESS Swath to Grid Conversion Script",
    "section": "",
    "text": "The ECOSTRESS_swath2grid.py script converts ECOSTRESS swath data products, stored in Hierarchical Data Format version 5 (HDF5, .h5) into projected GeoTIFFs. When executing this script, a user will submit a desired output projection and input directory containing ECOSTRESS swath data products as command line arguments. The script begins by opening any of the ECOSTRESS products listed below that are contained in the input directory. Next, it uses the latitude and longitude arrays from the ECO1BGEO product (except for L3/L4 ALEXI_USDA and ECO1BMAPRAD products) to resample the swath dataset to a grid using nearest neighbor resampling (Pyresample/kdtree). Note that you will need to download the ECO1BGEO files that correspond to your higher level product files. From there, the script defines the coordinate reference system (CRS) input by the user (options include UTM Zones and Geographic (EPSG:4326)). There is an optional argument to override the default UTM zone selected by the script (see below) if needed. Ultimately, the script exports the gridded array as a GeoTIFF (GDAL). By default, the script will loop through and perform the aforementioned steps for each science dataset (SDS) in the HDF5 file. There is an optional argument that allows you to select a subset of SDS layers within a given product (see details below). There is another optional argument that allows users to convert L1B radiance to brightness temperature. The resulting GeoTIFF files can be imported with spatial reference into GIS and Remote Sensing software programs. The script also will batch process all ECOSTRESS swath files contained in the input directory provided. For ECOSTRESS products that include a scale factor in the metadata, the output will be scaled, and for products that include a fill value in the file metadata, this will be carried over into the GeoTIFF outputs. For layers that do not contain a fill value in the file metadata, the fill value will be defined as the highest possible value for the given datatype of an SDS.\n\n\n\n\n\n\nECO1BGEO\n\nECO1BMAPRAD (lat/lon arrays contained within, ECO1BGEO not needed; radiance can be converted to brightness temperature)\n\nECO1BRAD (radiance can be converted to brightness temperature)\nECO2CLD\n\nECO2LSTE\n\nECO3ETALEXIU (30 m, in UTM Projection, ECO1BGEO not needed)\n\nECO3ETPTJPL\n\nECO3ANCQA\n\nECO4ESIPTJPL\n\nECO4ESIALEXIU (30 m, in UTM Projection, ECO1BGEO not needed)\n\nECO4WUE\n\n\n\n\n\nECO_L1B_GEO\n\nECO_L1B_ATT\n\nECO_L1B_RAD\n\nECO_L2_LSTE\n\nECO_L2_CLOUD\n\n\nNote that you will need to separately download the ECO1BGEO files that correspond to the files you have downloaded for products 2-4, 6-8, and 10 above.\n\n\n\n\n\n\nDisclaimer: Script has been tested on Windows and MacOS using the specifications identified below.\n\n\nPython version 3.8\n\nGDAL\n\nh5py\n\npyproj\n\npyresample\n\nnumpy\n\npandas\n\nscipy\n\nA NASA Earthdata Login account is required to download the data used in this tutorial. You can create an account at the link provided.\n\n\n\n\n\n\n\nDownload ECOSTRESS higher level products and corresponding ECO1BGEO files (ordered separately) from the LP DAAC Data Pool or Earthdata Search Client to a local directory (see above for applicable products)\n\nCopy/clone/download ECOSTRESS_swath2grid.py from LP DAAC Data User Resources Repository\n\n\n\n\n\nIt is recommended to use Conda, an environment manager to set up a compatible Python environment. Download Conda for your OS here: https://www.anaconda.com/download/. Once you have Conda installed, Follow the instructions below to successfully setup a Python environment on MacOS or Windows.\n\nUsing your preferred command line interface (command prompt, terminal, cmder, etc.) type the following to successfully create a compatible python environment:\n&gt; conda create -n ecostress -c conda-forge --yes python=3.8 h5py pyproj pyresample gdal pandas scipy  \n&gt; conda activate ecostress  \n\n\nNOTE: If you are having trouble activating your environment, or loading specific packages once you have activated your environment? Try entering conda update conda or conda update --all\n\nAdditional information on setting up and managing Conda environments.\nIf you prefer to not install Conda, the same setup and dependencies can be achieved by using another package manager such as pip.\nStill having trouble getting a compatible Python environment set up? Contact LP DAAC User Services.\n\n\n\n\nOnce you have set up your MacOS/Windows environment and it has been activated, run the script with the following in your Command Prompt/terminal window:\n&gt; python ECOSTRESS_swath2grid.py --proj &lt;insert reprojection desired, Options: GEO and UTM&gt; --dir &lt;insert input directory with ECOSTRESS files here&gt;\nWhere:\n\nGEO = Geographic lat/lon, EPSG code 4326\n\nUTM = Universal Transverse Mercator Zones (north/south) with WGS84 datum\n\n\nNOTE: --proj argument is case sensitive\n\nExample\n&gt; python ECOSTRESS_swath2grid.py --proj GEO --dir C:\\Users\\ECOSTRESS\\\nIf UTM is selected, the script will calculate the UTM zone by using the location of the center of each ECOSTRESS granule. If you prefer to set the UTM zone manually, you can do so by adding the optional argument --utmzone &lt;insert EPSG code for desired zone&gt;. This optional argument will override the default functionality for users who desire all ECOSTRESS granules to be in a common UTM projection, regardless of the center location of the granule.\nExample\n&gt; python ECOSTRESS_swath2grid.py --proj UTM --dir &lt;insert input directory with ECOSTRESS files here&gt; --utmzone &lt;insert EPSG code for desired UTM zone, e.g. 32610&gt;  \n\nNOTE: You can look up EPSG codes for UTM zones at: http://spatialreference.org/, note that only WGS84 datum is supported, and thus EPSG codes for UTM north zones will begin with 326 and utm south zones with 327\n\nThe default functionality is to export each science dataset (SDS) layer contained in an ECOSTRESS product as a GeoTIFF. If you prefer to only export one or more layers, you can do so by adding the optional argument --sds &lt;insert SDS layer names desired&gt; (comma separated with no spaces, see below for specific SDS layer names by product–note that the input is case sensitive)\nExample\n&gt; python ECOSTRESS_swath2grid.py --proj GEO --dir C:\\Users\\ECOSTRESS\\ --sds LST,QC,Emis1  \n\nNOTE: SDS layer names for each product are below in section Subsetting Layers.\n\nThe default functionality for L1B radiance is to export each layer contained as radiance (W/m^2/sr/um). If you prefer to export the L1B datasets as brightness temperature (K), you can do so by adding the optional argument --bt (see section Radiance to Brightness Temperature Conversion for additional information)\nExample\n&gt; python ECOSTRESS_swath2grid.py --proj GEO --dir C:\\Users\\ECOSTRESS\\ --bt`  \n\n\n\nTo use the --bt optional command line argument to convert L1B radiance science datasets (bands 1-5) from radiance (W/m^2/sr/um) to brightness temperature (K), you will need to add the --bt argument to your script execution command. Example:\n&gt; python ECOSTRESS_swath2grid.py --proj GEO --dir C:\\Users\\ECOSTRESS\\ --bt  \n\nNote: You will first need to download the EcostressBrightnessTemperatureV01.h5 Lookup Table (LUT) and save it to the directory containing your ECOSTRESS_swath2grid.py script, or the directory containing the ECOSTRESS L1B files to be processed.\n\n\n\n\nTo use the --sds optional command line argument in order to select a subset of science datasets from an ECOSTRESS granule, you will need to submit 1 or more SDS layers names into the --sds argument exactly as they appear in the list below. The SDS layers must be comma separated, with no spaces between SDS!\n\nExample for a single layer: --sds LST\n\nExample for multiple layers: --sds ETcanopy,ETdaily,ETinst\n\n\n\n\n\n\n\nECO1BGEO\n\nheight\n\nland_fraction\n\nlatitude\n\nlongitude\n\nsolar_azimuth\n\nsolar_zenith\n\nview_azimuth\n\nview_zenith\n\nECO1BMAPRAD\n\ndata_quality_1\ndata_quality_2\n\ndata_quality_3\n\ndata_quality_4\n\ndata_quality_5\n\nheight\n\nlatitude\n\nlongitude\n\nradiance_1 (can be exported to brightnesstemperature_1)\n\nradiance_2 (can be exported to brightnesstemperature_2)\n\nradiance_3 (can be exported to brightnesstemperature_3)\n\nradiance_4 (can be exported to brightnesstemperature_4)\n\nradiance_5 (can be exported to brightnesstemperature_5)\n\nsolar_azimuth\n\nsolar_zenith\n\nswir_dn\n\nview_azimuth\n\nview_zenith\n\nECO1BRAD\n\ndata_quality_1\n\ndata_quality_2\n\ndata_quality_3\n\ndata_quality_4\n\ndata_quality_5\n\nradiance_1 (can be exported to brightnesstemperature_1)\n\nradiance_2 (can be exported to brightnesstemperature_2)\n\nradiance_3 (can be exported to brightnesstemperature_3)\n\nradiance_4 (can be exported to brightnesstemperature_4)\n\nradiance_5 (can be exported to brightnesstemperature_5)\n\nswir_dn\n\nECO2CLD\n\nCloudMask\n\nECO2LSTE\n\nEmis1\n\nEmis1_err\n\nEmis2\n\nEmis2_err\n\nEmis3\n\nEmis3_err\n\nEmis4\n\nEmis4_err\n\nEmis5\n\nEmis5_err\n\nEmisWB\n\nLST\n\nLST_err\n\nPWV\n\nQC\n\nECO3ETALEXIU\n\nETdaily\n\nETdailyUncertainty\n\nQualityFlag\n\nECO3ETPTJPL\n\nETcanopy\n\nETdaily\n\nETinst\n\nETinstUncertainty\n\nETinterception\n\nETsoil\n\nECO3ANCQA\n\nECOSTRESS_L2_QC\n\nLandsat8_QC\n\nMCD12Q1_QC\n\nMCD43A3_QC\n\nMOD04_QC\n\nMOD06_1km_QC\n\nMOD06_5km_QC\n\nMOD07_QC\n\nMOD13Q1_QC\n\nMOD17A2H_QC\n\nMOD44W_QC\n\nECO4ESIALEXIU\n\nESIdaily\n\nESIdailyUncertainty\n\nQualityFlag\n\nECO4ESIPTJPL\n\nESIavg\n\nPET\n\nECO4WUE\n\nWUEavg\n\n\n\n\n\n\nECO_L1B_GEO\n\nheight\n\nland_fraction\n\nlatitude\n\nline_start_time_j2000\n\nlongitude\n\nsolar_azimuth\n\nsolar_zenith\n\nview_azimuth\n\nview_zenith\n\nECO_L1B_ATT\n\nAttitude quaternion\n\nAttitude time_j2000\n\nEphemeris eci_position\n\nEphemeris eci_velocity\n\nEphemeris time_j2000\n\nUncorrected Attitude quaternion\n\nUncorrected Attitude time_j2000\n\nUncorrected Ephemeris eci_position\n\nUncorrected Ephemeris eci_velocity\n\nUncorrected Ephemeris time_j2000\n\nECO_L1B_RAD\n\nradiance_1\n\nradiance_2\n\nradiance_3\n\nradiance_4\n\nradiance_5\n\ndata_quality_1\n\ndata_quality_2\n\ndata_quality_3\n\ndata_quality_4\n\ndata_quality_5\n\nswir_dn\n\nEncoderValue\n\nline_start_time_j2000\n\nECO_L2_LSTE\n\nLST\n\nQC\n\nEmis1\n\nEmis2\n\nEmis3\n\nEmis4\n\nEmis5\n\nLST_err\n\nEmis1_err\n\nEmis2_err\n\nEmis3_err\n\nEmis4_err\n\nEmis5_err\n\nEmisWB\n\nPWV\n\ncloud_mask\n\nwater_mask\n\nECO_L2_CLOUD\n\nCloud_confidence\n\nCloud_final\n\n\n\n\n\n\n\n\nContact: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)\nWebsite: https://lpdaac.usgs.gov/\nDate last modified: 11-21-2022\n1LP DAAC Work performed under NASA contract NNG14HH33I.",
    "crumbs": [
      "Scripts",
      "ECOSTRESS Swath to Grid Conversion Script"
    ]
  },
  {
    "objectID": "python/scripts/ecostress_swath2grid/swath2grid.html#objective",
    "href": "python/scripts/ecostress_swath2grid/swath2grid.html#objective",
    "title": "ECOSTRESS Swath to Grid Conversion Script",
    "section": "",
    "text": "The ECOSTRESS_swath2grid.py script converts ECOSTRESS swath data products, stored in Hierarchical Data Format version 5 (HDF5, .h5) into projected GeoTIFFs. When executing this script, a user will submit a desired output projection and input directory containing ECOSTRESS swath data products as command line arguments. The script begins by opening any of the ECOSTRESS products listed below that are contained in the input directory. Next, it uses the latitude and longitude arrays from the ECO1BGEO product (except for L3/L4 ALEXI_USDA and ECO1BMAPRAD products) to resample the swath dataset to a grid using nearest neighbor resampling (Pyresample/kdtree). Note that you will need to download the ECO1BGEO files that correspond to your higher level product files. From there, the script defines the coordinate reference system (CRS) input by the user (options include UTM Zones and Geographic (EPSG:4326)). There is an optional argument to override the default UTM zone selected by the script (see below) if needed. Ultimately, the script exports the gridded array as a GeoTIFF (GDAL). By default, the script will loop through and perform the aforementioned steps for each science dataset (SDS) in the HDF5 file. There is an optional argument that allows you to select a subset of SDS layers within a given product (see details below). There is another optional argument that allows users to convert L1B radiance to brightness temperature. The resulting GeoTIFF files can be imported with spatial reference into GIS and Remote Sensing software programs. The script also will batch process all ECOSTRESS swath files contained in the input directory provided. For ECOSTRESS products that include a scale factor in the metadata, the output will be scaled, and for products that include a fill value in the file metadata, this will be carried over into the GeoTIFF outputs. For layers that do not contain a fill value in the file metadata, the fill value will be defined as the highest possible value for the given datatype of an SDS.",
    "crumbs": [
      "Scripts",
      "ECOSTRESS Swath to Grid Conversion Script"
    ]
  },
  {
    "objectID": "python/scripts/ecostress_swath2grid/swath2grid.html#available-products",
    "href": "python/scripts/ecostress_swath2grid/swath2grid.html#available-products",
    "title": "ECOSTRESS Swath to Grid Conversion Script",
    "section": "",
    "text": "ECO1BGEO\n\nECO1BMAPRAD (lat/lon arrays contained within, ECO1BGEO not needed; radiance can be converted to brightness temperature)\n\nECO1BRAD (radiance can be converted to brightness temperature)\nECO2CLD\n\nECO2LSTE\n\nECO3ETALEXIU (30 m, in UTM Projection, ECO1BGEO not needed)\n\nECO3ETPTJPL\n\nECO3ANCQA\n\nECO4ESIPTJPL\n\nECO4ESIALEXIU (30 m, in UTM Projection, ECO1BGEO not needed)\n\nECO4WUE\n\n\n\n\n\nECO_L1B_GEO\n\nECO_L1B_ATT\n\nECO_L1B_RAD\n\nECO_L2_LSTE\n\nECO_L2_CLOUD\n\n\nNote that you will need to separately download the ECO1BGEO files that correspond to the files you have downloaded for products 2-4, 6-8, and 10 above.",
    "crumbs": [
      "Scripts",
      "ECOSTRESS Swath to Grid Conversion Script"
    ]
  },
  {
    "objectID": "python/scripts/ecostress_swath2grid/swath2grid.html#prerequisites",
    "href": "python/scripts/ecostress_swath2grid/swath2grid.html#prerequisites",
    "title": "ECOSTRESS Swath to Grid Conversion Script",
    "section": "",
    "text": "Disclaimer: Script has been tested on Windows and MacOS using the specifications identified below.\n\n\nPython version 3.8\n\nGDAL\n\nh5py\n\npyproj\n\npyresample\n\nnumpy\n\npandas\n\nscipy\n\nA NASA Earthdata Login account is required to download the data used in this tutorial. You can create an account at the link provided.",
    "crumbs": [
      "Scripts",
      "ECOSTRESS Swath to Grid Conversion Script"
    ]
  },
  {
    "objectID": "python/scripts/ecostress_swath2grid/swath2grid.html#procedures",
    "href": "python/scripts/ecostress_swath2grid/swath2grid.html#procedures",
    "title": "ECOSTRESS Swath to Grid Conversion Script",
    "section": "",
    "text": "Download ECOSTRESS higher level products and corresponding ECO1BGEO files (ordered separately) from the LP DAAC Data Pool or Earthdata Search Client to a local directory (see above for applicable products)\n\nCopy/clone/download ECOSTRESS_swath2grid.py from LP DAAC Data User Resources Repository\n\n\n\n\n\nIt is recommended to use Conda, an environment manager to set up a compatible Python environment. Download Conda for your OS here: https://www.anaconda.com/download/. Once you have Conda installed, Follow the instructions below to successfully setup a Python environment on MacOS or Windows.\n\nUsing your preferred command line interface (command prompt, terminal, cmder, etc.) type the following to successfully create a compatible python environment:\n&gt; conda create -n ecostress -c conda-forge --yes python=3.8 h5py pyproj pyresample gdal pandas scipy  \n&gt; conda activate ecostress  \n\n\nNOTE: If you are having trouble activating your environment, or loading specific packages once you have activated your environment? Try entering conda update conda or conda update --all\n\nAdditional information on setting up and managing Conda environments.\nIf you prefer to not install Conda, the same setup and dependencies can be achieved by using another package manager such as pip.\nStill having trouble getting a compatible Python environment set up? Contact LP DAAC User Services.",
    "crumbs": [
      "Scripts",
      "ECOSTRESS Swath to Grid Conversion Script"
    ]
  },
  {
    "objectID": "python/scripts/ecostress_swath2grid/swath2grid.html#script-execution",
    "href": "python/scripts/ecostress_swath2grid/swath2grid.html#script-execution",
    "title": "ECOSTRESS Swath to Grid Conversion Script",
    "section": "",
    "text": "Once you have set up your MacOS/Windows environment and it has been activated, run the script with the following in your Command Prompt/terminal window:\n&gt; python ECOSTRESS_swath2grid.py --proj &lt;insert reprojection desired, Options: GEO and UTM&gt; --dir &lt;insert input directory with ECOSTRESS files here&gt;\nWhere:\n\nGEO = Geographic lat/lon, EPSG code 4326\n\nUTM = Universal Transverse Mercator Zones (north/south) with WGS84 datum\n\n\nNOTE: --proj argument is case sensitive\n\nExample\n&gt; python ECOSTRESS_swath2grid.py --proj GEO --dir C:\\Users\\ECOSTRESS\\\nIf UTM is selected, the script will calculate the UTM zone by using the location of the center of each ECOSTRESS granule. If you prefer to set the UTM zone manually, you can do so by adding the optional argument --utmzone &lt;insert EPSG code for desired zone&gt;. This optional argument will override the default functionality for users who desire all ECOSTRESS granules to be in a common UTM projection, regardless of the center location of the granule.\nExample\n&gt; python ECOSTRESS_swath2grid.py --proj UTM --dir &lt;insert input directory with ECOSTRESS files here&gt; --utmzone &lt;insert EPSG code for desired UTM zone, e.g. 32610&gt;  \n\nNOTE: You can look up EPSG codes for UTM zones at: http://spatialreference.org/, note that only WGS84 datum is supported, and thus EPSG codes for UTM north zones will begin with 326 and utm south zones with 327\n\nThe default functionality is to export each science dataset (SDS) layer contained in an ECOSTRESS product as a GeoTIFF. If you prefer to only export one or more layers, you can do so by adding the optional argument --sds &lt;insert SDS layer names desired&gt; (comma separated with no spaces, see below for specific SDS layer names by product–note that the input is case sensitive)\nExample\n&gt; python ECOSTRESS_swath2grid.py --proj GEO --dir C:\\Users\\ECOSTRESS\\ --sds LST,QC,Emis1  \n\nNOTE: SDS layer names for each product are below in section Subsetting Layers.\n\nThe default functionality for L1B radiance is to export each layer contained as radiance (W/m^2/sr/um). If you prefer to export the L1B datasets as brightness temperature (K), you can do so by adding the optional argument --bt (see section Radiance to Brightness Temperature Conversion for additional information)\nExample\n&gt; python ECOSTRESS_swath2grid.py --proj GEO --dir C:\\Users\\ECOSTRESS\\ --bt`",
    "crumbs": [
      "Scripts",
      "ECOSTRESS Swath to Grid Conversion Script"
    ]
  },
  {
    "objectID": "python/scripts/ecostress_swath2grid/swath2grid.html#radiance-to-brightness-temperature-conversion",
    "href": "python/scripts/ecostress_swath2grid/swath2grid.html#radiance-to-brightness-temperature-conversion",
    "title": "ECOSTRESS Swath to Grid Conversion Script",
    "section": "",
    "text": "To use the --bt optional command line argument to convert L1B radiance science datasets (bands 1-5) from radiance (W/m^2/sr/um) to brightness temperature (K), you will need to add the --bt argument to your script execution command. Example:\n&gt; python ECOSTRESS_swath2grid.py --proj GEO --dir C:\\Users\\ECOSTRESS\\ --bt  \n\nNote: You will first need to download the EcostressBrightnessTemperatureV01.h5 Lookup Table (LUT) and save it to the directory containing your ECOSTRESS_swath2grid.py script, or the directory containing the ECOSTRESS L1B files to be processed.",
    "crumbs": [
      "Scripts",
      "ECOSTRESS Swath to Grid Conversion Script"
    ]
  },
  {
    "objectID": "python/scripts/ecostress_swath2grid/swath2grid.html#subsetting-layers",
    "href": "python/scripts/ecostress_swath2grid/swath2grid.html#subsetting-layers",
    "title": "ECOSTRESS Swath to Grid Conversion Script",
    "section": "",
    "text": "To use the --sds optional command line argument in order to select a subset of science datasets from an ECOSTRESS granule, you will need to submit 1 or more SDS layers names into the --sds argument exactly as they appear in the list below. The SDS layers must be comma separated, with no spaces between SDS!\n\nExample for a single layer: --sds LST\n\nExample for multiple layers: --sds ETcanopy,ETdaily,ETinst",
    "crumbs": [
      "Scripts",
      "ECOSTRESS Swath to Grid Conversion Script"
    ]
  },
  {
    "objectID": "python/scripts/ecostress_swath2grid/swath2grid.html#available-ecostress-layers",
    "href": "python/scripts/ecostress_swath2grid/swath2grid.html#available-ecostress-layers",
    "title": "ECOSTRESS Swath to Grid Conversion Script",
    "section": "",
    "text": "ECO1BGEO\n\nheight\n\nland_fraction\n\nlatitude\n\nlongitude\n\nsolar_azimuth\n\nsolar_zenith\n\nview_azimuth\n\nview_zenith\n\nECO1BMAPRAD\n\ndata_quality_1\ndata_quality_2\n\ndata_quality_3\n\ndata_quality_4\n\ndata_quality_5\n\nheight\n\nlatitude\n\nlongitude\n\nradiance_1 (can be exported to brightnesstemperature_1)\n\nradiance_2 (can be exported to brightnesstemperature_2)\n\nradiance_3 (can be exported to brightnesstemperature_3)\n\nradiance_4 (can be exported to brightnesstemperature_4)\n\nradiance_5 (can be exported to brightnesstemperature_5)\n\nsolar_azimuth\n\nsolar_zenith\n\nswir_dn\n\nview_azimuth\n\nview_zenith\n\nECO1BRAD\n\ndata_quality_1\n\ndata_quality_2\n\ndata_quality_3\n\ndata_quality_4\n\ndata_quality_5\n\nradiance_1 (can be exported to brightnesstemperature_1)\n\nradiance_2 (can be exported to brightnesstemperature_2)\n\nradiance_3 (can be exported to brightnesstemperature_3)\n\nradiance_4 (can be exported to brightnesstemperature_4)\n\nradiance_5 (can be exported to brightnesstemperature_5)\n\nswir_dn\n\nECO2CLD\n\nCloudMask\n\nECO2LSTE\n\nEmis1\n\nEmis1_err\n\nEmis2\n\nEmis2_err\n\nEmis3\n\nEmis3_err\n\nEmis4\n\nEmis4_err\n\nEmis5\n\nEmis5_err\n\nEmisWB\n\nLST\n\nLST_err\n\nPWV\n\nQC\n\nECO3ETALEXIU\n\nETdaily\n\nETdailyUncertainty\n\nQualityFlag\n\nECO3ETPTJPL\n\nETcanopy\n\nETdaily\n\nETinst\n\nETinstUncertainty\n\nETinterception\n\nETsoil\n\nECO3ANCQA\n\nECOSTRESS_L2_QC\n\nLandsat8_QC\n\nMCD12Q1_QC\n\nMCD43A3_QC\n\nMOD04_QC\n\nMOD06_1km_QC\n\nMOD06_5km_QC\n\nMOD07_QC\n\nMOD13Q1_QC\n\nMOD17A2H_QC\n\nMOD44W_QC\n\nECO4ESIALEXIU\n\nESIdaily\n\nESIdailyUncertainty\n\nQualityFlag\n\nECO4ESIPTJPL\n\nESIavg\n\nPET\n\nECO4WUE\n\nWUEavg\n\n\n\n\n\n\nECO_L1B_GEO\n\nheight\n\nland_fraction\n\nlatitude\n\nline_start_time_j2000\n\nlongitude\n\nsolar_azimuth\n\nsolar_zenith\n\nview_azimuth\n\nview_zenith\n\nECO_L1B_ATT\n\nAttitude quaternion\n\nAttitude time_j2000\n\nEphemeris eci_position\n\nEphemeris eci_velocity\n\nEphemeris time_j2000\n\nUncorrected Attitude quaternion\n\nUncorrected Attitude time_j2000\n\nUncorrected Ephemeris eci_position\n\nUncorrected Ephemeris eci_velocity\n\nUncorrected Ephemeris time_j2000\n\nECO_L1B_RAD\n\nradiance_1\n\nradiance_2\n\nradiance_3\n\nradiance_4\n\nradiance_5\n\ndata_quality_1\n\ndata_quality_2\n\ndata_quality_3\n\ndata_quality_4\n\ndata_quality_5\n\nswir_dn\n\nEncoderValue\n\nline_start_time_j2000\n\nECO_L2_LSTE\n\nLST\n\nQC\n\nEmis1\n\nEmis2\n\nEmis3\n\nEmis4\n\nEmis5\n\nLST_err\n\nEmis1_err\n\nEmis2_err\n\nEmis3_err\n\nEmis4_err\n\nEmis5_err\n\nEmisWB\n\nPWV\n\ncloud_mask\n\nwater_mask\n\nECO_L2_CLOUD\n\nCloud_confidence\n\nCloud_final",
    "crumbs": [
      "Scripts",
      "ECOSTRESS Swath to Grid Conversion Script"
    ]
  },
  {
    "objectID": "python/scripts/ecostress_swath2grid/swath2grid.html#contact-information",
    "href": "python/scripts/ecostress_swath2grid/swath2grid.html#contact-information",
    "title": "ECOSTRESS Swath to Grid Conversion Script",
    "section": "",
    "text": "Contact: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)\nWebsite: https://lpdaac.usgs.gov/\nDate last modified: 11-21-2022\n1LP DAAC Work performed under NASA contract NNG14HH33I.",
    "crumbs": [
      "Scripts",
      "ECOSTRESS Swath to Grid Conversion Script"
    ]
  },
  {
    "objectID": "guides/appeears_ecostress.html",
    "href": "guides/appeears_ecostress.html",
    "title": "Application for Extracting and Exploring Analysis Ready Samples (AρρEEARS)",
    "section": "",
    "text": "Application for Extracting and Exploring Analysis Ready Samples (AρρEEARS)\nThe Application for Extracting and Exploring Analysis Ready Samples (AppEEARS) offers a simple and efficient way to access and transform geospatial data from a variety of federal data archives in an easy-to-use web application interface. AppEEARS enables users to subset geospatial data spatially, temporally, and by band/layer for point and area samples. AppEEARS returns not only the requested data, but also the associated quality values, and offers interactive visualizations with summary statistics in the web interface. This tutorial shows how to use AppEEARS to access ECOSTRESS version 2 data hosted in the cloud.\n\nStep 1. Sign in\nSign in using your Earthdata login credentials. If you do not have an Earthdata account, Create an Earthdata Login account at https://urs.earthdata.nasa.gov.\n\n\n\nStep 2. Extract the Sample\nSelect the Point or Area sample using Extract dropdown. You will be directed to Extract Area or Point Sample page.\n1. Enter your sample name.\n2. Upload your area of interest or draw a polygon on the leaflet map for area sample. For point sample, provide the CSV file including the latitude and longitude coordinates. You can also use the map to manually select your locations or type them directly.\n3. Select your time period of interest.\n4. Add datasets you are interested in to your Selected Layers. You can choose from various data collections available in AppEEARS. You can click on the (i) icon for the dataset to see more details. In this example we are interested in the ECOSTRESS LSTE which is managed by the LP DAAC and made available from the NASA Earthdata Cloud archive hosted in the AWS cloud.\n\nFor area sample, you can select your output file format. You also have an option to reproject all your layers to another coordinate reference system.\n\nNow you can submit.\n\n\n\n\nFigure caption: Extract area and point sample for ECOSTRESS data available in AWS cloud in AppEEARS\n\n\nOnce your request is complete, you can View and Download your results from the Explore Requets page.\n\n\n\nFigure caption: Refine search\n\n\n\n\nStep 3. Explore the outputs\nFrom the Explore Requests page, click the View icon in order to view and interact with your results. This will take you to the View Area Sample page. \n\n\nStep 4. Download the outputs\nFinally navigate to Download Sample page by clicking the Download icon on the Explore Requests page or from View Sample page to download your results. Besides your actual outputs, you will have access to supporting files including a text file with URLs to source data, a JSON file you can use to recreate the same sample, decoded quality information, and CSV file with the layer statistics.\n\n\n\nFigure caption: Download Sample Results\n\n\nCheck out AppEEARS help documentation for more details. If you wish to access AppEEARS programatically check out AppEEARS API documenation.",
    "crumbs": [
      "Guides",
      "Access and Subset ECOSTRESS Data using AppEEARS"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html",
    "href": "CODE_OF_CONDUCT.html",
    "title": "Code of Conduct",
    "section": "",
    "text": "We are dedicated to fostering a respectful environment for everyone contributing to this project. We expect all participants to treat each other with respect, professionalism, and kindness.\n\n\n\n\nBe respectful and considerate of others.\nEngage in constructive discussions and offer helpful feedback.\nGracefully accept constructive criticism.\n\n\n\n\nThe following behaviors will not be tolerated:\n\nHarassment, discrimination, or intimidation of any kind.\nOffensive, abusive, or derogatory language and actions.\nPersonal attacks or insults.\nTrolling or disruptive conduct.\nSharing inappropriate content.\n\n\n\n\nIf you experience or witness any behavior that violates this Code of Conduct, please report it by contacting the project maintainers. All reports will be reviewed confidentially.\n\n\n\nViolations of this Code of Conduct may result in actions such as warnings, temporary bans, or permanent exclusion from participation at the discretion of the maintainers.\n\n\n\nEmail: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)¹\nWebsite: https://lpdaac.usgs.gov/\nDate last modified: 01-22-2025\n¹Work performed under USGS contract G15PD00467 for NASA contract NNG14HH33I.",
    "crumbs": [
      "Contributing",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#our-commitment",
    "href": "CODE_OF_CONDUCT.html#our-commitment",
    "title": "Code of Conduct",
    "section": "",
    "text": "We are dedicated to fostering a respectful environment for everyone contributing to this project. We expect all participants to treat each other with respect, professionalism, and kindness.",
    "crumbs": [
      "Contributing",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#expected-behavior",
    "href": "CODE_OF_CONDUCT.html#expected-behavior",
    "title": "Code of Conduct",
    "section": "",
    "text": "Be respectful and considerate of others.\nEngage in constructive discussions and offer helpful feedback.\nGracefully accept constructive criticism.",
    "crumbs": [
      "Contributing",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#unacceptable-behavior",
    "href": "CODE_OF_CONDUCT.html#unacceptable-behavior",
    "title": "Code of Conduct",
    "section": "",
    "text": "The following behaviors will not be tolerated:\n\nHarassment, discrimination, or intimidation of any kind.\nOffensive, abusive, or derogatory language and actions.\nPersonal attacks or insults.\nTrolling or disruptive conduct.\nSharing inappropriate content.",
    "crumbs": [
      "Contributing",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#reporting-violations",
    "href": "CODE_OF_CONDUCT.html#reporting-violations",
    "title": "Code of Conduct",
    "section": "",
    "text": "If you experience or witness any behavior that violates this Code of Conduct, please report it by contacting the project maintainers. All reports will be reviewed confidentially.",
    "crumbs": [
      "Contributing",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement",
    "href": "CODE_OF_CONDUCT.html#enforcement",
    "title": "Code of Conduct",
    "section": "",
    "text": "Violations of this Code of Conduct may result in actions such as warnings, temporary bans, or permanent exclusion from participation at the discretion of the maintainers.",
    "crumbs": [
      "Contributing",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#contact-info",
    "href": "CODE_OF_CONDUCT.html#contact-info",
    "title": "Code of Conduct",
    "section": "",
    "text": "Email: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)¹\nWebsite: https://lpdaac.usgs.gov/\nDate last modified: 01-22-2025\n¹Work performed under USGS contract G15PD00467 for NASA contract NNG14HH33I.",
    "crumbs": [
      "Contributing",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "ECOSTRESS-Data-Resources",
    "section": "",
    "text": "Apache License\n                       Version 2.0, January 2004\n                    http://www.apache.org/licenses/\nTERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\nDefinitions.\n“License” shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document.\n“Licensor” shall mean the copyright owner or entity authorized by the copyright owner that is granting the License.\n“Legal Entity” shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, “control” means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity.\n“You” (or “Your”) shall mean an individual or Legal Entity exercising permissions granted by this License.\n“Source” form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files.\n“Object” form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types.\n“Work” shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below).\n“Derivative Works” shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof.\n“Contribution” shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, “submitted” means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as “Not a Contribution.”\n“Contributor” shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work.\nGrant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form.\nGrant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed.\nRedistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions:\n\nYou must give any other recipients of the Work or Derivative Works a copy of this License; and\nYou must cause any modified files to carry prominent notices stating that You changed the files; and\nYou must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and\nIf the Work includes a “NOTICE” text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License.\n\nYou may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License.\nSubmission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions.\nTrademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file.\nDisclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an “AS IS” BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License.\nLimitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages.\nAccepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability.\n\nEND OF TERMS AND CONDITIONS\nAPPENDIX: How to apply the Apache License to your work.\n  To apply the Apache License to your work, attach the following\n  boilerplate notice, with the fields enclosed by brackets \"[]\"\n  replaced with your own identifying information. (Don't include\n  the brackets!)  The text should be enclosed in the appropriate\n  comment syntax for the file format. We also recommend that a\n  file or class name and description of purpose be included on the\n  same \"printed page\" as the copyright notice for easier\n  identification within third-party archives.\nCopyright [yyyy] [name of copyright owner]\nLicensed under the Apache License, Version 2.0 (the “License”); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n   http://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on an “AS IS” BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
  },
  {
    "objectID": "CHANGE_LOG.html",
    "href": "CHANGE_LOG.html",
    "title": "Change Log",
    "section": "",
    "text": "All notable changes to this project will be documented in this file. The format is based on Keep a Changelog and this project adheres to Semantic Versioning. _________________________________________________________________________ ## 2025-09-12\n\n\nAdded python/tutorials/Work_with_ECOSTRESS_Tiled_data.ipynb\nCreated a webbook\nAdded information for ECOSTESS SATM 2025\n\n\n\n\n\n\nAdded How to direct access notebooks\nAdded Working with ECOSTRESS L2T LSTE Data Tutorial\n\n\n\n\n\n\n\nAdded the extract_geolocation script\n\n\n\n\n\n\n\nAdded Existing ECOSTRESS resources\nAdded Updated README"
  },
  {
    "objectID": "CHANGE_LOG.html#section",
    "href": "CHANGE_LOG.html#section",
    "title": "Change Log",
    "section": "",
    "text": "Added How to direct access notebooks\nAdded Working with ECOSTRESS L2T LSTE Data Tutorial"
  },
  {
    "objectID": "CHANGE_LOG.html#section-1",
    "href": "CHANGE_LOG.html#section-1",
    "title": "Change Log",
    "section": "",
    "text": "Added the extract_geolocation script"
  },
  {
    "objectID": "CHANGE_LOG.html#section-2",
    "href": "CHANGE_LOG.html#section-2",
    "title": "Change Log",
    "section": "",
    "text": "Added Existing ECOSTRESS resources\nAdded Updated README"
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "ECOSTRESS-Data-Resources",
    "section": "",
    "text": "If you are attending the workshop:\n- Go to the Openscapes 2i2c JupyterHub and Log in using your email as the username and the shared password. Select Python and choose a session size of 14.8 GB RAM / up to 3.75 CPUs. Most notebooks will run with this option unless otherwise noted.\n\nReview the prerequisites and workshop set up instructions for details.\n\nIf you are running the notebooks on your local machine:\n- Follow the instructions for setting up Python environmnet locally.",
    "crumbs": [
      "Workshops",
      "Setup Instructions"
    ]
  },
  {
    "objectID": "setup.html#set-up-instruction",
    "href": "setup.html#set-up-instruction",
    "title": "ECOSTRESS-Data-Resources",
    "section": "",
    "text": "If you are attending the workshop:\n- Go to the Openscapes 2i2c JupyterHub and Log in using your email as the username and the shared password. Select Python and choose a session size of 14.8 GB RAM / up to 3.75 CPUs. Most notebooks will run with this option unless otherwise noted.\n\nReview the prerequisites and workshop set up instructions for details.\n\nIf you are running the notebooks on your local machine:\n- Follow the instructions for setting up Python environmnet locally.",
    "crumbs": [
      "Workshops",
      "Setup Instructions"
    ]
  },
  {
    "objectID": "setup.html#cloning-the-repository",
    "href": "setup.html#cloning-the-repository",
    "title": "ECOSTRESS-Data-Resources",
    "section": "Cloning the Repository",
    "text": "Cloning the Repository\nTo work with the notebooks or modules, clone the repository to your desired directory:\ngit clone https://github.com/nasa/ECOSTRESS-Data-Resources.git\nIf you plan to edit or contribute to the ECOSTRESS-Data-Resources repository, we recommend following a fork and pull workflow: first fork the repository, then clone your fork to your local machine, make changes, push changes to your fork, then make a pull request back to the main repository. An example can be found in the CONTRIBUTING.md file.",
    "crumbs": [
      "Workshops",
      "Setup Instructions"
    ]
  }
]